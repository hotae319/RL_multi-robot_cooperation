{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Practice 1\n",
    " _by Hotae Lee_\n",
    "* Q network - FrozenLake\n",
    "* Q network - CartPole\n",
    "* DQN 2013(Deeper Network & replay buffer) - CartPole\n",
    "* DQN 2015(Use double network to solve unstationary target) -CartPole\n",
    "* Policy Gradient (Actor-Critic) - CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q network - FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of successful episodes: 0.489%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0d2822f290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "# Input and output size based on the Env\n",
    "input_size = env.observation_space.n\n",
    "output_size = env.action_space.n\n",
    "learning_rate = 0.1\n",
    "\n",
    "# These lines establish the feed-forward part of the network used to\n",
    "# choose actions\n",
    "X = tf.placeholder(shape=[1, input_size], dtype=tf.float32)  # state input\n",
    "W = tf.Variable(tf.random_uniform(\n",
    "    [input_size, output_size], 0, 0.01))  # weight\n",
    "\n",
    "Qpred = tf.matmul(X, W)  # Out Q prediction\n",
    "Y = tf.placeholder(shape=[1, output_size], dtype=tf.float32)  # Y label\n",
    "\n",
    "loss = tf.reduce_sum(tf.square(Y - Qpred))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# Set Q-learning related parameters\n",
    "dis = .99\n",
    "num_episodes = 2000\n",
    "\n",
    "# Create lists to contain total rewards and steps per episode\n",
    "rList = []\n",
    "    \n",
    "\n",
    "def one_hot(x):\n",
    "    return np.identity(16)[x:x + 1]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        # Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        e = 1. / ((i / 50) + 10)\n",
    "        rAll = 0\n",
    "        done = False\n",
    "        local_loss = []\n",
    "\n",
    "        # The Q-Network training\n",
    "        while not done:\n",
    "            # Choose an action by greedily (with e chance of random action)\n",
    "            # from the Q-network\n",
    "            Qs = sess.run(Qpred, feed_dict={X: one_hot(s)})\n",
    "            if np.random.rand(1) < e:\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                a = np.argmax(Qs)\n",
    "\n",
    "            # Get new state and reward from environment\n",
    "            s1, reward, done, _ = env.step(a)\n",
    "            if done:\n",
    "                # Update Q, and no Qs+1, since it's a terminal state\n",
    "                Qs[0, a] = reward\n",
    "            else:\n",
    "                # Obtain the Q_s1 values by feeding the new state through our\n",
    "                # network\n",
    "                Qs1 = sess.run(Qpred, feed_dict={X: one_hot(s1)})\n",
    "                # Update Q\n",
    "                Qs[0, a] = reward + dis * np.max(Qs1)\n",
    "\n",
    "            # Train our network using target (Y) and predicted Q (Qpred) values\n",
    "            sess.run(train, feed_dict={X: one_hot(s), Y: Qs})\n",
    "\n",
    "            rAll += reward\n",
    "            s = s1\n",
    "        rList.append(rAll)\n",
    "\n",
    "print(\"Percent of successful episodes: \" +\n",
    "      str(sum(rList) / num_episodes) + \"%\")\n",
    "plt.bar(range(len(rList)), rList, color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPole Random action test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "(array([-0.00062584,  0.20521483, -0.02730596, -0.32927179]), 1.0, False)\n",
      "(array([ 0.00347845,  0.40071464, -0.03389139, -0.63043915]), 1.0, False)\n",
      "(array([ 0.01149275,  0.5962927 , -0.04650018, -0.93359993]), 1.0, False)\n",
      "(array([ 0.0234186 ,  0.7920101 , -0.06517217, -1.24052482]), 1.0, False)\n",
      "(array([ 0.0392588 ,  0.9879055 , -0.08998267, -1.55289116]), 1.0, False)\n",
      "(array([ 0.05901691,  1.18398371, -0.12104049, -1.87223779]), 1.0, False)\n",
      "(array([ 0.08269659,  1.38020241, -0.15848525, -2.19991147]), 1.0, False)\n",
      "(array([ 0.11030063,  1.57645655, -0.20248348, -2.53700319]), 1.0, False)\n",
      "(array([ 0.14182977,  1.38346807, -0.25322354, -2.31255875]), 1.0, True)\n",
      "('Reward for this episode was:', 9.0)\n",
      "\u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "(array([ 0.16949913,  1.57985932, -0.29947472, -2.67140586]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n",
      "(array([ 0.20109631,  1.38794821, -0.35290284, -2.48308707]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n",
      "(array([ 0.22885528,  1.19706692, -0.40256458, -2.31602341]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n",
      "(array([ 0.25279662,  1.0071967 , -0.44888505, -2.16916871]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n",
      "(array([ 0.27294055,  0.81829275, -0.49226842, -2.04146905]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n",
      "(array([ 0.2893064 ,  1.01426331, -0.5330978 , -2.43947355]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n",
      "(array([ 0.30959167,  0.82602511, -0.58188727, -2.34570893]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n",
      "(array([ 0.32611217,  1.02048369, -0.62880145, -2.75097569]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n",
      "(array([ 0.34652185,  0.83259704, -0.68382096, -2.69597418]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.04890821, -0.00819998,  0.04947044,  0.00126633])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "random_episodes = 0\n",
    "reward_sum = 0\n",
    "while random_episodes < 10:\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    print(observation, reward, done)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        random_episodes += 1\n",
    "        print(\"Reward for this episode was:\", reward_sum)\n",
    "        reward_sum = 0\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Network - CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode: 0  steps: 16\n",
      "Episode: 1  steps: 15\n",
      "Episode: 2  steps: 20\n",
      "Episode: 3  steps: 11\n",
      "Episode: 4  steps: 16\n",
      "Episode: 5  steps: 20\n",
      "Episode: 6  steps: 17\n",
      "Episode: 7  steps: 19\n",
      "Episode: 8  steps: 16\n",
      "Episode: 9  steps: 21\n",
      "Episode: 10  steps: 10\n",
      "Episode: 11  steps: 12\n",
      "Episode: 12  steps: 18\n",
      "Episode: 13  steps: 13\n",
      "Episode: 14  steps: 11\n",
      "Episode: 15  steps: 12\n",
      "Episode: 16  steps: 8\n",
      "Episode: 17  steps: 10\n",
      "Episode: 18  steps: 15\n",
      "Episode: 19  steps: 9\n",
      "Episode: 20  steps: 10\n",
      "Episode: 21  steps: 10\n",
      "Episode: 22  steps: 10\n",
      "Episode: 23  steps: 12\n",
      "Episode: 24  steps: 70\n",
      "Episode: 25  steps: 61\n",
      "Episode: 26  steps: 32\n",
      "Episode: 27  steps: 61\n",
      "Episode: 28  steps: 22\n",
      "Episode: 29  steps: 22\n",
      "Episode: 30  steps: 10\n",
      "Episode: 31  steps: 41\n",
      "Episode: 32  steps: 36\n",
      "Episode: 33  steps: 23\n",
      "Episode: 34  steps: 19\n",
      "Episode: 35  steps: 10\n",
      "Episode: 36  steps: 11\n",
      "Episode: 37  steps: 9\n",
      "Episode: 38  steps: 9\n",
      "Episode: 39  steps: 28\n",
      "Episode: 40  steps: 28\n",
      "Episode: 41  steps: 32\n",
      "Episode: 42  steps: 30\n",
      "Episode: 43  steps: 33\n",
      "Episode: 44  steps: 31\n",
      "Episode: 45  steps: 43\n",
      "Episode: 46  steps: 37\n",
      "Episode: 47  steps: 33\n",
      "Episode: 48  steps: 74\n",
      "Episode: 49  steps: 40\n",
      "Episode: 50  steps: 71\n",
      "Episode: 51  steps: 38\n",
      "Episode: 52  steps: 39\n",
      "Episode: 53  steps: 29\n",
      "Episode: 54  steps: 67\n",
      "Episode: 55  steps: 30\n",
      "Episode: 56  steps: 137\n",
      "Episode: 57  steps: 104\n",
      "Episode: 58  steps: 49\n",
      "Episode: 59  steps: 193\n",
      "Episode: 60  steps: 200\n",
      "Episode: 61  steps: 53\n",
      "Episode: 62  steps: 169\n",
      "Episode: 63  steps: 73\n",
      "Episode: 64  steps: 24\n",
      "Episode: 65  steps: 16\n",
      "Episode: 66  steps: 29\n",
      "Episode: 67  steps: 25\n",
      "Episode: 68  steps: 47\n",
      "Episode: 69  steps: 58\n",
      "Episode: 70  steps: 33\n",
      "Episode: 71  steps: 23\n",
      "Episode: 72  steps: 54\n",
      "Episode: 73  steps: 50\n",
      "Episode: 74  steps: 30\n",
      "Episode: 75  steps: 51\n",
      "Episode: 76  steps: 29\n",
      "Episode: 77  steps: 49\n",
      "Episode: 78  steps: 42\n",
      "Episode: 79  steps: 38\n",
      "Episode: 80  steps: 27\n",
      "Episode: 81  steps: 32\n",
      "Episode: 82  steps: 68\n",
      "Episode: 83  steps: 68\n",
      "Episode: 84  steps: 27\n",
      "Episode: 85  steps: 34\n",
      "Episode: 86  steps: 26\n",
      "Episode: 87  steps: 8\n",
      "Episode: 88  steps: 10\n",
      "Episode: 89  steps: 22\n",
      "Episode: 90  steps: 38\n",
      "Episode: 91  steps: 39\n",
      "Episode: 92  steps: 32\n",
      "Episode: 93  steps: 30\n",
      "Episode: 94  steps: 22\n",
      "Episode: 95  steps: 10\n",
      "Episode: 96  steps: 9\n",
      "Episode: 97  steps: 8\n",
      "Episode: 98  steps: 10\n",
      "Episode: 99  steps: 10\n",
      "Episode: 100  steps: 25\n",
      "Episode: 101  steps: 28\n",
      "Episode: 102  steps: 40\n",
      "Episode: 103  steps: 41\n",
      "Episode: 104  steps: 29\n",
      "Episode: 105  steps: 26\n",
      "Episode: 106  steps: 23\n",
      "Episode: 107  steps: 34\n",
      "Episode: 108  steps: 27\n",
      "Episode: 109  steps: 78\n",
      "Episode: 110  steps: 33\n",
      "Episode: 111  steps: 32\n",
      "Episode: 112  steps: 27\n",
      "Episode: 113  steps: 33\n",
      "Episode: 114  steps: 8\n",
      "Episode: 115  steps: 40\n",
      "Episode: 116  steps: 24\n",
      "Episode: 117  steps: 21\n",
      "Episode: 118  steps: 44\n",
      "Episode: 119  steps: 27\n",
      "Episode: 120  steps: 39\n",
      "Episode: 121  steps: 61\n",
      "Episode: 122  steps: 62\n",
      "Episode: 123  steps: 43\n",
      "Episode: 124  steps: 29\n",
      "Episode: 125  steps: 57\n",
      "Episode: 126  steps: 38\n",
      "Episode: 127  steps: 23\n",
      "Episode: 128  steps: 24\n",
      "Episode: 129  steps: 26\n",
      "Episode: 130  steps: 35\n",
      "Episode: 131  steps: 43\n",
      "Episode: 132  steps: 33\n",
      "Episode: 133  steps: 52\n",
      "Episode: 134  steps: 36\n",
      "Episode: 135  steps: 36\n",
      "Episode: 136  steps: 50\n",
      "Episode: 137  steps: 29\n",
      "Episode: 138  steps: 45\n",
      "Episode: 139  steps: 51\n",
      "Episode: 140  steps: 74\n",
      "Episode: 141  steps: 37\n",
      "Episode: 142  steps: 23\n",
      "Episode: 143  steps: 29\n",
      "Episode: 144  steps: 37\n",
      "Episode: 145  steps: 25\n",
      "Episode: 146  steps: 34\n",
      "Episode: 147  steps: 23\n",
      "Episode: 148  steps: 30\n",
      "Episode: 149  steps: 26\n",
      "Episode: 150  steps: 62\n",
      "Episode: 151  steps: 88\n",
      "Episode: 152  steps: 27\n",
      "Episode: 153  steps: 73\n",
      "Episode: 154  steps: 27\n",
      "Episode: 155  steps: 23\n",
      "Episode: 156  steps: 30\n",
      "Episode: 157  steps: 11\n",
      "Episode: 158  steps: 22\n",
      "Episode: 159  steps: 45\n",
      "Episode: 160  steps: 43\n",
      "Episode: 161  steps: 29\n",
      "Episode: 162  steps: 28\n",
      "Episode: 163  steps: 30\n",
      "Episode: 164  steps: 85\n",
      "Episode: 165  steps: 47\n",
      "Episode: 166  steps: 30\n",
      "Episode: 167  steps: 36\n",
      "Episode: 168  steps: 18\n",
      "Episode: 169  steps: 24\n",
      "Episode: 170  steps: 22\n",
      "Episode: 171  steps: 9\n",
      "Episode: 172  steps: 76\n",
      "Episode: 173  steps: 16\n",
      "Episode: 174  steps: 35\n",
      "Episode: 175  steps: 21\n",
      "Episode: 176  steps: 27\n",
      "Episode: 177  steps: 24\n",
      "Episode: 178  steps: 35\n",
      "Episode: 179  steps: 30\n",
      "Episode: 180  steps: 34\n",
      "Episode: 181  steps: 21\n",
      "Episode: 182  steps: 8\n",
      "Episode: 183  steps: 23\n",
      "Episode: 184  steps: 28\n",
      "Episode: 185  steps: 22\n",
      "Episode: 186  steps: 66\n",
      "Episode: 187  steps: 26\n",
      "Episode: 188  steps: 34\n",
      "Episode: 189  steps: 25\n",
      "Episode: 190  steps: 25\n",
      "Episode: 191  steps: 26\n",
      "Episode: 192  steps: 9\n",
      "Episode: 193  steps: 43\n",
      "Episode: 194  steps: 32\n",
      "Episode: 195  steps: 31\n",
      "Episode: 196  steps: 31\n",
      "Episode: 197  steps: 36\n",
      "Episode: 198  steps: 31\n",
      "Episode: 199  steps: 22\n",
      "Episode: 200  steps: 42\n",
      "Episode: 201  steps: 27\n",
      "Episode: 202  steps: 24\n",
      "Episode: 203  steps: 38\n",
      "Episode: 204  steps: 31\n",
      "Episode: 205  steps: 28\n",
      "Episode: 206  steps: 39\n",
      "Episode: 207  steps: 33\n",
      "Episode: 208  steps: 31\n",
      "Episode: 209  steps: 63\n",
      "Episode: 210  steps: 60\n",
      "Episode: 211  steps: 51\n",
      "Episode: 212  steps: 39\n",
      "Episode: 213  steps: 38\n",
      "Episode: 214  steps: 26\n",
      "Episode: 215  steps: 30\n",
      "Episode: 216  steps: 28\n",
      "Episode: 217  steps: 23\n",
      "Episode: 218  steps: 53\n",
      "Episode: 219  steps: 26\n",
      "Episode: 220  steps: 29\n",
      "Episode: 221  steps: 20\n",
      "Episode: 222  steps: 50\n",
      "Episode: 223  steps: 20\n",
      "Episode: 224  steps: 35\n",
      "Episode: 225  steps: 29\n",
      "Episode: 226  steps: 37\n",
      "Episode: 227  steps: 35\n",
      "Episode: 228  steps: 44\n",
      "Episode: 229  steps: 28\n",
      "Episode: 230  steps: 29\n",
      "Episode: 231  steps: 24\n",
      "Episode: 232  steps: 27\n",
      "Episode: 233  steps: 27\n",
      "Episode: 234  steps: 55\n",
      "Episode: 235  steps: 37\n",
      "Episode: 236  steps: 53\n",
      "Episode: 237  steps: 55\n",
      "Episode: 238  steps: 61\n",
      "Episode: 239  steps: 54\n",
      "Episode: 240  steps: 42\n",
      "Episode: 241  steps: 43\n",
      "Episode: 242  steps: 112\n",
      "Episode: 243  steps: 28\n",
      "Episode: 244  steps: 21\n",
      "Episode: 245  steps: 35\n",
      "Episode: 246  steps: 15\n",
      "Episode: 247  steps: 46\n",
      "Episode: 248  steps: 30\n",
      "Episode: 249  steps: 23\n",
      "Episode: 250  steps: 33\n",
      "Episode: 251  steps: 33\n",
      "Episode: 252  steps: 44\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f388c8ca0780>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Train our network using target and predicted Q values on each episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "This code is based on\n",
    "https://github.com/hunkim/DeepRL-Agents\n",
    "'''\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Constants defining our neural network\n",
    "learning_rate = 1e-1\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, input_size], name = \"input_x\")\n",
    "\n",
    "# First layer of weights\n",
    "W1 = tf.get_variable(\"W1\", shape=[input_size, output_size],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "Qpred = tf.matmul(X, W1)\n",
    "\n",
    "# We need to define the parts of the network needed for learning a policy\n",
    "Y = tf.placeholder(shape=[None, output_size], dtype=tf.float32)\n",
    "\n",
    "# Loss function\n",
    "loss = tf.reduce_sum(tf.square(Y - Qpred))\n",
    "# Learning\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# Values for q learning\n",
    "max_episodes = 5000\n",
    "dis = 0.9\n",
    "step_history = []\n",
    "\n",
    "\n",
    "# Setting up our environment\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    e = 1. / ((episode / 10) + 1)\n",
    "    step_count = 0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # The Q-Network training\n",
    "    while not done:\n",
    "        step_count += 1\n",
    "        x = np.reshape(state, [1, input_size])\n",
    "        # Choose an action by greedily (with e chance of random action) from\n",
    "        # the Q-network\n",
    "        Q = sess.run(Qpred, feed_dict={X: x})\n",
    "        if np.random.rand(1) < e:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q)\n",
    "\n",
    "        # Get new state and reward from environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            Q[0, action] = -100\n",
    "        else:\n",
    "            x_next = np.reshape(next_state, [1, input_size])\n",
    "            # Obtain the Q' values by feeding the new state through our network\n",
    "            Q_next = sess.run(Qpred, feed_dict={X: x_next})\n",
    "            Q[0, action] = reward + dis * np.max(Q_next)\n",
    "\n",
    "        # Train our network using target and predicted Q values on each episode\n",
    "        sess.run(train, feed_dict={X: x, Y: Q})\n",
    "        state = next_state\n",
    "\n",
    "    step_history.append(step_count)\n",
    "    print(\"Episode: {}  steps: {}\".format(episode, step_count))\n",
    "    # If last 10's avg steps are 500, it's good enough\n",
    "    if len(step_history) > 10 and np.mean(step_history[-10:]) > 500:\n",
    "        break\n",
    "\n",
    "# See our trained network in action\n",
    "observation = env.reset()\n",
    "reward_sum = 0\n",
    "while True:\n",
    "    env.render()\n",
    "\n",
    "    x = np.reshape(observation, [1, input_size])\n",
    "    Q = sess.run(Qpred, feed_dict={X: x})\n",
    "    action = np.argmax(Q)\n",
    "\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        print(\"Total score: {}\".format(reward_sum))\n",
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN NIPS2013 TEST\n",
    "* by _sung kim_\n",
    "\n",
    "about cart_pole using DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[Episode     0]  steps:    15 e:  1.00\n",
      "[Episode     1]  steps:    13 e:  0.98\n",
      "[Episode     2]  steps:    21 e:  0.96\n",
      "[Episode     3]  steps:    50 e:  0.94\n",
      "[Episode     4]  steps:    16 e:  0.92\n",
      "[Episode     5]  steps:    38 e:  0.90\n",
      "[Episode     6]  steps:    19 e:  0.88\n",
      "[Episode     7]  steps:    47 e:  0.86\n",
      "[Episode     8]  steps:    15 e:  0.84\n",
      "[Episode     9]  steps:    20 e:  0.82\n",
      "[Episode    10]  steps:    17 e:  0.80\n",
      "[Episode    11]  steps:    12 e:  0.78\n",
      "[Episode    12]  steps:    15 e:  0.76\n",
      "[Episode    13]  steps:    12 e:  0.74\n",
      "[Episode    14]  steps:    13 e:  0.72\n",
      "[Episode    15]  steps:    12 e:  0.70\n",
      "[Episode    16]  steps:    12 e:  0.68\n",
      "[Episode    17]  steps:    21 e:  0.66\n",
      "[Episode    18]  steps:    10 e:  0.64\n",
      "[Episode    19]  steps:    10 e:  0.62\n",
      "[Episode    20]  steps:     8 e:  0.60\n",
      "[Episode    21]  steps:    10 e:  0.58\n",
      "[Episode    22]  steps:    11 e:  0.56\n",
      "[Episode    23]  steps:     9 e:  0.54\n",
      "[Episode    24]  steps:    11 e:  0.52\n",
      "[Episode    25]  steps:    11 e:  0.50\n",
      "[Episode    26]  steps:    12 e:  0.48\n",
      "[Episode    27]  steps:    13 e:  0.46\n",
      "[Episode    28]  steps:    10 e:  0.44\n",
      "[Episode    29]  steps:    15 e:  0.42\n",
      "[Episode    30]  steps:    15 e:  0.40\n",
      "[Episode    31]  steps:    11 e:  0.38\n",
      "[Episode    32]  steps:    11 e:  0.36\n",
      "[Episode    33]  steps:    14 e:  0.34\n",
      "[Episode    34]  steps:    11 e:  0.32\n",
      "[Episode    35]  steps:    14 e:  0.30\n",
      "[Episode    36]  steps:    14 e:  0.28\n",
      "[Episode    37]  steps:     8 e:  0.26\n",
      "[Episode    38]  steps:     9 e:  0.24\n",
      "[Episode    39]  steps:    12 e:  0.22\n",
      "[Episode    40]  steps:    13 e:  0.20\n",
      "[Episode    41]  steps:    16 e:  0.18\n",
      "[Episode    42]  steps:     9 e:  0.16\n",
      "[Episode    43]  steps:     9 e:  0.14\n",
      "[Episode    44]  steps:     8 e:  0.12\n",
      "[Episode    45]  steps:     9 e:  0.10\n",
      "[Episode    46]  steps:     9 e:  0.08\n",
      "[Episode    47]  steps:    10 e:  0.06\n",
      "[Episode    48]  steps:    10 e:  0.04\n",
      "[Episode    49]  steps:    11 e:  0.02\n",
      "[Episode    50]  steps:    10 e:  0.00\n",
      "[Episode    51]  steps:     8 e:  0.00\n",
      "[Episode    52]  steps:    10 e:  0.00\n",
      "[Episode    53]  steps:    10 e:  0.00\n",
      "[Episode    54]  steps:    10 e:  0.00\n",
      "[Episode    55]  steps:    10 e:  0.00\n",
      "[Episode    56]  steps:     9 e:  0.00\n",
      "[Episode    57]  steps:    10 e:  0.00\n",
      "[Episode    58]  steps:    11 e:  0.00\n",
      "[Episode    59]  steps:    13 e:  0.00\n",
      "[Episode    60]  steps:    13 e:  0.00\n",
      "[Episode    61]  steps:    16 e:  0.00\n",
      "[Episode    62]  steps:    28 e:  0.00\n",
      "[Episode    63]  steps:    19 e:  0.00\n",
      "[Episode    64]  steps:    31 e:  0.00\n",
      "[Episode    65]  steps:    25 e:  0.00\n",
      "[Episode    66]  steps:    50 e:  0.00\n",
      "[Episode    67]  steps:    68 e:  0.00\n",
      "[Episode    68]  steps:   137 e:  0.00\n",
      "[Episode    69]  steps:   200 e:  0.00\n",
      "[Episode    70]  steps:   157 e:  0.00\n",
      "[Episode    71]  steps:    18 e:  0.00\n",
      "[Episode    72]  steps:    13 e:  0.00\n",
      "[Episode    73]  steps:    13 e:  0.00\n",
      "[Episode    74]  steps:    10 e:  0.00\n",
      "[Episode    75]  steps:    10 e:  0.00\n",
      "[Episode    76]  steps:    14 e:  0.00\n",
      "[Episode    77]  steps:   111 e:  0.00\n",
      "[Episode    78]  steps:   114 e:  0.00\n",
      "[Episode    79]  steps:    15 e:  0.00\n",
      "[Episode    80]  steps:    18 e:  0.00\n",
      "[Episode    81]  steps:    12 e:  0.00\n",
      "[Episode    82]  steps:    11 e:  0.00\n",
      "[Episode    83]  steps:    11 e:  0.00\n",
      "[Episode    84]  steps:    13 e:  0.00\n",
      "[Episode    85]  steps:    17 e:  0.00\n",
      "[Episode    86]  steps:    17 e:  0.00\n",
      "[Episode    87]  steps:    15 e:  0.00\n",
      "[Episode    88]  steps:    14 e:  0.00\n",
      "[Episode    89]  steps:    11 e:  0.00\n",
      "[Episode    90]  steps:     9 e:  0.00\n",
      "[Episode    91]  steps:    11 e:  0.00\n",
      "[Episode    92]  steps:    14 e:  0.00\n",
      "[Episode    93]  steps:    13 e:  0.00\n",
      "[Episode    94]  steps:    16 e:  0.00\n",
      "[Episode    95]  steps:    12 e:  0.00\n",
      "[Episode    96]  steps:    13 e:  0.00\n",
      "[Episode    97]  steps:    16 e:  0.00\n",
      "[Episode    98]  steps:    10 e:  0.00\n",
      "[Episode    99]  steps:    11 e:  0.00\n",
      "[Episode   100]  steps:    18 e:  0.00\n",
      "[Episode   101]  steps:    15 e:  0.00\n",
      "[Episode   102]  steps:    82 e:  0.00\n",
      "[Episode   103]  steps:    56 e:  0.00\n",
      "[Episode   104]  steps:    35 e:  0.00\n",
      "[Episode   105]  steps:    22 e:  0.00\n",
      "[Episode   106]  steps:    21 e:  0.00\n",
      "[Episode   107]  steps:    16 e:  0.00\n",
      "[Episode   108]  steps:    14 e:  0.00\n",
      "[Episode   109]  steps:    47 e:  0.00\n",
      "[Episode   110]  steps:    58 e:  0.00\n",
      "[Episode   111]  steps:    53 e:  0.00\n",
      "[Episode   112]  steps:    62 e:  0.00\n",
      "[Episode   113]  steps:    83 e:  0.00\n",
      "[Episode   114]  steps:   114 e:  0.00\n",
      "[Episode   115]  steps:    84 e:  0.00\n",
      "[Episode   116]  steps:   168 e:  0.00\n",
      "[Episode   117]  steps:    60 e:  0.00\n",
      "[Episode   118]  steps:    89 e:  0.00\n",
      "[Episode   119]  steps:    78 e:  0.00\n",
      "[Episode   120]  steps:    88 e:  0.00\n",
      "[Episode   121]  steps:    44 e:  0.00\n",
      "[Episode   122]  steps:    43 e:  0.00\n",
      "[Episode   123]  steps:    80 e:  0.00\n",
      "[Episode   124]  steps:    74 e:  0.00\n",
      "[Episode   125]  steps:    40 e:  0.00\n",
      "[Episode   126]  steps:    39 e:  0.00\n",
      "[Episode   127]  steps:    26 e:  0.00\n",
      "[Episode   128]  steps:    59 e:  0.00\n",
      "[Episode   129]  steps:    42 e:  0.00\n",
      "[Episode   130]  steps:    27 e:  0.00\n",
      "[Episode   131]  steps:    28 e:  0.00\n",
      "[Episode   132]  steps:    53 e:  0.00\n",
      "[Episode   133]  steps:    48 e:  0.00\n",
      "[Episode   134]  steps:    29 e:  0.00\n",
      "[Episode   135]  steps:    46 e:  0.00\n",
      "[Episode   136]  steps:    21 e:  0.00\n",
      "[Episode   137]  steps:    29 e:  0.00\n",
      "[Episode   138]  steps:    32 e:  0.00\n",
      "[Episode   139]  steps:    38 e:  0.00\n",
      "[Episode   140]  steps:    33 e:  0.00\n",
      "[Episode   141]  steps:    28 e:  0.00\n",
      "[Episode   142]  steps:    28 e:  0.00\n",
      "[Episode   143]  steps:    59 e:  0.00\n",
      "[Episode   144]  steps:    34 e:  0.00\n",
      "[Episode   145]  steps:    38 e:  0.00\n",
      "[Episode   146]  steps:    29 e:  0.00\n",
      "[Episode   147]  steps:    30 e:  0.00\n",
      "[Episode   148]  steps:    33 e:  0.00\n",
      "[Episode   149]  steps:    40 e:  0.00\n",
      "[Episode   150]  steps:    93 e:  0.00\n",
      "[Episode   151]  steps:   122 e:  0.00\n",
      "[Episode   152]  steps:    66 e:  0.00\n",
      "[Episode   153]  steps:    28 e:  0.00\n",
      "[Episode   154]  steps:    72 e:  0.00\n",
      "[Episode   155]  steps:   112 e:  0.00\n",
      "[Episode   156]  steps:    38 e:  0.00\n",
      "[Episode   157]  steps:    44 e:  0.00\n",
      "[Episode   158]  steps:    28 e:  0.00\n",
      "[Episode   159]  steps:    41 e:  0.00\n",
      "[Episode   160]  steps:    71 e:  0.00\n",
      "[Episode   161]  steps:    60 e:  0.00\n",
      "[Episode   162]  steps:   107 e:  0.00\n",
      "[Episode   163]  steps:    39 e:  0.00\n",
      "[Episode   164]  steps:    33 e:  0.00\n",
      "[Episode   165]  steps:    33 e:  0.00\n",
      "[Episode   166]  steps:    38 e:  0.00\n",
      "[Episode   167]  steps:    43 e:  0.00\n",
      "[Episode   168]  steps:    39 e:  0.00\n",
      "[Episode   169]  steps:    35 e:  0.00\n",
      "[Episode   170]  steps:    43 e:  0.00\n",
      "[Episode   171]  steps:    45 e:  0.00\n",
      "[Episode   172]  steps:    56 e:  0.00\n",
      "[Episode   173]  steps:    58 e:  0.00\n",
      "[Episode   174]  steps:   122 e:  0.00\n",
      "[Episode   175]  steps:    39 e:  0.00\n",
      "[Episode   176]  steps:    54 e:  0.00\n",
      "[Episode   177]  steps:   200 e:  0.00\n",
      "[Episode   178]  steps:    93 e:  0.00\n",
      "[Episode   179]  steps:   114 e:  0.00\n",
      "[Episode   180]  steps:    88 e:  0.00\n",
      "[Episode   181]  steps:    60 e:  0.00\n",
      "[Episode   182]  steps:   200 e:  0.00\n",
      "[Episode   183]  steps:    74 e:  0.00\n",
      "[Episode   184]  steps:    70 e:  0.00\n",
      "[Episode   185]  steps:   131 e:  0.00\n",
      "[Episode   186]  steps:    58 e:  0.00\n",
      "[Episode   187]  steps:   200 e:  0.00\n",
      "[Episode   188]  steps:   109 e:  0.00\n",
      "[Episode   189]  steps:    70 e:  0.00\n",
      "[Episode   190]  steps:   171 e:  0.00\n",
      "[Episode   191]  steps:   148 e:  0.00\n",
      "[Episode   192]  steps:    84 e:  0.00\n",
      "[Episode   193]  steps:    96 e:  0.00\n",
      "[Episode   194]  steps:   152 e:  0.00\n",
      "[Episode   195]  steps:   200 e:  0.00\n",
      "[Episode   196]  steps:   200 e:  0.00\n",
      "[Episode   197]  steps:   200 e:  0.00\n",
      "[Episode   198]  steps:   200 e:  0.00\n",
      "[Episode   199]  steps:   123 e:  0.00\n",
      "[Episode   200]  steps:   200 e:  0.00\n",
      "[Episode   201]  steps:   125 e:  0.00\n",
      "[Episode   202]  steps:   200 e:  0.00\n",
      "[Episode   203]  steps:   200 e:  0.00\n",
      "[Episode   204]  steps:   122 e:  0.00\n",
      "[Episode   205]  steps:   200 e:  0.00\n",
      "[Episode   206]  steps:   200 e:  0.00\n",
      "[Episode   207]  steps:   200 e:  0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode   208]  steps:   200 e:  0.00\n",
      "[Episode   209]  steps:   200 e:  0.00\n",
      "[Episode   210]  steps:   200 e:  0.00\n",
      "[Episode   211]  steps:   200 e:  0.00\n",
      "[Episode   212]  steps:   200 e:  0.00\n",
      "[Episode   213]  steps:   200 e:  0.00\n",
      "[Episode   214]  steps:   200 e:  0.00\n",
      "[Episode   215]  steps:   200 e:  0.00\n",
      "[Episode   216]  steps:   200 e:  0.00\n",
      "[Episode   217]  steps:   200 e:  0.00\n",
      "[Episode   218]  steps:   200 e:  0.00\n",
      "[Episode   219]  steps:   200 e:  0.00\n",
      "[Episode   220]  steps:   200 e:  0.00\n",
      "[Episode   221]  steps:   200 e:  0.00\n",
      "[Episode   222]  steps:   200 e:  0.00\n",
      "[Episode   223]  steps:   200 e:  0.00\n",
      "[Episode   224]  steps:   200 e:  0.00\n",
      "[Episode   225]  steps:   200 e:  0.00\n",
      "[Episode   226]  steps:   200 e:  0.00\n",
      "[Episode   227]  steps:   200 e:  0.00\n",
      "[Episode   228]  steps:   200 e:  0.00\n",
      "[Episode   229]  steps:   200 e:  0.00\n",
      "[Episode   230]  steps:   200 e:  0.00\n",
      "[Episode   231]  steps:   200 e:  0.00\n",
      "[Episode   232]  steps:   200 e:  0.00\n",
      "[Episode   233]  steps:   200 e:  0.00\n",
      "[Episode   234]  steps:   200 e:  0.00\n",
      "[Episode   235]  steps:   200 e:  0.00\n",
      "[Episode   236]  steps:   200 e:  0.00\n",
      "[Episode   237]  steps:   200 e:  0.00\n",
      "[Episode   238]  steps:   200 e:  0.00\n",
      "[Episode   239]  steps:   200 e:  0.00\n",
      "[Episode   240]  steps:   200 e:  0.00\n",
      "[Episode   241]  steps:   200 e:  0.00\n",
      "[Episode   242]  steps:   200 e:  0.00\n",
      "[Episode   243]  steps:   200 e:  0.00\n",
      "[Episode   244]  steps:   200 e:  0.00\n",
      "[Episode   245]  steps:   200 e:  0.00\n",
      "[Episode   246]  steps:   200 e:  0.00\n",
      "[Episode   247]  steps:   200 e:  0.00\n",
      "[Episode   248]  steps:   200 e:  0.00\n",
      "[Episode   249]  steps:   200 e:  0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-27ccc79a6ba2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-27ccc79a6ba2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                     \u001b[0mminibatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                     \u001b[0mtrain_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[Episode {:>5}]  steps: {:>5} e: {:>5.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-27ccc79a6ba2>\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[0;34m(DQN, train_batch)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# Train our network using target and predicted Q values on each episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/rl_test/dqn.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, x_stack, y_stack)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Y\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         }\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DQN (NIPS 2013)\n",
    "Playing Atari with Deep Reinforcement Learning\n",
    "https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import dqn\n",
    "import gym\n",
    "from collections import deque\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.wrappers.Monitor(env, 'gym-results/', force=True)\n",
    "INPUT_SIZE = env.observation_space.shape[0]\n",
    "OUTPUT_SIZE = env.action_space.n\n",
    "\n",
    "DISCOUNT_RATE = 0.99\n",
    "REPLAY_MEMORY = 50000\n",
    "MAX_EPISODE = 5000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# minimum epsilon for epsilon greedy\n",
    "MIN_E = 0.0\n",
    "# epsilon will be `MIN_E` at `EPSILON_DECAYING_EPISODE`\n",
    "EPSILON_DECAYING_EPISODE = MAX_EPISODE * 0.01\n",
    "\n",
    "\n",
    "def bot_play(mainDQN):\n",
    "    \"\"\"Runs a single episode with rendering and prints a reward\n",
    "    Args:\n",
    "        mainDQN (dqn.DQN): DQN Agent\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = np.argmax(mainDQN.predict(state))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            print(\"Total score: {}\".format(total_reward))\n",
    "            break\n",
    "\n",
    "\n",
    "def train_minibatch(DQN, train_batch):\n",
    "    \"\"\"Prepare X_batch, y_batch and train them\n",
    "    Recall our loss function is\n",
    "        target = reward + discount * max Q(s',a)\n",
    "                 or reward if done early\n",
    "        Loss function: [target - Q(s, a)]^2\n",
    "    Hence,\n",
    "        X_batch is a state list\n",
    "        y_batch is reward + discount * max Q\n",
    "                   or reward if terminated early\n",
    "    Args:\n",
    "        DQN (dqn.DQN): DQN Agent to train & run\n",
    "        train_batch (list): Minibatch of Replay memory\n",
    "            Eeach element is a tuple of (s, a, r, s', done)\n",
    "    Returns:\n",
    "        loss: Returns a loss\n",
    "    \"\"\"\n",
    "    state_array = np.vstack([x[0] for x in train_batch])\n",
    "    action_array = np.array([x[1] for x in train_batch])\n",
    "    reward_array = np.array([x[2] for x in train_batch])\n",
    "    next_state_array = np.vstack([x[3] for x in train_batch])\n",
    "    done_array = np.array([x[4] for x in train_batch])\n",
    "\n",
    "    X_batch = state_array\n",
    "    y_batch = DQN.predict(state_array)\n",
    "\n",
    "    Q_target = reward_array + DISCOUNT_RATE * np.max(DQN.predict(next_state_array), axis=1) * ~done_array\n",
    "    y_batch[np.arange(len(X_batch)), action_array] = Q_target\n",
    "\n",
    "    # Train our network using target and predicted Q values on each episode\n",
    "    loss, _ = DQN.update(X_batch, y_batch)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def annealing_epsilon(episode, min_e, max_e, target_episode):\n",
    "    \"\"\"Return an linearly annealed epsilon\n",
    "    Epsilon will decrease over time until it reaches `target_episode`\n",
    "         (epsilon)\n",
    "             |\n",
    "    max_e ---|\\\n",
    "             | \\\n",
    "             |  \\\n",
    "             |   \\\n",
    "    min_e ---|____\\_______________(episode)\n",
    "                  |\n",
    "                 target_episode\n",
    "     slope = (min_e - max_e) / (target_episode)\n",
    "     intercept = max_e\n",
    "     e = slope * episode + intercept\n",
    "    Args:\n",
    "        episode (int): Current episode\n",
    "        min_e (float): Minimum epsilon\n",
    "        max_e (float): Maximum epsilon\n",
    "        target_episode (int): epsilon becomes the `min_e` at `target_episode`\n",
    "    Returns:\n",
    "        float: epsilon between `min_e` and `max_e`\n",
    "    \"\"\"\n",
    "\n",
    "    slope = (min_e - max_e) / (target_episode)\n",
    "    intercept = max_e\n",
    "\n",
    "    return max(min_e, slope * episode + intercept)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # store the previous observations in replay memory\n",
    "    replay_buffer = deque(maxlen=REPLAY_MEMORY)\n",
    "    last_100_game_reward = deque(maxlen=100)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        mainDQN = dqn.DQN(sess, INPUT_SIZE, OUTPUT_SIZE)\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        for episode in range(MAX_EPISODE):\n",
    "            e = annealing_epsilon(episode, MIN_E, 1.0, EPSILON_DECAYING_EPISODE)\n",
    "            done = False\n",
    "            state = env.reset()\n",
    "\n",
    "            step_count = 0\n",
    "            while not done:\n",
    "\n",
    "                if np.random.rand() < e:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = np.argmax(mainDQN.predict(state))\n",
    "\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                if done:\n",
    "                    reward = -1\n",
    "\n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "                state = next_state\n",
    "                step_count += 1\n",
    "\n",
    "                if len(replay_buffer) > BATCH_SIZE:\n",
    "                    minibatch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "                    train_minibatch(mainDQN, minibatch)\n",
    "\n",
    "            print(\"[Episode {:>5}]  steps: {:>5} e: {:>5.2f}\".format(episode, step_count, e))\n",
    "\n",
    "            # CartPole-v0 Game Clear Logic\n",
    "            last_100_game_reward.append(step_count)\n",
    "            if len(last_100_game_reward) == last_100_game_reward.maxlen:\n",
    "                avg_reward = np.mean(last_100_game_reward)\n",
    "                if avg_reward > 199.0:\n",
    "                    print(\"Game Cleared within {} episodes with avg reward {}\".format(episode, avg_reward))\n",
    "                    break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
