{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Practice 1\n",
    " _by Hotae Lee_\n",
    "* Q network - FrozenLake\n",
    "* Q network - CartPole\n",
    "* DQN 2013(Deeper Network & replay buffer) - CartPole\n",
    "* DQN 2015(Use double network to solve unstationary target) -CartPole\n",
    "* Policy Gradient (Actor-Critic) - CartPole\n",
    "* Make test environment\n",
    "* Q Network for 3 WMRs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q network - FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of successful episodes: 0.489%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0d2822f290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "# Input and output size based on the Env\n",
    "input_size = env.observation_space.n\n",
    "output_size = env.action_space.n\n",
    "learning_rate = 0.1\n",
    "\n",
    "# These lines establish the feed-forward part of the network used to\n",
    "# choose actions\n",
    "X = tf.placeholder(shape=[1, input_size], dtype=tf.float32)  # state input\n",
    "W = tf.Variable(tf.random_uniform(\n",
    "    [input_size, output_size], 0, 0.01))  # weight\n",
    "\n",
    "Qpred = tf.matmul(X, W)  # Out Q prediction\n",
    "Y = tf.placeholder(shape=[1, output_size], dtype=tf.float32)  # Y label\n",
    "\n",
    "loss = tf.reduce_sum(tf.square(Y - Qpred))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# Set Q-learning related parameters\n",
    "dis = .99\n",
    "num_episodes = 2000\n",
    "\n",
    "# Create lists to contain total rewards and steps per episode\n",
    "rList = []\n",
    "    \n",
    "\n",
    "def one_hot(x):\n",
    "    return np.identity(16)[x:x + 1]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        # Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        e = 1. / ((i / 50) + 10)\n",
    "        rAll = 0\n",
    "        done = False\n",
    "        local_loss = []\n",
    "\n",
    "        # The Q-Network training\n",
    "        while not done:\n",
    "            # Choose an action by greedily (with e chance of random action)\n",
    "            # from the Q-network\n",
    "            Qs = sess.run(Qpred, feed_dict={X: one_hot(s)})\n",
    "            if np.random.rand(1) < e:\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                a = np.argmax(Qs)\n",
    "\n",
    "            # Get new state and reward from environment\n",
    "            s1, reward, done, _ = env.step(a)\n",
    "            if done:\n",
    "                # Update Q, and no Qs+1, since it's a terminal state\n",
    "                Qs[0, a] = reward\n",
    "            else:\n",
    "                # Obtain the Q_s1 values by feeding the new state through our\n",
    "                # network\n",
    "                Qs1 = sess.run(Qpred, feed_dict={X: one_hot(s1)})\n",
    "                # Update Q\n",
    "                Qs[0, a] = reward + dis * np.max(Qs1)\n",
    "\n",
    "            # Train our network using target (Y) and predicted Q (Qpred) values\n",
    "            sess.run(train, feed_dict={X: one_hot(s), Y: Qs})\n",
    "\n",
    "            rAll += reward\n",
    "            s = s1\n",
    "        rList.append(rAll)\n",
    "\n",
    "print(\"Percent of successful episodes: \" +\n",
    "      str(sum(rList) / num_episodes) + \"%\")\n",
    "plt.bar(range(len(rList)), rList, color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPole Random action test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "(array([-0.00062584,  0.20521483, -0.02730596, -0.32927179]), 1.0, False)\n",
      "(array([ 0.00347845,  0.40071464, -0.03389139, -0.63043915]), 1.0, False)\n",
      "(array([ 0.01149275,  0.5962927 , -0.04650018, -0.93359993]), 1.0, False)\n",
      "(array([ 0.0234186 ,  0.7920101 , -0.06517217, -1.24052482]), 1.0, False)\n",
      "(array([ 0.0392588 ,  0.9879055 , -0.08998267, -1.55289116]), 1.0, False)\n",
      "(array([ 0.05901691,  1.18398371, -0.12104049, -1.87223779]), 1.0, False)\n",
      "(array([ 0.08269659,  1.38020241, -0.15848525, -2.19991147]), 1.0, False)\n",
      "(array([ 0.11030063,  1.57645655, -0.20248348, -2.53700319]), 1.0, False)\n",
      "(array([ 0.14182977,  1.38346807, -0.25322354, -2.31255875]), 1.0, True)\n",
      "('Reward for this episode was:', 9.0)\n",
      "\u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "(array([ 0.16949913,  1.57985932, -0.29947472, -2.67140586]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n",
      "(array([ 0.20109631,  1.38794821, -0.35290284, -2.48308707]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n",
      "(array([ 0.22885528,  1.19706692, -0.40256458, -2.31602341]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n",
      "(array([ 0.25279662,  1.0071967 , -0.44888505, -2.16916871]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n",
      "(array([ 0.27294055,  0.81829275, -0.49226842, -2.04146905]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n",
      "(array([ 0.2893064 ,  1.01426331, -0.5330978 , -2.43947355]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n",
      "(array([ 0.30959167,  0.82602511, -0.58188727, -2.34570893]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n",
      "(array([ 0.32611217,  1.02048369, -0.62880145, -2.75097569]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n",
      "(array([ 0.34652185,  0.83259704, -0.68382096, -2.69597418]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.04890821, -0.00819998,  0.04947044,  0.00126633])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "random_episodes = 0\n",
    "reward_sum = 0\n",
    "while random_episodes < 10:\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    print(observation, reward, done)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        random_episodes += 1\n",
    "        print(\"Reward for this episode was:\", reward_sum)\n",
    "        reward_sum = 0\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Network - CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode: 0  steps: 16\n",
      "Episode: 1  steps: 15\n",
      "Episode: 2  steps: 20\n",
      "Episode: 3  steps: 11\n",
      "Episode: 4  steps: 16\n",
      "Episode: 5  steps: 20\n",
      "Episode: 6  steps: 17\n",
      "Episode: 7  steps: 19\n",
      "Episode: 8  steps: 16\n",
      "Episode: 9  steps: 21\n",
      "Episode: 10  steps: 10\n",
      "Episode: 11  steps: 12\n",
      "Episode: 12  steps: 18\n",
      "Episode: 13  steps: 13\n",
      "Episode: 14  steps: 11\n",
      "Episode: 15  steps: 12\n",
      "Episode: 16  steps: 8\n",
      "Episode: 17  steps: 10\n",
      "Episode: 18  steps: 15\n",
      "Episode: 19  steps: 9\n",
      "Episode: 20  steps: 10\n",
      "Episode: 21  steps: 10\n",
      "Episode: 22  steps: 10\n",
      "Episode: 23  steps: 12\n",
      "Episode: 24  steps: 70\n",
      "Episode: 25  steps: 61\n",
      "Episode: 26  steps: 32\n",
      "Episode: 27  steps: 61\n",
      "Episode: 28  steps: 22\n",
      "Episode: 29  steps: 22\n",
      "Episode: 30  steps: 10\n",
      "Episode: 31  steps: 41\n",
      "Episode: 32  steps: 36\n",
      "Episode: 33  steps: 23\n",
      "Episode: 34  steps: 19\n",
      "Episode: 35  steps: 10\n",
      "Episode: 36  steps: 11\n",
      "Episode: 37  steps: 9\n",
      "Episode: 38  steps: 9\n",
      "Episode: 39  steps: 28\n",
      "Episode: 40  steps: 28\n",
      "Episode: 41  steps: 32\n",
      "Episode: 42  steps: 30\n",
      "Episode: 43  steps: 33\n",
      "Episode: 44  steps: 31\n",
      "Episode: 45  steps: 43\n",
      "Episode: 46  steps: 37\n",
      "Episode: 47  steps: 33\n",
      "Episode: 48  steps: 74\n",
      "Episode: 49  steps: 40\n",
      "Episode: 50  steps: 71\n",
      "Episode: 51  steps: 38\n",
      "Episode: 52  steps: 39\n",
      "Episode: 53  steps: 29\n",
      "Episode: 54  steps: 67\n",
      "Episode: 55  steps: 30\n",
      "Episode: 56  steps: 137\n",
      "Episode: 57  steps: 104\n",
      "Episode: 58  steps: 49\n",
      "Episode: 59  steps: 193\n",
      "Episode: 60  steps: 200\n",
      "Episode: 61  steps: 53\n",
      "Episode: 62  steps: 169\n",
      "Episode: 63  steps: 73\n",
      "Episode: 64  steps: 24\n",
      "Episode: 65  steps: 16\n",
      "Episode: 66  steps: 29\n",
      "Episode: 67  steps: 25\n",
      "Episode: 68  steps: 47\n",
      "Episode: 69  steps: 58\n",
      "Episode: 70  steps: 33\n",
      "Episode: 71  steps: 23\n",
      "Episode: 72  steps: 54\n",
      "Episode: 73  steps: 50\n",
      "Episode: 74  steps: 30\n",
      "Episode: 75  steps: 51\n",
      "Episode: 76  steps: 29\n",
      "Episode: 77  steps: 49\n",
      "Episode: 78  steps: 42\n",
      "Episode: 79  steps: 38\n",
      "Episode: 80  steps: 27\n",
      "Episode: 81  steps: 32\n",
      "Episode: 82  steps: 68\n",
      "Episode: 83  steps: 68\n",
      "Episode: 84  steps: 27\n",
      "Episode: 85  steps: 34\n",
      "Episode: 86  steps: 26\n",
      "Episode: 87  steps: 8\n",
      "Episode: 88  steps: 10\n",
      "Episode: 89  steps: 22\n",
      "Episode: 90  steps: 38\n",
      "Episode: 91  steps: 39\n",
      "Episode: 92  steps: 32\n",
      "Episode: 93  steps: 30\n",
      "Episode: 94  steps: 22\n",
      "Episode: 95  steps: 10\n",
      "Episode: 96  steps: 9\n",
      "Episode: 97  steps: 8\n",
      "Episode: 98  steps: 10\n",
      "Episode: 99  steps: 10\n",
      "Episode: 100  steps: 25\n",
      "Episode: 101  steps: 28\n",
      "Episode: 102  steps: 40\n",
      "Episode: 103  steps: 41\n",
      "Episode: 104  steps: 29\n",
      "Episode: 105  steps: 26\n",
      "Episode: 106  steps: 23\n",
      "Episode: 107  steps: 34\n",
      "Episode: 108  steps: 27\n",
      "Episode: 109  steps: 78\n",
      "Episode: 110  steps: 33\n",
      "Episode: 111  steps: 32\n",
      "Episode: 112  steps: 27\n",
      "Episode: 113  steps: 33\n",
      "Episode: 114  steps: 8\n",
      "Episode: 115  steps: 40\n",
      "Episode: 116  steps: 24\n",
      "Episode: 117  steps: 21\n",
      "Episode: 118  steps: 44\n",
      "Episode: 119  steps: 27\n",
      "Episode: 120  steps: 39\n",
      "Episode: 121  steps: 61\n",
      "Episode: 122  steps: 62\n",
      "Episode: 123  steps: 43\n",
      "Episode: 124  steps: 29\n",
      "Episode: 125  steps: 57\n",
      "Episode: 126  steps: 38\n",
      "Episode: 127  steps: 23\n",
      "Episode: 128  steps: 24\n",
      "Episode: 129  steps: 26\n",
      "Episode: 130  steps: 35\n",
      "Episode: 131  steps: 43\n",
      "Episode: 132  steps: 33\n",
      "Episode: 133  steps: 52\n",
      "Episode: 134  steps: 36\n",
      "Episode: 135  steps: 36\n",
      "Episode: 136  steps: 50\n",
      "Episode: 137  steps: 29\n",
      "Episode: 138  steps: 45\n",
      "Episode: 139  steps: 51\n",
      "Episode: 140  steps: 74\n",
      "Episode: 141  steps: 37\n",
      "Episode: 142  steps: 23\n",
      "Episode: 143  steps: 29\n",
      "Episode: 144  steps: 37\n",
      "Episode: 145  steps: 25\n",
      "Episode: 146  steps: 34\n",
      "Episode: 147  steps: 23\n",
      "Episode: 148  steps: 30\n",
      "Episode: 149  steps: 26\n",
      "Episode: 150  steps: 62\n",
      "Episode: 151  steps: 88\n",
      "Episode: 152  steps: 27\n",
      "Episode: 153  steps: 73\n",
      "Episode: 154  steps: 27\n",
      "Episode: 155  steps: 23\n",
      "Episode: 156  steps: 30\n",
      "Episode: 157  steps: 11\n",
      "Episode: 158  steps: 22\n",
      "Episode: 159  steps: 45\n",
      "Episode: 160  steps: 43\n",
      "Episode: 161  steps: 29\n",
      "Episode: 162  steps: 28\n",
      "Episode: 163  steps: 30\n",
      "Episode: 164  steps: 85\n",
      "Episode: 165  steps: 47\n",
      "Episode: 166  steps: 30\n",
      "Episode: 167  steps: 36\n",
      "Episode: 168  steps: 18\n",
      "Episode: 169  steps: 24\n",
      "Episode: 170  steps: 22\n",
      "Episode: 171  steps: 9\n",
      "Episode: 172  steps: 76\n",
      "Episode: 173  steps: 16\n",
      "Episode: 174  steps: 35\n",
      "Episode: 175  steps: 21\n",
      "Episode: 176  steps: 27\n",
      "Episode: 177  steps: 24\n",
      "Episode: 178  steps: 35\n",
      "Episode: 179  steps: 30\n",
      "Episode: 180  steps: 34\n",
      "Episode: 181  steps: 21\n",
      "Episode: 182  steps: 8\n",
      "Episode: 183  steps: 23\n",
      "Episode: 184  steps: 28\n",
      "Episode: 185  steps: 22\n",
      "Episode: 186  steps: 66\n",
      "Episode: 187  steps: 26\n",
      "Episode: 188  steps: 34\n",
      "Episode: 189  steps: 25\n",
      "Episode: 190  steps: 25\n",
      "Episode: 191  steps: 26\n",
      "Episode: 192  steps: 9\n",
      "Episode: 193  steps: 43\n",
      "Episode: 194  steps: 32\n",
      "Episode: 195  steps: 31\n",
      "Episode: 196  steps: 31\n",
      "Episode: 197  steps: 36\n",
      "Episode: 198  steps: 31\n",
      "Episode: 199  steps: 22\n",
      "Episode: 200  steps: 42\n",
      "Episode: 201  steps: 27\n",
      "Episode: 202  steps: 24\n",
      "Episode: 203  steps: 38\n",
      "Episode: 204  steps: 31\n",
      "Episode: 205  steps: 28\n",
      "Episode: 206  steps: 39\n",
      "Episode: 207  steps: 33\n",
      "Episode: 208  steps: 31\n",
      "Episode: 209  steps: 63\n",
      "Episode: 210  steps: 60\n",
      "Episode: 211  steps: 51\n",
      "Episode: 212  steps: 39\n",
      "Episode: 213  steps: 38\n",
      "Episode: 214  steps: 26\n",
      "Episode: 215  steps: 30\n",
      "Episode: 216  steps: 28\n",
      "Episode: 217  steps: 23\n",
      "Episode: 218  steps: 53\n",
      "Episode: 219  steps: 26\n",
      "Episode: 220  steps: 29\n",
      "Episode: 221  steps: 20\n",
      "Episode: 222  steps: 50\n",
      "Episode: 223  steps: 20\n",
      "Episode: 224  steps: 35\n",
      "Episode: 225  steps: 29\n",
      "Episode: 226  steps: 37\n",
      "Episode: 227  steps: 35\n",
      "Episode: 228  steps: 44\n",
      "Episode: 229  steps: 28\n",
      "Episode: 230  steps: 29\n",
      "Episode: 231  steps: 24\n",
      "Episode: 232  steps: 27\n",
      "Episode: 233  steps: 27\n",
      "Episode: 234  steps: 55\n",
      "Episode: 235  steps: 37\n",
      "Episode: 236  steps: 53\n",
      "Episode: 237  steps: 55\n",
      "Episode: 238  steps: 61\n",
      "Episode: 239  steps: 54\n",
      "Episode: 240  steps: 42\n",
      "Episode: 241  steps: 43\n",
      "Episode: 242  steps: 112\n",
      "Episode: 243  steps: 28\n",
      "Episode: 244  steps: 21\n",
      "Episode: 245  steps: 35\n",
      "Episode: 246  steps: 15\n",
      "Episode: 247  steps: 46\n",
      "Episode: 248  steps: 30\n",
      "Episode: 249  steps: 23\n",
      "Episode: 250  steps: 33\n",
      "Episode: 251  steps: 33\n",
      "Episode: 252  steps: 44\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f388c8ca0780>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Train our network using target and predicted Q values on each episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "This code is based on\n",
    "https://github.com/hunkim/DeepRL-Agents\n",
    "'''\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Constants defining our neural network\n",
    "learning_rate = 1e-1\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, input_size], name = \"input_x\")\n",
    "\n",
    "# First layer of weights\n",
    "W1 = tf.get_variable(\"W1\", shape=[input_size, output_size],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "Qpred = tf.matmul(X, W1)\n",
    "\n",
    "# We need to define the parts of the network needed for learning a policy\n",
    "Y = tf.placeholder(shape=[None, output_size], dtype=tf.float32)\n",
    "\n",
    "# Loss function\n",
    "loss = tf.reduce_sum(tf.square(Y - Qpred))\n",
    "# Learning\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# Values for q learning\n",
    "max_episodes = 5000\n",
    "dis = 0.9\n",
    "step_history = []\n",
    "\n",
    "\n",
    "# Setting up our environment\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    e = 1. / ((episode / 10) + 1)\n",
    "    step_count = 0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # The Q-Network training\n",
    "    while not done:\n",
    "        step_count += 1\n",
    "        x = np.reshape(state, [1, input_size])\n",
    "        # Choose an action by greedily (with e chance of random action) from\n",
    "        # the Q-network\n",
    "        Q = sess.run(Qpred, feed_dict={X: x})\n",
    "        if np.random.rand(1) < e:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q)\n",
    "\n",
    "        # Get new state and reward from environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            Q[0, action] = -100\n",
    "        else:\n",
    "            x_next = np.reshape(next_state, [1, input_size])\n",
    "            # Obtain the Q' values by feeding the new state through our network\n",
    "            Q_next = sess.run(Qpred, feed_dict={X: x_next})\n",
    "            Q[0, action] = reward + dis * np.max(Q_next)\n",
    "\n",
    "        # Train our network using target and predicted Q values on each episode\n",
    "        sess.run(train, feed_dict={X: x, Y: Q})\n",
    "        state = next_state\n",
    "\n",
    "    step_history.append(step_count)\n",
    "    print(\"Episode: {}  steps: {}\".format(episode, step_count))\n",
    "    # If last 10's avg steps are 500, it's good enough\n",
    "    if len(step_history) > 10 and np.mean(step_history[-10:]) > 500:\n",
    "        break\n",
    "\n",
    "# See our trained network in action\n",
    "observation = env.reset()\n",
    "reward_sum = 0\n",
    "while True:\n",
    "    env.render()\n",
    "\n",
    "    x = np.reshape(observation, [1, input_size])\n",
    "    Q = sess.run(Qpred, feed_dict={X: x})\n",
    "    action = np.argmax(Q)\n",
    "\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        print(\"Total score: {}\".format(reward_sum))\n",
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN NIPS2013 TEST\n",
    "* by _sung kim_\n",
    "\n",
    "about cart_pole using DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[Episode     0]  steps:    17 e:  1.00  loss: 0\n",
      "[Episode     1]  steps:     9 e:  0.98  loss: 0\n",
      "[Episode     2]  steps:    21 e:  0.96  loss: 0\n",
      "[Episode     3]  steps:    12 e:  0.94  loss: 0\n",
      "[Episode     4]  steps:     9 e:  0.92  loss: 388.61706543\n",
      "[Episode     5]  steps:    12 e:  0.90  loss: 309.770690918\n",
      "[Episode     6]  steps:    11 e:  0.88  loss: 232.552307129\n",
      "[Episode     7]  steps:    16 e:  0.86  loss: 385.866149902\n",
      "[Episode     8]  steps:    20 e:  0.84  loss: 461.846435547\n",
      "[Episode     9]  steps:    23 e:  0.82  loss: 306.743713379\n",
      "[Episode    10]  steps:    13 e:  0.80  loss: 307.124389648\n",
      "[Episode    11]  steps:    16 e:  0.78  loss: 229.019256592\n",
      "[Episode    12]  steps:    12 e:  0.76  loss: 304.277832031\n",
      "[Episode    13]  steps:    24 e:  0.74  loss: 227.81817627\n",
      "[Episode    14]  steps:    12 e:  0.72  loss: 227.852508545\n",
      "[Episode    15]  steps:    12 e:  0.70  loss: 222.835205078\n",
      "[Episode    16]  steps:     8 e:  0.68  loss: 374.933654785\n",
      "[Episode    17]  steps:    16 e:  0.66  loss: 374.509460449\n",
      "[Episode    18]  steps:    13 e:  0.64  loss: 519.070922852\n",
      "[Episode    19]  steps:    11 e:  0.62  loss: 296.274017334\n",
      "[Episode    20]  steps:    13 e:  0.60  loss: 512.452636719\n",
      "[Episode    21]  steps:     9 e:  0.58  loss: 292.70413208\n",
      "[Episode    22]  steps:     8 e:  0.56  loss: 366.472473145\n",
      "[Episode    23]  steps:    12 e:  0.54  loss: 361.497192383\n",
      "[Episode    24]  steps:    15 e:  0.52  loss: 429.885070801\n",
      "[Episode    25]  steps:    15 e:  0.50  loss: 289.914398193\n",
      "[Episode    26]  steps:    11 e:  0.48  loss: 496.299041748\n",
      "[Episode    27]  steps:    13 e:  0.46  loss: 350.545043945\n",
      "[Episode    28]  steps:    14 e:  0.44  loss: 347.153961182\n",
      "[Episode    29]  steps:    11 e:  0.42  loss: 345.072662354\n",
      "[Episode    30]  steps:    11 e:  0.40  loss: 418.068939209\n",
      "[Episode    31]  steps:    15 e:  0.38  loss: 275.96270752\n",
      "[Episode    32]  steps:     8 e:  0.36  loss: 274.127624512\n",
      "[Episode    33]  steps:    11 e:  0.34  loss: 404.28112793\n",
      "[Episode    34]  steps:    13 e:  0.32  loss: 328.640319824\n",
      "[Episode    35]  steps:    17 e:  0.30  loss: 261.663146973\n",
      "[Episode    36]  steps:    13 e:  0.28  loss: 390.08581543\n",
      "[Episode    37]  steps:    11 e:  0.26  loss: 263.522766113\n",
      "[Episode    38]  steps:    11 e:  0.24  loss: 440.474884033\n",
      "[Episode    39]  steps:     9 e:  0.22  loss: 383.70324707\n",
      "[Episode    40]  steps:    10 e:  0.20  loss: 242.575622559\n",
      "[Episode    41]  steps:    12 e:  0.18  loss: 370.331787109\n",
      "[Episode    42]  steps:    10 e:  0.16  loss: 367.346252441\n",
      "[Episode    43]  steps:    10 e:  0.14  loss: 294.267120361\n",
      "[Episode    44]  steps:    10 e:  0.12  loss: 399.648925781\n",
      "[Episode    45]  steps:    11 e:  0.10  loss: 307.154266357\n",
      "[Episode    46]  steps:     9 e:  0.08  loss: 453.914978027\n",
      "[Episode    47]  steps:    12 e:  0.06  loss: 227.698287964\n",
      "[Episode    48]  steps:     8 e:  0.04  loss: 107.602966309\n",
      "[Episode    49]  steps:     8 e:  0.02  loss: 328.727478027\n",
      "[Episode    50]  steps:    10 e:  0.00  loss: 273.246673584\n",
      "[Episode    51]  steps:    10 e:  0.00  loss: 104.918228149\n",
      "[Episode    52]  steps:     9 e:  0.00  loss: 96.6230621338\n",
      "[Episode    53]  steps:    71 e:  0.00  loss: 227.718963623\n",
      "[Episode    54]  steps:   124 e:  0.00  loss: 101.127716064\n",
      "[Episode    55]  steps:    45 e:  0.00  loss: 139.182632446\n",
      "[Episode    56]  steps:    67 e:  0.00  loss: 112.178924561\n",
      "[Episode    57]  steps:    39 e:  0.00  loss: 146.420501709\n",
      "[Episode    58]  steps:    45 e:  0.00  loss: 102.645774841\n",
      "[Episode    59]  steps:    41 e:  0.00  loss: 124.541732788\n",
      "[Episode    60]  steps:    53 e:  0.00  loss: 71.4624481201\n",
      "[Episode    61]  steps:    30 e:  0.00  loss: 82.0631103516\n",
      "[Episode    62]  steps:    50 e:  0.00  loss: 176.323394775\n",
      "[Episode    63]  steps:    46 e:  0.00  loss: 47.0619430542\n",
      "[Episode    64]  steps:    32 e:  0.00  loss: 41.7422027588\n",
      "[Episode    65]  steps:    28 e:  0.00  loss: 79.575088501\n",
      "[Episode    66]  steps:    44 e:  0.00  loss: 59.4593849182\n",
      "[Episode    67]  steps:    30 e:  0.00  loss: 98.4201889038\n",
      "[Episode    68]  steps:    28 e:  0.00  loss: 92.3119049072\n",
      "[Episode    69]  steps:    30 e:  0.00  loss: 29.7038917542\n",
      "[Episode    70]  steps:    34 e:  0.00  loss: 66.7160339355\n",
      "[Episode    71]  steps:    30 e:  0.00  loss: 16.4808654785\n",
      "[Episode    72]  steps:    35 e:  0.00  loss: 97.695640564\n",
      "[Episode    73]  steps:    36 e:  0.00  loss: 69.1804122925\n",
      "[Episode    74]  steps:    28 e:  0.00  loss: 51.7682228088\n",
      "[Episode    75]  steps:    29 e:  0.00  loss: 176.83241272\n",
      "[Episode    76]  steps:    33 e:  0.00  loss: 75.2195129395\n",
      "[Episode    77]  steps:    35 e:  0.00  loss: 175.53717041\n",
      "[Episode    78]  steps:    26 e:  0.00  loss: 24.6161060333\n",
      "[Episode    79]  steps:    35 e:  0.00  loss: 84.7657089233\n",
      "[Episode    80]  steps:    26 e:  0.00  loss: 1.24377131462\n",
      "[Episode    81]  steps:    54 e:  0.00  loss: 129.140136719\n",
      "[Episode    82]  steps:    49 e:  0.00  loss: 106.081237793\n",
      "[Episode    83]  steps:    49 e:  0.00  loss: 2.1994600296\n",
      "[Episode    84]  steps:    33 e:  0.00  loss: 14.3577260971\n",
      "[Episode    85]  steps:    38 e:  0.00  loss: 85.1593780518\n",
      "[Episode    86]  steps:    48 e:  0.00  loss: 42.63854599\n",
      "[Episode    87]  steps:    36 e:  0.00  loss: 44.129322052\n",
      "[Episode    88]  steps:    35 e:  0.00  loss: 14.4479799271\n",
      "[Episode    89]  steps:    55 e:  0.00  loss: 83.1131439209\n",
      "[Episode    90]  steps:    47 e:  0.00  loss: 15.4214916229\n",
      "[Episode    91]  steps:    28 e:  0.00  loss: 43.3532104492\n",
      "[Episode    92]  steps:   181 e:  0.00  loss: 80.8455963135\n",
      "[Episode    93]  steps:    35 e:  0.00  loss: 96.4036712646\n",
      "[Episode    94]  steps:   170 e:  0.00  loss: 8.69355773926\n",
      "[Episode    95]  steps:    44 e:  0.00  loss: 76.0370254517\n",
      "[Episode    96]  steps:    51 e:  0.00  loss: 2.24245119095\n",
      "[Episode    97]  steps:    41 e:  0.00  loss: 54.8271026611\n",
      "[Episode    98]  steps:    45 e:  0.00  loss: 34.750705719\n",
      "[Episode    99]  steps:    69 e:  0.00  loss: 2.17028856277\n",
      "[Episode   100]  steps:    60 e:  0.00  loss: 30.0774326324\n",
      "[Episode   101]  steps:   153 e:  0.00  loss: 30.7557983398\n",
      "[Episode   102]  steps:   103 e:  0.00  loss: 29.040353775\n",
      "[Episode   103]  steps:   170 e:  0.00  loss: 2.51635670662\n",
      "[Episode   104]  steps:    58 e:  0.00  loss: 40.2124710083\n",
      "[Episode   105]  steps:   146 e:  0.00  loss: 33.6653442383\n",
      "[Episode   106]  steps:    49 e:  0.00  loss: 50.1067428589\n",
      "[Episode   107]  steps:    50 e:  0.00  loss: 4.67712211609\n",
      "[Episode   108]  steps:    46 e:  0.00  loss: 2.41780757904\n",
      "[Episode   109]  steps:    54 e:  0.00  loss: 26.260679245\n",
      "[Episode   110]  steps:    50 e:  0.00  loss: 30.7577381134\n",
      "[Episode   111]  steps:    66 e:  0.00  loss: 27.2036838531\n",
      "[Episode   112]  steps:    47 e:  0.00  loss: 73.6486129761\n",
      "[Episode   113]  steps:    45 e:  0.00  loss: 32.4467353821\n",
      "[Episode   114]  steps:    32 e:  0.00  loss: 26.6050930023\n",
      "[Episode   115]  steps:    39 e:  0.00  loss: 54.2092971802\n",
      "[Episode   116]  steps:    57 e:  0.00  loss: 35.3656806946\n",
      "[Episode   117]  steps:    35 e:  0.00  loss: 27.6147174835\n",
      "[Episode   118]  steps:    84 e:  0.00  loss: 37.7094955444\n",
      "[Episode   119]  steps:    83 e:  0.00  loss: 2.83139944077\n",
      "[Episode   120]  steps:    87 e:  0.00  loss: 12.2986154556\n",
      "[Episode   121]  steps:    39 e:  0.00  loss: 26.663143158\n",
      "[Episode   122]  steps:    90 e:  0.00  loss: 150.259613037\n",
      "[Episode   123]  steps:   111 e:  0.00  loss: 25.6371555328\n",
      "[Episode   124]  steps:   125 e:  0.00  loss: 99.2683410645\n",
      "[Episode   125]  steps:    58 e:  0.00  loss: 46.2319259644\n",
      "[Episode   126]  steps:    29 e:  0.00  loss: 5.36570644379\n",
      "[Episode   127]  steps:    74 e:  0.00  loss: 64.7076721191\n",
      "[Episode   128]  steps:    61 e:  0.00  loss: 21.6365737915\n",
      "[Episode   129]  steps:   200 e:  0.00  loss: 3.49241733551\n",
      "[Episode   130]  steps:    40 e:  0.00  loss: 85.7428131104\n",
      "[Episode   131]  steps:    50 e:  0.00  loss: 39.5964927673\n",
      "[Episode   132]  steps:   134 e:  0.00  loss: 42.4175109863\n",
      "[Episode   133]  steps:   149 e:  0.00  loss: 36.4221801758\n",
      "[Episode   134]  steps:    41 e:  0.00  loss: 11.365858078\n",
      "[Episode   135]  steps:   193 e:  0.00  loss: 41.3326148987\n",
      "[Episode   136]  steps:    48 e:  0.00  loss: 3.76397752762\n",
      "[Episode   137]  steps:    44 e:  0.00  loss: 78.9808807373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode   138]  steps:    44 e:  0.00  loss: 44.8855323792\n",
      "[Episode   139]  steps:   194 e:  0.00  loss: 43.7727394104\n",
      "[Episode   140]  steps:    50 e:  0.00  loss: 78.2951126099\n",
      "[Episode   141]  steps:    58 e:  0.00  loss: 3.40687179565\n",
      "[Episode   142]  steps:    66 e:  0.00  loss: 72.8000640869\n",
      "[Episode   143]  steps:    55 e:  0.00  loss: 5.88366985321\n",
      "[Episode   144]  steps:    99 e:  0.00  loss: 15.7391824722\n",
      "[Episode   145]  steps:    48 e:  0.00  loss: 72.1005859375\n",
      "[Episode   146]  steps:    56 e:  0.00  loss: 47.8727264404\n",
      "[Episode   147]  steps:    71 e:  0.00  loss: 13.906036377\n",
      "[Episode   148]  steps:    81 e:  0.00  loss: 28.3537902832\n",
      "[Episode   149]  steps:    74 e:  0.00  loss: 4.16057872772\n",
      "[Episode   150]  steps:    72 e:  0.00  loss: 4.02708292007\n",
      "[Episode   151]  steps:    64 e:  0.00  loss: 15.1226110458\n",
      "[Episode   152]  steps:    63 e:  0.00  loss: 4.22234773636\n",
      "[Episode   153]  steps:   104 e:  0.00  loss: 22.5192222595\n",
      "[Episode   154]  steps:    92 e:  0.00  loss: 95.7164916992\n",
      "[Episode   155]  steps:   105 e:  0.00  loss: 5.03407239914\n",
      "[Episode   156]  steps:   104 e:  0.00  loss: 3.08832073212\n",
      "[Episode   157]  steps:    52 e:  0.00  loss: 3.63670563698\n",
      "[Episode   158]  steps:   124 e:  0.00  loss: 9.82066345215\n",
      "[Episode   159]  steps:   130 e:  0.00  loss: 6.73787164688\n",
      "[Episode   160]  steps:   142 e:  0.00  loss: 2.37798070908\n",
      "[Episode   161]  steps:   161 e:  0.00  loss: 12.9496622086\n",
      "[Episode   162]  steps:   200 e:  0.00  loss: 12.125295639\n",
      "[Episode   163]  steps:   185 e:  0.00  loss: 29.4439716339\n",
      "[Episode   164]  steps:   200 e:  0.00  loss: 10.7012672424\n",
      "[Episode   165]  steps:   200 e:  0.00  loss: 6.28877878189\n",
      "[Episode   166]  steps:   200 e:  0.00  loss: 23.8741760254\n",
      "[Episode   167]  steps:   195 e:  0.00  loss: 78.6653213501\n",
      "[Episode   168]  steps:   200 e:  0.00  loss: 80.1619110107\n",
      "[Episode   169]  steps:   200 e:  0.00  loss: 29.523481369\n",
      "[Episode   170]  steps:   200 e:  0.00  loss: 3.94971942902\n",
      "[Episode   171]  steps:   200 e:  0.00  loss: 50.4746398926\n",
      "[Episode   172]  steps:   200 e:  0.00  loss: 25.1037902832\n",
      "[Episode   173]  steps:   200 e:  0.00  loss: 42.8907203674\n",
      "[Episode   174]  steps:   159 e:  0.00  loss: 117.113098145\n",
      "[Episode   175]  steps:   173 e:  0.00  loss: 139.403137207\n",
      "[Episode   176]  steps:   187 e:  0.00  loss: 51.3553962708\n",
      "[Episode   177]  steps:   161 e:  0.00  loss: 3.11190891266\n",
      "[Episode   178]  steps:   172 e:  0.00  loss: 18.9213886261\n",
      "[Episode   179]  steps:   157 e:  0.00  loss: 3.30662727356\n",
      "[Episode   180]  steps:   162 e:  0.00  loss: 14.6622714996\n",
      "[Episode   181]  steps:   164 e:  0.00  loss: 2.64129400253\n",
      "[Episode   182]  steps:   188 e:  0.00  loss: 28.2921962738\n",
      "[Episode   183]  steps:   187 e:  0.00  loss: 107.338287354\n",
      "[Episode   184]  steps:   200 e:  0.00  loss: 143.876708984\n",
      "[Episode   185]  steps:   200 e:  0.00  loss: 116.570854187\n",
      "[Episode   186]  steps:   176 e:  0.00  loss: 4.59895610809\n",
      "[Episode   187]  steps:   173 e:  0.00  loss: 2.414955616\n",
      "[Episode   188]  steps:   165 e:  0.00  loss: 161.749130249\n",
      "[Episode   189]  steps:   186 e:  0.00  loss: 47.8665046692\n",
      "[Episode   190]  steps:   162 e:  0.00  loss: 3.34639072418\n",
      "[Episode   191]  steps:   200 e:  0.00  loss: 4.36394643784\n",
      "[Episode   192]  steps:   194 e:  0.00  loss: 3.8680536747\n",
      "[Episode   193]  steps:   188 e:  0.00  loss: 134.505874634\n",
      "[Episode   194]  steps:   200 e:  0.00  loss: 56.2869415283\n",
      "[Episode   195]  steps:   181 e:  0.00  loss: 37.6566505432\n",
      "[Episode   196]  steps:   185 e:  0.00  loss: 107.958145142\n",
      "[Episode   197]  steps:   187 e:  0.00  loss: 55.4411125183\n",
      "[Episode   198]  steps:   162 e:  0.00  loss: 110.803642273\n",
      "[Episode   199]  steps:   200 e:  0.00  loss: 3.67404866219\n",
      "[Episode   200]  steps:   155 e:  0.00  loss: 11.358751297\n",
      "[Episode   201]  steps:   200 e:  0.00  loss: 6.72767019272\n",
      "[Episode   202]  steps:   162 e:  0.00  loss: 46.5269165039\n",
      "[Episode   203]  steps:   158 e:  0.00  loss: 28.5443820953\n",
      "[Episode   204]  steps:   200 e:  0.00  loss: 3.0064406395\n",
      "[Episode   205]  steps:   176 e:  0.00  loss: 4.4631280899\n",
      "[Episode   206]  steps:   177 e:  0.00  loss: 151.248428345\n",
      "[Episode   207]  steps:   196 e:  0.00  loss: 22.7125492096\n",
      "[Episode   208]  steps:   154 e:  0.00  loss: 2.67479014397\n",
      "[Episode   209]  steps:   153 e:  0.00  loss: 3.23328399658\n",
      "[Episode   210]  steps:   157 e:  0.00  loss: 46.5945549011\n",
      "[Episode   211]  steps:   193 e:  0.00  loss: 3.34821033478\n",
      "[Episode   212]  steps:   155 e:  0.00  loss: 50.0719032288\n",
      "[Episode   213]  steps:   161 e:  0.00  loss: 45.7761611938\n",
      "[Episode   214]  steps:   171 e:  0.00  loss: 63.0491600037\n",
      "[Episode   215]  steps:   181 e:  0.00  loss: 4.14102315903\n",
      "[Episode   216]  steps:   180 e:  0.00  loss: 2.67492103577\n",
      "[Episode   217]  steps:   158 e:  0.00  loss: 138.803131104\n",
      "[Episode   218]  steps:   175 e:  0.00  loss: 4.06093025208\n",
      "[Episode   219]  steps:   170 e:  0.00  loss: 2.05846238136\n",
      "[Episode   220]  steps:   163 e:  0.00  loss: 170.619476318\n",
      "[Episode   221]  steps:   173 e:  0.00  loss: 145.144561768\n",
      "[Episode   222]  steps:   179 e:  0.00  loss: 3.94756269455\n",
      "[Episode   223]  steps:   173 e:  0.00  loss: 48.507598877\n",
      "[Episode   224]  steps:   190 e:  0.00  loss: 4.63695335388\n",
      "[Episode   225]  steps:   168 e:  0.00  loss: 2.34006714821\n",
      "[Episode   226]  steps:   181 e:  0.00  loss: 4.00594949722\n",
      "[Episode   227]  steps:   175 e:  0.00  loss: 1.71711242199\n",
      "[Episode   228]  steps:   170 e:  0.00  loss: 55.0784416199\n",
      "[Episode   229]  steps:   179 e:  0.00  loss: 1.59248006344\n",
      "[Episode   230]  steps:   184 e:  0.00  loss: 1.74314117432\n",
      "[Episode   231]  steps:   171 e:  0.00  loss: 2.62299871445\n",
      "[Episode   232]  steps:   179 e:  0.00  loss: 92.926651001\n",
      "[Episode   233]  steps:   200 e:  0.00  loss: 17.0393028259\n",
      "[Episode   234]  steps:   176 e:  0.00  loss: 150.347702026\n",
      "[Episode   235]  steps:   187 e:  0.00  loss: 26.2499713898\n",
      "[Episode   236]  steps:   187 e:  0.00  loss: 2.48211622238\n",
      "[Episode   237]  steps:   173 e:  0.00  loss: 4.88024759293\n",
      "[Episode   238]  steps:   187 e:  0.00  loss: 3.6916821003\n",
      "[Episode   239]  steps:   200 e:  0.00  loss: 4.76905822754\n",
      "[Episode   240]  steps:   183 e:  0.00  loss: 71.3582382202\n",
      "[Episode   241]  steps:   180 e:  0.00  loss: 2.58605909348\n",
      "[Episode   242]  steps:   190 e:  0.00  loss: 128.242904663\n",
      "[Episode   243]  steps:   191 e:  0.00  loss: 6.09032821655\n",
      "[Episode   244]  steps:   177 e:  0.00  loss: 1.66820383072\n",
      "[Episode   245]  steps:   200 e:  0.00  loss: 2.18146467209\n",
      "[Episode   246]  steps:   195 e:  0.00  loss: 116.40763092\n",
      "[Episode   247]  steps:   182 e:  0.00  loss: 74.769821167\n",
      "[Episode   248]  steps:   200 e:  0.00  loss: 49.0691108704\n",
      "[Episode   249]  steps:   189 e:  0.00  loss: 4.49918365479\n",
      "[Episode   250]  steps:   193 e:  0.00  loss: 5.18948554993\n",
      "[Episode   251]  steps:   200 e:  0.00  loss: 17.7945976257\n",
      "[Episode   252]  steps:   200 e:  0.00  loss: 2.37089276314\n",
      "[Episode   253]  steps:   192 e:  0.00  loss: 2.35844564438\n",
      "[Episode   254]  steps:   200 e:  0.00  loss: 4.08210325241\n",
      "[Episode   255]  steps:   200 e:  0.00  loss: 58.3592453003\n",
      "[Episode   256]  steps:   200 e:  0.00  loss: 3.48175382614\n",
      "[Episode   257]  steps:   200 e:  0.00  loss: 3.78219246864\n",
      "[Episode   258]  steps:   200 e:  0.00  loss: 120.860786438\n",
      "[Episode   259]  steps:   200 e:  0.00  loss: 2.70012044907\n",
      "[Episode   260]  steps:   200 e:  0.00  loss: 3.04091119766\n",
      "[Episode   261]  steps:   200 e:  0.00  loss: 3.20128321648\n",
      "[Episode   262]  steps:   200 e:  0.00  loss: 175.604263306\n",
      "[Episode   263]  steps:   200 e:  0.00  loss: 157.989028931\n",
      "[Episode   264]  steps:   200 e:  0.00  loss: 2.96360301971\n",
      "[Episode   265]  steps:   198 e:  0.00  loss: 3.38134121895\n",
      "[Episode   266]  steps:   200 e:  0.00  loss: 4.84665870667\n",
      "[Episode   267]  steps:   200 e:  0.00  loss: 3.01935195923\n",
      "[Episode   268]  steps:   195 e:  0.00  loss: 7.05252027512\n",
      "[Episode   269]  steps:   181 e:  0.00  loss: 5.43270301819\n",
      "[Episode   270]  steps:   200 e:  0.00  loss: 2.98524188995\n",
      "[Episode   271]  steps:   200 e:  0.00  loss: 5.08315467834\n",
      "[Episode   272]  steps:   200 e:  0.00  loss: 3.03259754181\n",
      "[Episode   273]  steps:   200 e:  0.00  loss: 6.36851024628\n",
      "[Episode   274]  steps:   200 e:  0.00  loss: 2.57770204544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode   275]  steps:   200 e:  0.00  loss: 5.87952232361\n",
      "[Episode   276]  steps:   200 e:  0.00  loss: 86.4215240479\n",
      "[Episode   277]  steps:   200 e:  0.00  loss: 3.19387102127\n",
      "[Episode   278]  steps:   200 e:  0.00  loss: 5.60913610458\n",
      "[Episode   279]  steps:   200 e:  0.00  loss: 3.16126680374\n",
      "[Episode   280]  steps:   200 e:  0.00  loss: 2.57448720932\n",
      "[Episode   281]  steps:   200 e:  0.00  loss: 182.597869873\n",
      "[Episode   282]  steps:   200 e:  0.00  loss: 162.962753296\n",
      "[Episode   283]  steps:   200 e:  0.00  loss: 3.49363064766\n",
      "[Episode   284]  steps:   200 e:  0.00  loss: 5.58902454376\n",
      "[Episode   285]  steps:   200 e:  0.00  loss: 2.84147787094\n",
      "[Episode   286]  steps:   200 e:  0.00  loss: 121.47555542\n",
      "[Episode   287]  steps:   200 e:  0.00  loss: 158.991287231\n",
      "[Episode   288]  steps:   200 e:  0.00  loss: 5.2177734375\n",
      "[Episode   289]  steps:   200 e:  0.00  loss: 3.7150478363\n",
      "[Episode   290]  steps:   200 e:  0.00  loss: 1.76796960831\n",
      "[Episode   291]  steps:   200 e:  0.00  loss: 1.72849726677\n",
      "[Episode   292]  steps:   200 e:  0.00  loss: 4.69208335876\n",
      "[Episode   293]  steps:   200 e:  0.00  loss: 3.06239557266\n",
      "[Episode   294]  steps:   200 e:  0.00  loss: 100.851867676\n",
      "[Episode   295]  steps:   200 e:  0.00  loss: 28.5237102509\n",
      "[Episode   296]  steps:   200 e:  0.00  loss: 4.24350166321\n",
      "[Episode   297]  steps:   200 e:  0.00  loss: 6.41453933716\n",
      "[Episode   298]  steps:   200 e:  0.00  loss: 8.58530712128\n",
      "[Episode   299]  steps:   200 e:  0.00  loss: 5.76362514496\n",
      "[Episode   300]  steps:   200 e:  0.00  loss: 4.6477689743\n",
      "[Episode   301]  steps:   200 e:  0.00  loss: 2.56970214844\n",
      "[Episode   302]  steps:   200 e:  0.00  loss: 3.34692692757\n",
      "[Episode   303]  steps:   200 e:  0.00  loss: 31.2612724304\n",
      "[Episode   304]  steps:   200 e:  0.00  loss: 80.8986282349\n",
      "[Episode   305]  steps:   200 e:  0.00  loss: 92.5060958862\n",
      "[Episode   306]  steps:   200 e:  0.00  loss: 189.568023682\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8b1b04e63c96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-8b1b04e63c96>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m                     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/rl_test/dqn.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \"\"\"\n\u001b[1;32m     67\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Qpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_X\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DQN (NIPS 2013)\n",
    "Playing Atari with Deep Reinforcement Learning\n",
    "https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import dqn\n",
    "import gym\n",
    "from collections import deque\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.wrappers.Monitor(env, 'gym-results/', force=True)\n",
    "INPUT_SIZE = env.observation_space.shape[0]\n",
    "OUTPUT_SIZE = env.action_space.n\n",
    "\n",
    "DISCOUNT_RATE = 0.99\n",
    "REPLAY_MEMORY = 50000\n",
    "MAX_EPISODE = 5000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# minimum epsilon for epsilon greedy\n",
    "MIN_E = 0.0\n",
    "# epsilon will be `MIN_E` at `EPSILON_DECAYING_EPISODE`\n",
    "EPSILON_DECAYING_EPISODE = MAX_EPISODE * 0.01\n",
    "\n",
    "\n",
    "def bot_play(mainDQN):\n",
    "    \"\"\"Runs a single episode with rendering and prints a reward\n",
    "    Args:\n",
    "        mainDQN (dqn.DQN): DQN Agentbot_play\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = np.argmax(mainDQN.predict(state))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            print(\"Total score: {}\".format(total_reward))\n",
    "            break\n",
    "\n",
    "\n",
    "def train_minibatch(DQN, train_batch):\n",
    "    \"\"\"Prepare X_batch, y_batch and train them\n",
    "    Recall our loss function is\n",
    "        target = reward + discount * max Q(s',a)\n",
    "                 or reward if done early\n",
    "        Loss function: [target - Q(s, a)]^2\n",
    "    Hence,\n",
    "        X_batch is a state list\n",
    "        y_batch is reward + discount * max Q\n",
    "                   or reward if terminated early\n",
    "    Args:\n",
    "        DQN (dqn.DQN): DQN Agent to train & run\n",
    "        train_batch (list): Minibatch of Replay memory\n",
    "            Eeach element is a tuple of (s, a, r, s', done)\n",
    "    Returns:\n",
    "        loss: Returns a loss\n",
    "    \"\"\"\n",
    "    state_array = np.vstack([x[0] for x in train_batch])\n",
    "    action_array = np.array([x[1] for x in train_batch])\n",
    "    reward_array = np.array([x[2] for x in train_batch])\n",
    "    next_state_array = np.vstack([x[3] for x in train_batch])\n",
    "    done_array = np.array([x[4] for x in train_batch])\n",
    "\n",
    "    X_batch = state_array\n",
    "    y_batch = DQN.predict(state_array) #training by Neural Net\n",
    "\n",
    "    Q_target = reward_array + DISCOUNT_RATE * np.max(DQN.predict(next_state_array), axis=1) * ~done_array\n",
    "    y_batch[np.arange(len(X_batch)), action_array] = Q_target\n",
    "\n",
    "    # Train our network using target and predicted Q values on each episode\n",
    "    loss, _ = DQN.update(X_batch, y_batch)\n",
    "   \n",
    "    return loss\n",
    "\n",
    "\n",
    "def annealing_epsilon(episode, min_e, max_e, target_episode):\n",
    "    \"\"\"Return an linearly annealed epsilon\n",
    "    Epsilon will decrease over time until it reaches `target_episode`\n",
    "         (epsilon)\n",
    "             |\n",
    "    max_e ---|\\\n",
    "             | \\\n",
    "             |  \\\n",
    "             |   \\\n",
    "    min_e ---|____\\_______________(episode)\n",
    "                  |\n",
    "                 target_episode\n",
    "     slope = (min_e - max_e) / (target_episode)\n",
    "     intercept = max_e\n",
    "     e = slope * episode + intercept\n",
    "    Args:\n",
    "        episode (int): Current episode\n",
    "        min_e (float): Minimum epsilon\n",
    "        max_e (float): Maximum epsilon\n",
    "        target_episode (int): epsilon becomes the `min_e` at `target_episode`\n",
    "    Returns:\n",
    "        float: epsilon between `min_e` and `max_e`\n",
    "    \"\"\"\n",
    "\n",
    "    slope = (min_e - max_e) / (target_episode)\n",
    "    intercept = max_e\n",
    "\n",
    "    return max(min_e, slope * episode + intercept)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # store the previous observations in replay memory\n",
    "    replay_buffer = deque(maxlen=REPLAY_MEMORY) \n",
    "    last_100_game_reward = deque(maxlen=100)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        mainDQN = dqn.DQN(sess, INPUT_SIZE, OUTPUT_SIZE)\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        loss = 0\n",
    "\n",
    "        for episode in range(MAX_EPISODE):\n",
    "            e = annealing_epsilon(episode, MIN_E, 1.0, EPSILON_DECAYING_EPISODE)\n",
    "            done = False\n",
    "            state = env.reset()            \n",
    "\n",
    "            step_count = 0\n",
    "            while not done:\n",
    "\n",
    "                if np.random.rand() < e:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = np.argmax(mainDQN.predict(state))\n",
    "\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                if done:\n",
    "                    reward = -100\n",
    "\n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "                state = next_state\n",
    "                step_count += 1\n",
    "\n",
    "                if len(replay_buffer) > BATCH_SIZE:\n",
    "                    minibatch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "                    loss = train_minibatch(mainDQN, minibatch)\n",
    "\n",
    "            print(\"[Episode {:>5}]  steps: {:>5} e: {:>5.2f}  loss: {}\".format(episode, step_count, e, loss))\n",
    "            \n",
    "\n",
    "            # CartPole-v0 Game Clear Logic\n",
    "            last_100_game_reward.append(step_count)\n",
    "            if len(last_100_game_reward) == last_100_game_reward.maxlen:\n",
    "                avg_reward = np.mean(last_100_game_reward)\n",
    "                if avg_reward > 199.0:\n",
    "                    print(\"Game Cleared within {} episodes with avg reward {}\".format(episode, avg_reward))\n",
    "                    break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN 2015 Nature\n",
    "* Made by Sung Kim\n",
    "* Revised by Hotae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode: 0  steps: 16\n",
      "Episode: 1  steps: 44\n",
      "Episode: 2  steps: 20\n",
      "Episode: 3  steps: 19\n",
      "Episode: 4  steps: 14\n",
      "Episode: 5  steps: 33\n",
      "Episode: 6  steps: 19\n",
      "Episode: 7  steps: 26\n",
      "Episode: 8  steps: 12\n",
      "Episode: 9  steps: 13\n",
      "Episode: 10  steps: 17\n",
      "Episode: 11  steps: 10\n",
      "Episode: 12  steps: 18\n",
      "Episode: 13  steps: 9\n",
      "Episode: 14  steps: 16\n",
      "Episode: 15  steps: 10\n",
      "Episode: 16  steps: 18\n",
      "Episode: 17  steps: 10\n",
      "Episode: 18  steps: 15\n",
      "Episode: 19  steps: 11\n",
      "Episode: 20  steps: 11\n",
      "Episode: 21  steps: 9\n",
      "Episode: 22  steps: 12\n",
      "Episode: 23  steps: 14\n",
      "Episode: 24  steps: 13\n",
      "Episode: 25  steps: 9\n",
      "Episode: 26  steps: 13\n",
      "Episode: 27  steps: 8\n",
      "Episode: 28  steps: 8\n",
      "Episode: 29  steps: 10\n",
      "Episode: 30  steps: 17\n",
      "Episode: 31  steps: 12\n",
      "Episode: 32  steps: 9\n",
      "Episode: 33  steps: 11\n",
      "Episode: 34  steps: 14\n",
      "Episode: 35  steps: 11\n",
      "Episode: 36  steps: 14\n",
      "Episode: 37  steps: 9\n",
      "Episode: 38  steps: 13\n",
      "Episode: 39  steps: 8\n",
      "Episode: 40  steps: 8\n",
      "Episode: 41  steps: 10\n",
      "Episode: 42  steps: 10\n",
      "Episode: 43  steps: 8\n",
      "Episode: 44  steps: 9\n",
      "Episode: 45  steps: 9\n",
      "Episode: 46  steps: 15\n",
      "Episode: 47  steps: 12\n",
      "Episode: 48  steps: 8\n",
      "Episode: 49  steps: 14\n",
      "Episode: 50  steps: 10\n",
      "Episode: 51  steps: 9\n",
      "Episode: 52  steps: 11\n",
      "Episode: 53  steps: 12\n",
      "Episode: 54  steps: 12\n",
      "Episode: 55  steps: 12\n",
      "Episode: 56  steps: 11\n",
      "Episode: 57  steps: 12\n",
      "Episode: 58  steps: 16\n",
      "Episode: 59  steps: 16\n",
      "Episode: 60  steps: 23\n",
      "Episode: 61  steps: 23\n",
      "Episode: 62  steps: 28\n",
      "Episode: 63  steps: 11\n",
      "Episode: 64  steps: 17\n",
      "Episode: 65  steps: 12\n",
      "Episode: 66  steps: 20\n",
      "Episode: 67  steps: 9\n",
      "Episode: 68  steps: 11\n",
      "Episode: 69  steps: 24\n",
      "Episode: 70  steps: 48\n",
      "Episode: 71  steps: 38\n",
      "Episode: 72  steps: 32\n",
      "Episode: 73  steps: 48\n",
      "Episode: 74  steps: 32\n",
      "Episode: 75  steps: 32\n",
      "Episode: 76  steps: 42\n",
      "Episode: 77  steps: 81\n",
      "Episode: 78  steps: 62\n",
      "Episode: 79  steps: 44\n",
      "Episode: 80  steps: 67\n",
      "Episode: 81  steps: 79\n",
      "Episode: 82  steps: 31\n",
      "Episode: 83  steps: 38\n",
      "Episode: 84  steps: 36\n",
      "Episode: 85  steps: 35\n",
      "Episode: 86  steps: 26\n",
      "Episode: 87  steps: 41\n",
      "Episode: 88  steps: 44\n",
      "Episode: 89  steps: 36\n",
      "Episode: 90  steps: 38\n",
      "Episode: 91  steps: 40\n",
      "Episode: 92  steps: 62\n",
      "Episode: 93  steps: 30\n",
      "Episode: 94  steps: 28\n",
      "Episode: 95  steps: 43\n",
      "Episode: 96  steps: 24\n",
      "Episode: 97  steps: 30\n",
      "Episode: 98  steps: 39\n",
      "Episode: 99  steps: 36\n",
      "Episode: 100  steps: 31\n",
      "Episode: 101  steps: 25\n",
      "Episode: 102  steps: 34\n",
      "Episode: 103  steps: 26\n",
      "Episode: 104  steps: 25\n",
      "Episode: 105  steps: 85\n",
      "Episode: 106  steps: 54\n",
      "Episode: 107  steps: 153\n",
      "Episode: 108  steps: 76\n",
      "Episode: 109  steps: 47\n",
      "Episode: 110  steps: 107\n",
      "Episode: 111  steps: 85\n",
      "Episode: 112  steps: 51\n",
      "Episode: 113  steps: 64\n",
      "Episode: 114  steps: 31\n",
      "Episode: 115  steps: 76\n",
      "Episode: 116  steps: 115\n",
      "Episode: 117  steps: 79\n",
      "Episode: 118  steps: 55\n",
      "Episode: 119  steps: 200\n",
      "Episode: 120  steps: 196\n",
      "Episode: 121  steps: 147\n",
      "Episode: 122  steps: 130\n",
      "Episode: 123  steps: 174\n",
      "Episode: 124  steps: 53\n",
      "Episode: 125  steps: 107\n",
      "Episode: 126  steps: 49\n",
      "Episode: 127  steps: 200\n",
      "Episode: 128  steps: 200\n",
      "Episode: 129  steps: 200\n",
      "Episode: 130  steps: 62\n",
      "Episode: 131  steps: 200\n",
      "Episode: 132  steps: 200\n",
      "Episode: 133  steps: 116\n",
      "Episode: 134  steps: 200\n",
      "Episode: 135  steps: 200\n",
      "Episode: 136  steps: 200\n",
      "Episode: 137  steps: 200\n",
      "Episode: 138  steps: 200\n",
      "Episode: 139  steps: 200\n",
      "Episode: 140  steps: 200\n",
      "Episode: 141  steps: 200\n",
      "Episode: 142  steps: 200\n",
      "Episode: 143  steps: 200\n",
      "Episode: 144  steps: 200\n",
      "Episode: 145  steps: 200\n",
      "Episode: 146  steps: 200\n",
      "Episode: 147  steps: 200\n",
      "Episode: 148  steps: 200\n",
      "Episode: 149  steps: 200\n",
      "Episode: 150  steps: 200\n",
      "Episode: 151  steps: 200\n",
      "Episode: 152  steps: 200\n",
      "Episode: 153  steps: 200\n",
      "Episode: 154  steps: 200\n",
      "Episode: 155  steps: 200\n",
      "Episode: 156  steps: 200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bdf6977f521d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-bdf6977f521d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0;31m# Choose an action by greedily from the Q-network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# Get new state and reward from environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/rl_test/dqn.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \"\"\"\n\u001b[1;32m     67\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Qpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_X\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m           if (not is_tensor_handle_feed and\n\u001b[0;32m-> 1107\u001b[0;31m               not subfeed_t.get_shape().is_compatible_with(np_val.shape)):\n\u001b[0m\u001b[1;32m   1108\u001b[0m             raise ValueError('Cannot feed value of shape %r for Tensor %r, '\n\u001b[1;32m   1109\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.pyc\u001b[0m in \u001b[0;36mis_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \"\"\"\n\u001b[0;32m--> 822\u001b[0;31m     \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.pyc\u001b[0m in \u001b[0;36mas_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m   \u001b[0;34m\"\"\"Converts the given object to a TensorShape.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Double DQN (Nature 2015)\n",
    "http://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf\n",
    "Notes:\n",
    "    The difference is that now there are two DQNs (DQN & Target DQN)\n",
    "    y_i = r_i + 𝛾 * max(Q(next_state, action; 𝜃_target))\n",
    "    Loss: (y_i - Q(state, action; 𝜃))^2\n",
    "    Every C step, 𝜃_target <- 𝜃\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque\n",
    "import dqn\n",
    "\n",
    "import gym\n",
    "from typing import List\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.wrappers.Monitor(env, directory=\"gym-results/\", force=True)\n",
    "\n",
    "# Constants defining our neural network\n",
    "INPUT_SIZE = env.observation_space.shape[0]\n",
    "OUTPUT_SIZE = env.action_space.n\n",
    "\n",
    "DISCOUNT_RATE = 0.99\n",
    "REPLAY_MEMORY = 50000\n",
    "BATCH_SIZE = 64\n",
    "TARGET_UPDATE_FREQUENCY = 5\n",
    "MAX_EPISODES = 50000\n",
    "\n",
    "\n",
    "def replay_train(mainDQN, targetDQN, train_batch):\n",
    "    \"\"\"Trains `mainDQN` with target Q values given by `targetDQN`\n",
    "    Args:\n",
    "        mainDQN (dqn.DQN): Main DQN that will be trained\n",
    "        targetDQN (dqn.DQN): Target DQN that will predict Q_target\n",
    "        train_batch (list): Minibatch of replay memory\n",
    "            Each element is (s, a, r, s', done)\n",
    "            [(state, action, reward, next_state, done), ...]\n",
    "    Returns:\n",
    "        float: After updating `mainDQN`, it returns a `loss`\n",
    "    \"\"\"\n",
    "    states = np.vstack([x[0] for x in train_batch])\n",
    "    actions = np.array([x[1] for x in train_batch])\n",
    "    rewards = np.array([x[2] for x in train_batch])\n",
    "    next_states = np.vstack([x[3] for x in train_batch])\n",
    "    done = np.array([x[4] for x in train_batch])\n",
    "\n",
    "    X = states\n",
    "\n",
    "    Q_target = rewards + DISCOUNT_RATE * np.max(targetDQN.predict(next_states), axis=1) * ~done\n",
    "\n",
    "    y = mainDQN.predict(states)\n",
    "    y[np.arange(len(X)), actions] = Q_target\n",
    "\n",
    "    # Train our network using target and predicted Q values on each episode\n",
    "    return mainDQN.update(X, y)\n",
    "\n",
    "\n",
    "def get_copy_var_ops(dest_scope_name, src_scope_name):\n",
    "    \"\"\"Creates TF operations that copy weights from `src_scope` to `dest_scope`\n",
    "    Args:\n",
    "        dest_scope_name (str): Destination weights (copy to)\n",
    "        src_scope_name (str): Source weight (copy from)\n",
    "    Returns:\n",
    "        List[tf.Operation]: Update operations are created and returned\n",
    "    \"\"\"\n",
    "    # Copy variables src_scope to dest_scope\n",
    "    op_holder = []\n",
    "\n",
    "    src_vars = tf.get_collection(\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES, scope=src_scope_name)\n",
    "    dest_vars = tf.get_collection(\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES, scope=dest_scope_name)\n",
    "\n",
    "    for src_var, dest_var in zip(src_vars, dest_vars):\n",
    "        op_holder.append(dest_var.assign(src_var.value()))\n",
    "\n",
    "    return op_holder\n",
    "\n",
    "\n",
    "def bot_play(mainDQN, env):\n",
    "    \"\"\"Test runs with rendering and prints the total score\n",
    "    Args:\n",
    "        mainDQN (dqn.DQN): DQN agent to run a test\n",
    "        env (gym.Env): Gym Environment\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    reward_sum = 0\n",
    "\n",
    "    while True:\n",
    "\n",
    "        env.render()\n",
    "        action = np.argmax(mainDQN.predict(state))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        if done:\n",
    "            print(\"Total score: {}\".format(reward_sum))\n",
    "            break\n",
    "\n",
    "\n",
    "def main():\n",
    "    # store the previous observations in replay memory\n",
    "    replay_buffer = deque(maxlen=REPLAY_MEMORY)\n",
    "\n",
    "    last_100_game_reward = deque(maxlen=100)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        mainDQN = dqn.DQN(sess, INPUT_SIZE, OUTPUT_SIZE, name=\"main\")\n",
    "        targetDQN = dqn.DQN(sess, INPUT_SIZE, OUTPUT_SIZE, name=\"target\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # initial copy q_net -> target_net\n",
    "        copy_ops = get_copy_var_ops(dest_scope_name=\"target\",\n",
    "                                    src_scope_name=\"main\")\n",
    "        sess.run(copy_ops)\n",
    "\n",
    "        for episode in range(MAX_EPISODES):\n",
    "            e = 1. / ((episode / 10) + 1)\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            state = env.reset()\n",
    "\n",
    "            while not done:\n",
    "                if np.random.rand() < e:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    # Choose an action by greedily from the Q-network\n",
    "                    action = np.argmax(mainDQN.predict(state))\n",
    "\n",
    "                # Get new state and reward from environment\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                if done:  # Penalty\n",
    "                    reward = -1\n",
    "\n",
    "                # Save the experience to our buffer\n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "                if len(replay_buffer) > BATCH_SIZE:\n",
    "                    minibatch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "                    loss, _ = replay_train(mainDQN, targetDQN, minibatch)\n",
    "\n",
    "                if step_count % TARGET_UPDATE_FREQUENCY == 0:\n",
    "                    sess.run(copy_ops)\n",
    "\n",
    "                state = next_state\n",
    "                step_count += 1\n",
    "\n",
    "            print(\"Episode: {}  steps: {}\".format(episode, step_count))\n",
    "\n",
    "            # CartPole-v0 Game Clear Checking Logic\n",
    "            last_100_game_reward.append(step_count)\n",
    "\n",
    "            if len(last_100_game_reward) == last_100_game_reward.maxlen:\n",
    "                avg_reward = np.mean(last_100_game_reward)\n",
    "\n",
    "                if avg_reward > 199:\n",
    "                    print(\"Game Cleared in {episode} episodes with avg reward {avg_reward}\")\n",
    "                    break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy gradient - cartpole\n",
    "* by Sung KIm\n",
    "* revised Hotae, logistic classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[Episode     0] Reward: 13.0 Loss:    0.60864\n",
      "[Episode     1] Reward: 21.0 Loss:     2.0819\n",
      "[Episode     2] Reward:  9.0 Loss:   -0.88321\n",
      "[Episode     3] Reward: 13.0 Loss:   -0.29381\n",
      "[Episode     4] Reward: 55.0 Loss:    0.50959\n",
      "[Episode     5] Reward: 63.0 Loss:   -0.51001\n",
      "[Episode     6] Reward: 11.0 Loss:   -0.13696\n",
      "[Episode     7] Reward: 21.0 Loss:     -1.075\n",
      "[Episode     8] Reward: 18.0 Loss:   -0.30048\n",
      "[Episode     9] Reward: 20.0 Loss:   -0.49842\n",
      "[Episode    10] Reward: 33.0 Loss:     1.3165\n",
      "[Episode    11] Reward: 14.0 Loss:    0.99391\n",
      "[Episode    12] Reward: 12.0 Loss:    0.69467\n",
      "[Episode    13] Reward: 14.0 Loss:    -0.3432\n",
      "[Episode    14] Reward: 11.0 Loss:    0.98703\n",
      "[Episode    15] Reward: 44.0 Loss:    0.35846\n",
      "[Episode    16] Reward: 51.0 Loss:     1.1509\n",
      "[Episode    17] Reward: 11.0 Loss:    0.16704\n",
      "[Episode    18] Reward: 12.0 Loss:    0.75571\n",
      "[Episode    19] Reward: 31.0 Loss:   -0.59869\n",
      "[Episode    20] Reward: 13.0 Loss:   -0.28639\n",
      "[Episode    21] Reward: 11.0 Loss:    0.11176\n",
      "[Episode    22] Reward: 14.0 Loss:   -0.22324\n",
      "[Episode    23] Reward: 25.0 Loss:    0.93837\n",
      "[Episode    24] Reward: 11.0 Loss:     1.9565\n",
      "[Episode    25] Reward: 41.0 Loss:   -0.52439\n",
      "[Episode    26] Reward: 27.0 Loss:     0.5838\n",
      "[Episode    27] Reward: 13.0 Loss:   -0.38735\n",
      "[Episode    28] Reward: 21.0 Loss:    0.34701\n",
      "[Episode    29] Reward: 20.0 Loss:   -0.13592\n",
      "[Episode    30] Reward: 26.0 Loss:   0.029688\n",
      "[Episode    31] Reward: 21.0 Loss:     1.1387\n",
      "[Episode    32] Reward: 37.0 Loss:    0.42123\n",
      "[Episode    33] Reward: 22.0 Loss:    0.77763\n",
      "[Episode    34] Reward: 18.0 Loss:  -0.016177\n",
      "[Episode    35] Reward: 19.0 Loss:    0.73739\n",
      "[Episode    36] Reward: 26.0 Loss:    0.91943\n",
      "[Episode    37] Reward: 16.0 Loss:   -0.21661\n",
      "[Episode    38] Reward: 14.0 Loss:   -0.19652\n",
      "[Episode    39] Reward: 30.0 Loss:   0.094099\n",
      "[Episode    40] Reward: 30.0 Loss:   -0.39061\n",
      "[Episode    41] Reward: 18.0 Loss:  -0.049321\n",
      "[Episode    42] Reward: 32.0 Loss:   -0.26643\n",
      "[Episode    43] Reward: 32.0 Loss:   -0.50275\n",
      "[Episode    44] Reward: 28.0 Loss:    0.22925\n",
      "[Episode    45] Reward: 23.0 Loss:   -0.12492\n",
      "[Episode    46] Reward: 13.0 Loss:     1.0095\n",
      "[Episode    47] Reward: 16.0 Loss:  0.0034029\n",
      "[Episode    48] Reward: 39.0 Loss:   -0.32242\n",
      "[Episode    49] Reward: 12.0 Loss:  -0.063647\n",
      "[Episode    50] Reward: 28.0 Loss:     0.7546\n",
      "[Episode    51] Reward: 17.0 Loss:   -0.01877\n",
      "[Episode    52] Reward: 17.0 Loss:    0.33006\n",
      "[Episode    53] Reward: 20.0 Loss:   0.067014\n",
      "[Episode    54] Reward: 41.0 Loss:    -1.4623\n",
      "[Episode    55] Reward: 22.0 Loss:   0.046865\n",
      "[Episode    56] Reward: 13.0 Loss:    0.09867\n",
      "[Episode    57] Reward: 142.0 Loss:   -0.23395\n",
      "[Episode    58] Reward: 42.0 Loss:   0.044139\n",
      "[Episode    59] Reward: 44.0 Loss:   -0.98909\n",
      "[Episode    60] Reward: 27.0 Loss:    -2.3236\n",
      "[Episode    61] Reward: 30.0 Loss:    0.11867\n",
      "[Episode    62] Reward: 49.0 Loss:   -0.41675\n",
      "[Episode    63] Reward: 13.0 Loss:  -0.060206\n",
      "[Episode    64] Reward: 30.0 Loss:   -0.18612\n",
      "[Episode    65] Reward: 14.0 Loss:  0.0091887\n",
      "[Episode    66] Reward: 57.0 Loss:   -0.70738\n",
      "[Episode    67] Reward: 47.0 Loss:    -1.0445\n",
      "[Episode    68] Reward: 24.0 Loss:    -2.5143\n",
      "[Episode    69] Reward: 29.0 Loss:   -0.20105\n",
      "[Episode    70] Reward: 14.0 Loss:   0.088086\n",
      "[Episode    71] Reward: 39.0 Loss:    0.47532\n",
      "[Episode    72] Reward: 86.0 Loss:    -2.6407\n",
      "[Episode    73] Reward: 51.0 Loss:     0.2153\n",
      "[Episode    74] Reward: 66.0 Loss:   -0.55062\n",
      "[Episode    75] Reward: 72.0 Loss:     1.4068\n",
      "[Episode    76] Reward: 54.0 Loss:    0.84872\n",
      "[Episode    77] Reward: 34.0 Loss:     -1.502\n",
      "[Episode    78] Reward: 44.0 Loss:   -0.14526\n",
      "[Episode    79] Reward: 25.0 Loss:   0.037046\n",
      "[Episode    80] Reward: 44.0 Loss:    0.39408\n",
      "[Episode    81] Reward: 89.0 Loss:    0.87354\n",
      "[Episode    82] Reward: 67.0 Loss:     0.6836\n",
      "[Episode    83] Reward: 24.0 Loss:   -0.30477\n",
      "[Episode    84] Reward: 28.0 Loss:   -0.33958\n",
      "[Episode    85] Reward: 70.0 Loss:    -3.3876\n",
      "[Episode    86] Reward: 19.0 Loss:     1.6192\n",
      "[Episode    87] Reward: 64.0 Loss:   -0.57433\n",
      "[Episode    88] Reward: 29.0 Loss:   -0.36058\n",
      "[Episode    89] Reward: 14.0 Loss:    -0.9324\n",
      "[Episode    90] Reward: 34.0 Loss:    -1.0288\n",
      "[Episode    91] Reward: 85.0 Loss:    -4.4498\n",
      "[Episode    92] Reward: 157.0 Loss:     3.5297\n",
      "[Episode    93] Reward: 47.0 Loss:    0.86689\n",
      "[Episode    94] Reward: 68.0 Loss:    0.53727\n",
      "[Episode    95] Reward: 37.0 Loss:   -0.79644\n",
      "[Episode    96] Reward: 32.0 Loss:    0.19718\n",
      "[Episode    97] Reward: 37.0 Loss:    -0.9725\n",
      "[Episode    98] Reward: 49.0 Loss:       1.02\n",
      "[Episode    99] Reward: 21.0 Loss:    -1.6452\n",
      "[Episode   100] Reward: 29.0 Loss:    -1.4042\n",
      "[Episode   101] Reward: 42.0 Loss:     1.2698\n",
      "[Episode   102] Reward: 82.0 Loss:     4.1644\n",
      "[Episode   103] Reward: 59.0 Loss:   -0.84099\n",
      "[Episode   104] Reward: 58.0 Loss:   -0.45244\n",
      "[Episode   105] Reward: 79.0 Loss:    -1.5215\n",
      "[Episode   106] Reward: 39.0 Loss:    -3.9586\n",
      "[Episode   107] Reward: 15.0 Loss:    0.45966\n",
      "[Episode   108] Reward: 17.0 Loss:    0.96966\n",
      "[Episode   109] Reward: 166.0 Loss:     -1.026\n",
      "[Episode   110] Reward: 73.0 Loss:     1.1272\n",
      "[Episode   111] Reward: 74.0 Loss: -0.0097425\n",
      "[Episode   112] Reward: 39.0 Loss:    0.54327\n",
      "[Episode   113] Reward: 110.0 Loss:     -2.513\n",
      "[Episode   114] Reward: 70.0 Loss:    -2.7411\n",
      "[Episode   115] Reward: 184.0 Loss:   -0.22054\n",
      "[Episode   116] Reward: 47.0 Loss:    0.27831\n",
      "[Episode   117] Reward: 59.0 Loss:    -1.5699\n",
      "[Episode   118] Reward: 79.0 Loss:     2.0112\n",
      "[Episode   119] Reward: 90.0 Loss:   -0.34555\n",
      "[Episode   120] Reward: 72.0 Loss:     1.1913\n",
      "[Episode   121] Reward: 83.0 Loss:   -0.81514\n",
      "[Episode   122] Reward: 90.0 Loss:    0.28941\n",
      "[Episode   123] Reward: 55.0 Loss:   -0.26958\n",
      "[Episode   124] Reward: 92.0 Loss:   -0.73711\n",
      "[Episode   125] Reward: 102.0 Loss:    -2.2724\n",
      "[Episode   126] Reward: 129.0 Loss:    0.98968\n",
      "[Episode   127] Reward: 108.0 Loss:    -2.2703\n",
      "[Episode   128] Reward: 107.0 Loss:     -3.235\n",
      "[Episode   129] Reward: 70.0 Loss:    -2.8212\n",
      "[Episode   130] Reward: 147.0 Loss:     2.0055\n",
      "[Episode   131] Reward: 94.0 Loss:    -2.7137\n",
      "[Episode   132] Reward: 104.0 Loss:    -3.6149\n",
      "[Episode   133] Reward: 35.0 Loss:     1.5514\n",
      "[Episode   134] Reward: 41.0 Loss:   -0.64563\n",
      "[Episode   135] Reward: 61.0 Loss:     1.8414\n",
      "[Episode   136] Reward: 36.0 Loss:  -0.055276\n",
      "[Episode   137] Reward: 126.0 Loss:   -0.38001\n",
      "[Episode   138] Reward: 113.0 Loss:    -5.6867\n",
      "[Episode   139] Reward: 95.0 Loss:    -1.2922\n",
      "[Episode   140] Reward: 42.0 Loss:    -1.0541\n",
      "[Episode   141] Reward: 200.0 Loss:     2.4268\n",
      "[Episode   142] Reward: 185.0 Loss:    -2.4322\n",
      "[Episode   143] Reward: 152.0 Loss:    0.85668\n",
      "[Episode   144] Reward: 92.0 Loss:    0.54619\n",
      "[Episode   145] Reward: 131.0 Loss:    -1.6014\n",
      "[Episode   146] Reward: 136.0 Loss:   -0.80686\n",
      "[Episode   147] Reward: 120.0 Loss:    -1.5749\n",
      "[Episode   148] Reward: 61.0 Loss:    -0.2386\n",
      "[Episode   149] Reward: 115.0 Loss:     1.2187\n",
      "[Episode   150] Reward: 149.0 Loss:    -1.5939\n",
      "[Episode   151] Reward: 84.0 Loss:    -1.4636\n",
      "[Episode   152] Reward: 64.0 Loss:     1.6269\n",
      "[Episode   153] Reward: 91.0 Loss:   -0.12398\n",
      "[Episode   154] Reward: 150.0 Loss:   -0.14096\n",
      "[Episode   155] Reward: 100.0 Loss:   -0.21873\n",
      "[Episode   156] Reward: 98.0 Loss:     2.6766\n",
      "[Episode   157] Reward: 167.0 Loss:     1.3534\n",
      "[Episode   158] Reward: 180.0 Loss:   -0.89085\n",
      "[Episode   159] Reward: 200.0 Loss:     2.6081\n",
      "[Episode   160] Reward: 125.0 Loss:     1.3086\n",
      "[Episode   161] Reward: 154.0 Loss:    -2.5247\n",
      "[Episode   162] Reward: 197.0 Loss:     3.0227\n",
      "[Episode   163] Reward: 107.0 Loss:    -7.2642\n",
      "[Episode   164] Reward: 119.0 Loss:    -7.0043\n",
      "[Episode   165] Reward: 159.0 Loss:     6.0137\n",
      "[Episode   166] Reward: 200.0 Loss:     1.8648\n",
      "[Episode   167] Reward: 173.0 Loss:    -3.9627\n",
      "[Episode   168] Reward: 200.0 Loss:   -0.67355\n",
      "[Episode   169] Reward: 49.0 Loss:    -0.5164\n",
      "[Episode   170] Reward: 147.0 Loss:    -5.8238\n",
      "[Episode   171] Reward: 58.0 Loss:     1.4405\n",
      "[Episode   172] Reward: 197.0 Loss:     5.1937\n",
      "[Episode   173] Reward: 163.0 Loss:   -0.69203\n",
      "[Episode   174] Reward: 107.0 Loss:     1.0991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode   175] Reward: 200.0 Loss:    0.39511\n",
      "[Episode   176] Reward: 88.0 Loss:      5.993\n",
      "[Episode   177] Reward: 138.0 Loss:      -2.44\n",
      "[Episode   178] Reward: 74.0 Loss:   -0.66181\n",
      "[Episode   179] Reward: 114.0 Loss:     3.7968\n",
      "[Episode   180] Reward: 78.0 Loss:     2.7774\n",
      "[Episode   181] Reward: 176.0 Loss:     2.1454\n",
      "[Episode   182] Reward: 139.0 Loss:    0.42752\n",
      "[Episode   183] Reward: 200.0 Loss:    -5.3246\n",
      "[Episode   184] Reward: 69.0 Loss:      6.814\n",
      "[Episode   185] Reward: 200.0 Loss:    -1.1385\n",
      "[Episode   186] Reward: 81.0 Loss:    -1.8012\n",
      "[Episode   187] Reward: 144.0 Loss:     -3.799\n",
      "[Episode   188] Reward: 92.0 Loss:   -0.34131\n",
      "[Episode   189] Reward: 200.0 Loss:    -2.9547\n",
      "[Episode   190] Reward: 200.0 Loss:    -2.6662\n",
      "[Episode   191] Reward: 119.0 Loss:     3.5621\n",
      "[Episode   192] Reward: 54.0 Loss:     3.1151\n",
      "[Episode   193] Reward: 109.0 Loss:     3.2466\n",
      "[Episode   194] Reward: 128.0 Loss:     1.2536\n",
      "[Episode   195] Reward: 180.0 Loss:     6.4702\n",
      "[Episode   196] Reward: 103.0 Loss:     2.5601\n",
      "[Episode   197] Reward: 200.0 Loss:     7.6012\n",
      "[Episode   198] Reward: 83.0 Loss:    0.91261\n",
      "[Episode   199] Reward: 200.0 Loss:   -0.15339\n",
      "[Episode   200] Reward: 200.0 Loss:     5.8998\n",
      "[Episode   201] Reward: 168.0 Loss:    -6.9029\n",
      "[Episode   202] Reward: 103.0 Loss:    -2.0872\n",
      "[Episode   203] Reward: 200.0 Loss:   -0.66076\n",
      "[Episode   204] Reward: 90.0 Loss:     4.4422\n",
      "[Episode   205] Reward: 111.0 Loss:     2.6441\n",
      "[Episode   206] Reward: 158.0 Loss:     2.3522\n",
      "[Episode   207] Reward: 200.0 Loss:     11.403\n",
      "[Episode   208] Reward: 112.0 Loss:     5.9494\n",
      "[Episode   209] Reward: 200.0 Loss:    0.49789\n",
      "[Episode   210] Reward: 200.0 Loss:     6.4823\n",
      "[Episode   211] Reward: 184.0 Loss:     8.3135\n",
      "[Episode   212] Reward: 200.0 Loss:     4.0535\n",
      "[Episode   213] Reward: 200.0 Loss:     5.3911\n",
      "[Episode   214] Reward: 185.0 Loss:     1.5798\n",
      "[Episode   215] Reward: 200.0 Loss:     3.6039\n",
      "[Episode   216] Reward: 200.0 Loss:     2.7837\n",
      "[Episode   217] Reward: 142.0 Loss:     2.2348\n",
      "[Episode   218] Reward: 200.0 Loss:     1.0664\n",
      "[Episode   219] Reward: 191.0 Loss:     2.3941\n",
      "[Episode   220] Reward: 200.0 Loss:    -4.2515\n",
      "[Episode   221] Reward: 200.0 Loss:    -1.9854\n",
      "[Episode   222] Reward: 200.0 Loss:    -6.9287\n",
      "[Episode   223] Reward: 200.0 Loss:       1.92\n",
      "[Episode   224] Reward: 200.0 Loss:     2.3378\n",
      "[Episode   225] Reward: 200.0 Loss:     2.6579\n",
      "[Episode   226] Reward: 200.0 Loss:     8.4384\n",
      "[Episode   227] Reward: 145.0 Loss:     6.0015\n",
      "[Episode   228] Reward: 200.0 Loss:     2.3793\n",
      "[Episode   229] Reward: 200.0 Loss:     5.0544\n",
      "[Episode   230] Reward: 200.0 Loss:    -4.7184\n",
      "[Episode   231] Reward: 200.0 Loss:     3.1229\n",
      "[Episode   232] Reward: 200.0 Loss:   -0.22728\n",
      "[Episode   233] Reward: 200.0 Loss:     0.1902\n",
      "[Episode   234] Reward: 200.0 Loss:   -0.98368\n",
      "[Episode   235] Reward: 200.0 Loss:    0.43076\n",
      "[Episode   236] Reward: 200.0 Loss:    -3.7379\n",
      "[Episode   237] Reward: 200.0 Loss:    -1.8907\n",
      "[Episode   238] Reward: 46.0 Loss:    -10.025\n",
      "[Episode   239] Reward: 200.0 Loss:   -0.54661\n",
      "[Episode   240] Reward: 193.0 Loss:    0.71645\n",
      "[Episode   241] Reward: 200.0 Loss:     1.0107\n",
      "[Episode   242] Reward: 200.0 Loss:    -2.4109\n",
      "[Episode   243] Reward: 200.0 Loss:     4.3607\n",
      "[Episode   244] Reward: 103.0 Loss:    -1.0032\n",
      "[Episode   245] Reward: 200.0 Loss:     4.2593\n",
      "[Episode   246] Reward: 200.0 Loss:     2.7161\n",
      "[Episode   247] Reward: 200.0 Loss:     -4.829\n",
      "[Episode   248] Reward: 200.0 Loss:  -0.068662\n",
      "[Episode   249] Reward: 200.0 Loss:     2.1741\n",
      "[Episode   250] Reward: 200.0 Loss:     5.8622\n",
      "[Episode   251] Reward: 200.0 Loss:     1.3466\n",
      "[Episode   252] Reward: 200.0 Loss:    0.72019\n",
      "[Episode   253] Reward: 166.0 Loss:    0.44705\n",
      "[Episode   254] Reward: 193.0 Loss:     4.4569\n",
      "[Episode   255] Reward: 200.0 Loss:   -0.29782\n",
      "[Episode   256] Reward: 200.0 Loss:    -6.2066\n",
      "[Episode   257] Reward: 200.0 Loss:     5.0521\n",
      "[Episode   258] Reward: 200.0 Loss:   -0.97117\n",
      "[Episode   259] Reward: 200.0 Loss:    -2.2188\n",
      "[Episode   260] Reward: 143.0 Loss:     7.4637\n",
      "[Episode   261] Reward: 200.0 Loss:   -0.92849\n",
      "[Episode   262] Reward: 200.0 Loss:     5.1277\n",
      "[Episode   263] Reward: 200.0 Loss:    -3.2393\n",
      "[Episode   264] Reward: 200.0 Loss:     -5.251\n",
      "[Episode   265] Reward: 200.0 Loss:     -2.305\n",
      "[Episode   266] Reward: 200.0 Loss:   -0.57321\n",
      "[Episode   267] Reward: 200.0 Loss:     1.9255\n",
      "[Episode   268] Reward: 200.0 Loss:    -1.9155\n",
      "[Episode   269] Reward: 196.0 Loss:    -3.0204\n",
      "[Episode   270] Reward: 200.0 Loss:     5.3062\n",
      "[Episode   271] Reward: 200.0 Loss:     4.3113\n",
      "[Episode   272] Reward: 179.0 Loss:    -4.8041\n",
      "[Episode   273] Reward: 200.0 Loss:   -0.17921\n",
      "[Episode   274] Reward: 183.0 Loss:   -0.07391\n",
      "[Episode   275] Reward: 200.0 Loss:     1.6845\n",
      "[Episode   276] Reward: 200.0 Loss:     6.5226\n",
      "[Episode   277] Reward: 200.0 Loss:     6.7444\n",
      "[Episode   278] Reward: 200.0 Loss:     2.4166\n",
      "[Episode   279] Reward: 200.0 Loss:    -7.4485\n",
      "[Episode   280] Reward: 200.0 Loss:     3.7385\n",
      "[Episode   281] Reward: 200.0 Loss:     5.1119\n",
      "[Episode   282] Reward: 196.0 Loss:     7.1571\n",
      "[Episode   283] Reward: 200.0 Loss:     1.0134\n",
      "[Episode   284] Reward: 192.0 Loss:    -2.1154\n",
      "[Episode   285] Reward: 200.0 Loss:    -7.0866\n",
      "[Episode   286] Reward: 200.0 Loss:      7.201\n",
      "[Episode   287] Reward: 200.0 Loss:     3.1392\n",
      "[Episode   288] Reward: 200.0 Loss:     2.2838\n",
      "[Episode   289] Reward: 200.0 Loss:     3.4486\n",
      "[Episode   290] Reward: 165.0 Loss:    -2.0334\n",
      "[Episode   291] Reward: 200.0 Loss:     9.0665\n",
      "[Episode   292] Reward: 200.0 Loss:     6.7516\n",
      "[Episode   293] Reward: 200.0 Loss:     4.2306\n",
      "[Episode   294] Reward: 200.0 Loss:    -1.2202\n",
      "[Episode   295] Reward: 200.0 Loss:     5.0285\n",
      "[Episode   296] Reward: 200.0 Loss:     2.0167\n",
      "[Episode   297] Reward: 200.0 Loss:    -2.5742\n",
      "[Episode   298] Reward: 200.0 Loss:     7.7966\n",
      "[Episode   299] Reward: 200.0 Loss:   0.087677\n",
      "[Episode   300] Reward: 200.0 Loss:     2.6849\n",
      "[Episode   301] Reward: 200.0 Loss:     5.2488\n",
      "[Episode   302] Reward: 200.0 Loss:   -0.54825\n",
      "[Episode   303] Reward: 200.0 Loss:   -0.91877\n",
      "[Episode   304] Reward: 86.0 Loss:   -0.58179\n",
      "[Episode   305] Reward: 200.0 Loss:     0.8424\n",
      "[Episode   306] Reward: 144.0 Loss:    -5.2871\n",
      "[Episode   307] Reward: 200.0 Loss:    -1.5907\n",
      "[Episode   308] Reward: 200.0 Loss:    -4.6229\n",
      "[Episode   309] Reward: 200.0 Loss:     9.1101\n",
      "[Episode   310] Reward: 200.0 Loss:     4.5846\n",
      "[Episode   311] Reward: 200.0 Loss:    0.17664\n",
      "[Episode   312] Reward: 200.0 Loss:     1.1347\n",
      "[Episode   313] Reward: 200.0 Loss:     6.4283\n",
      "[Episode   314] Reward: 200.0 Loss:    -2.5254\n",
      "[Episode   315] Reward: 200.0 Loss:    -3.7775\n",
      "[Episode   316] Reward: 200.0 Loss:    -1.2602\n",
      "[Episode   317] Reward: 200.0 Loss:     -4.801\n",
      "[Episode   318] Reward: 200.0 Loss:    -3.4891\n",
      "[Episode   319] Reward: 200.0 Loss:     1.6588\n",
      "[Episode   320] Reward: 200.0 Loss:      2.751\n",
      "[Episode   321] Reward: 200.0 Loss:    -2.5134\n",
      "[Episode   322] Reward: 200.0 Loss:    0.78784\n",
      "[Episode   323] Reward: 200.0 Loss:    -1.5275\n",
      "[Episode   324] Reward: 200.0 Loss:      1.491\n",
      "[Episode   325] Reward: 140.0 Loss:    -8.0618\n",
      "[Episode   326] Reward: 200.0 Loss:   -0.26736\n",
      "[Episode   327] Reward: 200.0 Loss:    -1.7236\n",
      "[Episode   328] Reward: 200.0 Loss:      1.445\n",
      "[Episode   329] Reward: 162.0 Loss:     2.5437\n",
      "[Episode   330] Reward: 200.0 Loss:    -3.3629\n",
      "[Episode   331] Reward: 150.0 Loss:    -4.8565\n",
      "[Episode   332] Reward: 200.0 Loss:     4.6593\n",
      "[Episode   333] Reward: 200.0 Loss:     1.3396\n",
      "[Episode   334] Reward: 200.0 Loss:    -1.5258\n",
      "[Episode   335] Reward: 200.0 Loss:    -3.8407\n",
      "[Episode   336] Reward: 200.0 Loss:   -0.26338\n",
      "[Episode   337] Reward: 200.0 Loss:     1.1155\n",
      "[Episode   338] Reward: 200.0 Loss:     -2.779\n",
      "[Episode   339] Reward: 200.0 Loss:     3.4815\n",
      "[Episode   340] Reward: 200.0 Loss:      -3.91\n",
      "[Episode   341] Reward: 200.0 Loss:     0.8358\n",
      "[Episode   342] Reward: 200.0 Loss:    -3.1986\n",
      "[Episode   343] Reward: 200.0 Loss:    -2.9637\n",
      "[Episode   344] Reward: 200.0 Loss:   -0.31828\n",
      "[Episode   345] Reward: 200.0 Loss:    -0.2093\n",
      "[Episode   346] Reward: 200.0 Loss:     7.8822\n",
      "[Episode   347] Reward: 200.0 Loss:     3.9251\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4a4244a8e203>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# Run the neural net to determine output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0maction_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Determine the output based on our net, allowing for some randomness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1120\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \"\"\"\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;31m# Did not find anything.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    280\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[0;32m--> 282\u001b[0;31m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[1;32m    283\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         raise TypeError('Fetch argument %r has invalid type %r, '\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3589\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3590\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3592\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3666\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3667\u001b[0m       \u001b[0;31m# Actually obj is just the object it's referring to.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3668\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3669\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3670\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mgraph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;34m\"\"\"The `Graph` that contains this tensor.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "This code is based on:\n",
    "https://github.com/hunkim/DeepRL-Agents\n",
    "http://karpathy.github.io/2016/05/31/rl/\n",
    "'''\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "hidden_layer_neurons = 24\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# Constants defining our neural network\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = 1  # logistic regression, one p output\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, input_size], name=\"input_x\")\n",
    "\n",
    "# First layer of weights\n",
    "W1 = tf.get_variable(\"W1\", shape=[input_size, hidden_layer_neurons],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W1))\n",
    "\n",
    "# Second layer of weights\n",
    "W2 = tf.get_variable(\"W2\", shape=[hidden_layer_neurons, output_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "action_pred = tf.nn.sigmoid(tf.matmul(layer1, W2))\n",
    "\n",
    "# Y (fake) and advantages (rewards)\n",
    "Y = tf.placeholder(tf.float32, [None, output_size], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32, name=\"reward_signal\")\n",
    "\n",
    "# Loss function: log_likelihood * advantages\n",
    "#log_lik = -tf.log(Y * action_pred + (1 - Y) * (1 - action_pred))     # using author(awjuliani)'s original cost function (maybe log_likelihood)\n",
    "log_lik = -Y*tf.log(action_pred) - (1 - Y)*tf.log(1 - action_pred)    # using logistic regression cost function, 0 or 1 two classification\n",
    "loss = tf.reduce_sum(log_lik * advantages)\n",
    "\n",
    "# Learning\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "def discount_rewards(r, gamma=0.99):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r, dtype=np.float32)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(len(r))):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "\n",
    "    return discounted_r\n",
    "\n",
    "# Testing Code\n",
    "# It's always recommended to test your code\n",
    "input = [1, 1, 1]\n",
    "output = discount_rewards(input)\n",
    "expect = [1 + 0.99 + 0.99**2, 1 + 0.99, 1]\n",
    "np.testing.assert_almost_equal(output, expect)\n",
    "\n",
    "\n",
    "\n",
    "# Setting up our environment\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "max_num_episodes = 500\n",
    "\n",
    "# This list will contain episode rewards from the most recent 100 games\n",
    "# Clear Condition: Average reward per episode >= 195.0 over 100 games\n",
    "EPISODE_100_REWARD_LIST = []\n",
    "for step in range(max_num_episodes):\n",
    "    # Initialize x stack, y stack, and rewards\n",
    "    xs = np.empty(shape=[0, input_size])\n",
    "    ys = np.empty(shape=[0, 1])\n",
    "    rewards = np.empty(shape=[0, 1])\n",
    "\n",
    "    reward_sum = 0\n",
    "    observation = env.reset()\n",
    "\n",
    "    while True:\n",
    "        x = np.reshape(observation, [1, input_size])\n",
    "\n",
    "        # Run the neural net to determine output\n",
    "        action_prob = sess.run(action_pred, feed_dict={X: x})\n",
    "\n",
    "        # Determine the output based on our net, allowing for some randomness\n",
    "        action = 0 if action_prob < np.random.uniform() else 1\n",
    "\n",
    "        # Append the observations and outputs for learning\n",
    "        xs = np.vstack([xs, x])\n",
    "        ys = np.vstack([ys, action])  # Fake action\n",
    "\n",
    "        # Determine the outcome of our action\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        rewards = np.vstack([rewards, reward])\n",
    "        reward_sum += reward\n",
    "\n",
    "        if done:\n",
    "            # Determine standardized rewards\n",
    "            discounted_rewards = discount_rewards(rewards)\n",
    "            # Normalization\n",
    "            discounted_rewards = (discounted_rewards - discounted_rewards.mean())/(discounted_rewards.std() + 1e-7)\n",
    "            l, _ = sess.run([loss, train],\n",
    "                            feed_dict={X: xs, Y: ys, advantages: discounted_rewards})\n",
    "\n",
    "            EPISODE_100_REWARD_LIST.append(reward_sum)\n",
    "            if len(EPISODE_100_REWARD_LIST) > 100:\n",
    "                EPISODE_100_REWARD_LIST = EPISODE_100_REWARD_LIST[1:]\n",
    "            break\n",
    "\n",
    "    # Print status\n",
    "    print(\"[Episode {step:>5}] Reward: {reward_sum:>4} Loss: {l:>10.5}\".format(step=step, reward_sum=reward_sum, l=l))    \n",
    "    \n",
    "    if np.mean(EPISODE_100_REWARD_LIST) >= 195:\n",
    "        print(\"Game Cleared within {step} steps with the average reward: {np.mean(EPISODE_100_REWARD_LIST)}\".format(step, np.mean(EPISODE_100_REWARD_LIST)))\n",
    "        break\n",
    "\n",
    "# See our trained bot in action\n",
    "observation = env.reset()\n",
    "reward_sum = 0\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    x = np.reshape(observation, [1, input_size])\n",
    "    action_prob = sess.run(action_pred, feed_dict={X: x})\n",
    "    action = 0 if action_prob < 0.5 else 1  # No randomness\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        print(\"Total score: {}\".format(reward_sum))\n",
    "        break\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  PG with softmax\n",
    "*revised by hotae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "(<tf.Tensor 'input_y:0' shape=(?, 2) dtype=float32>, <tf.Tensor 'Softmax:0' shape=(?, 2) dtype=float32>)\n",
      "[Episode 0] Reward: 34.0 Loss: -0.012835\n",
      "[Episode 1] Reward: 22.0 Loss: 0.0052052\n",
      "[Episode 2] Reward: 27.0 Loss: -0.0028947\n",
      "[Episode 3] Reward: 16.0 Loss: 0.046365\n",
      "[Episode 4] Reward: 13.0 Loss: 0.16681\n",
      "[Episode 5] Reward: 33.0 Loss: 0.059537\n",
      "[Episode 6] Reward: 41.0 Loss: 0.10446\n",
      "[Episode 7] Reward: 14.0 Loss: 0.027798\n",
      "[Episode 8] Reward: 20.0 Loss: -0.02997\n",
      "[Episode 9] Reward: 26.0 Loss: 0.013564\n",
      "[Episode 10] Reward: 20.0 Loss: 0.012534\n",
      "[Episode 11] Reward: 24.0 Loss: 0.037356\n",
      "[Episode 12] Reward: 31.0 Loss: -0.0030864\n",
      "[Episode 13] Reward: 19.0 Loss: 0.043295\n",
      "[Episode 14] Reward: 38.0 Loss: 0.011046\n",
      "[Episode 15] Reward: 23.0 Loss: -0.031297\n",
      "[Episode 16] Reward: 61.0 Loss: 0.0043522\n",
      "[Episode 17] Reward: 14.0 Loss: -0.057166\n",
      "[Episode 18] Reward: 13.0 Loss: -0.011575\n",
      "[Episode 19] Reward: 29.0 Loss: -0.0053068\n",
      "[Episode 20] Reward: 19.0 Loss: 0.010474\n",
      "[Episode 21] Reward: 37.0 Loss: -0.0084054\n",
      "[Episode 22] Reward: 12.0 Loss: -0.09638\n",
      "[Episode 23] Reward: 48.0 Loss: -0.087595\n",
      "[Episode 24] Reward: 41.0 Loss: 0.0057407\n",
      "[Episode 25] Reward: 35.0 Loss: -0.10424\n",
      "[Episode 26] Reward: 57.0 Loss: -0.013281\n",
      "[Episode 27] Reward: 16.0 Loss: -0.010284\n",
      "[Episode 28] Reward: 13.0 Loss: 0.081994\n",
      "[Episode 29] Reward: 53.0 Loss: -0.023939\n",
      "[Episode 30] Reward: 47.0 Loss: -0.009289\n",
      "[Episode 31] Reward: 22.0 Loss: 0.015816\n",
      "[Episode 32] Reward: 22.0 Loss: -0.052673\n",
      "[Episode 33] Reward: 45.0 Loss: -0.031702\n",
      "[Episode 34] Reward: 145.0 Loss: 0.0024651\n",
      "[Episode 35] Reward: 41.0 Loss: 0.007227\n",
      "[Episode 36] Reward: 52.0 Loss: 0.0029274\n",
      "[Episode 37] Reward: 62.0 Loss: 0.0025668\n",
      "[Episode 38] Reward: 91.0 Loss: 0.00042833\n",
      "[Episode 39] Reward: 51.0 Loss: -0.016651\n",
      "[Episode 40] Reward: 42.0 Loss: -0.0020908\n",
      "[Episode 41] Reward: 17.0 Loss: -0.016893\n",
      "[Episode 42] Reward: 159.0 Loss: -0.0095389\n",
      "[Episode 43] Reward: 81.0 Loss: -0.020099\n",
      "[Episode 44] Reward: 34.0 Loss: -0.11956\n",
      "[Episode 45] Reward: 111.0 Loss: -0.0086253\n",
      "[Episode 46] Reward: 39.0 Loss: -0.08923\n",
      "[Episode 47] Reward: 32.0 Loss: -0.0088636\n",
      "[Episode 48] Reward: 29.0 Loss: -0.0418\n",
      "[Episode 49] Reward: 46.0 Loss: -0.060408\n",
      "[Episode 50] Reward: 85.0 Loss: -0.02227\n",
      "[Episode 51] Reward: 61.0 Loss: -0.04507\n",
      "[Episode 52] Reward: 37.0 Loss: -0.017329\n",
      "[Episode 53] Reward: 48.0 Loss: 0.004807\n",
      "[Episode 54] Reward: 41.0 Loss: -0.0067083\n",
      "[Episode 55] Reward: 72.0 Loss: -0.04051\n",
      "[Episode 56] Reward: 75.0 Loss: -0.022336\n",
      "[Episode 57] Reward: 72.0 Loss: 0.0063893\n",
      "[Episode 58] Reward: 52.0 Loss: 0.0054087\n",
      "[Episode 59] Reward: 108.0 Loss: -0.0074419\n",
      "[Episode 60] Reward: 23.0 Loss: -0.27425\n",
      "[Episode 61] Reward: 104.0 Loss: 0.020135\n",
      "[Episode 62] Reward: 126.0 Loss: -0.0017943\n",
      "[Episode 63] Reward: 44.0 Loss: -0.0080138\n",
      "[Episode 64] Reward: 73.0 Loss: -0.0064137\n",
      "[Episode 65] Reward: 37.0 Loss: -0.14846\n",
      "[Episode 66] Reward: 81.0 Loss: 0.00276\n",
      "[Episode 67] Reward: 139.0 Loss: 0.011051\n",
      "[Episode 68] Reward: 60.0 Loss: 0.029498\n",
      "[Episode 69] Reward: 142.0 Loss: 0.0021974\n",
      "[Episode 70] Reward: 148.0 Loss: 0.012379\n",
      "[Episode 71] Reward: 103.0 Loss: 0.002371\n",
      "[Episode 72] Reward: 112.0 Loss: 0.035309\n",
      "[Episode 73] Reward: 95.0 Loss: 0.023376\n",
      "[Episode 74] Reward: 120.0 Loss: 0.0070093\n",
      "[Episode 75] Reward: 139.0 Loss: 0.01888\n",
      "[Episode 76] Reward: 102.0 Loss: -0.042207\n",
      "[Episode 77] Reward: 190.0 Loss: -0.0029921\n",
      "[Episode 78] Reward: 175.0 Loss: -0.017063\n",
      "[Episode 79] Reward: 200.0 Loss: 0.010747\n",
      "[Episode 80] Reward: 77.0 Loss: -0.050343\n",
      "[Episode 81] Reward: 135.0 Loss: 0.042747\n",
      "[Episode 82] Reward: 173.0 Loss: -0.012731\n",
      "[Episode 83] Reward: 200.0 Loss: -0.015699\n",
      "[Episode 84] Reward: 169.0 Loss: 0.020748\n",
      "[Episode 85] Reward: 106.0 Loss: -0.019942\n",
      "[Episode 86] Reward: 131.0 Loss: -0.049879\n",
      "[Episode 87] Reward: 200.0 Loss: 0.011188\n",
      "[Episode 88] Reward: 170.0 Loss: 0.016184\n",
      "[Episode 89] Reward: 200.0 Loss: -0.0010427\n",
      "[Episode 90] Reward: 123.0 Loss: 0.016008\n",
      "[Episode 91] Reward: 200.0 Loss: 0.033238\n",
      "[Episode 92] Reward: 166.0 Loss: -0.0039194\n",
      "[Episode 93] Reward: 93.0 Loss: 0.00091045\n",
      "[Episode 94] Reward: 200.0 Loss: -0.0079454\n",
      "[Episode 95] Reward: 200.0 Loss: 0.033052\n",
      "[Episode 96] Reward: 200.0 Loss: 0.023489\n",
      "[Episode 97] Reward: 200.0 Loss: 0.03788\n",
      "[Episode 98] Reward: 120.0 Loss: -0.039069\n",
      "[Episode 99] Reward: 155.0 Loss: 0.0023989\n",
      "[Episode 100] Reward: 108.0 Loss: 0.056285\n",
      "[Episode 101] Reward: 136.0 Loss: -0.005192\n",
      "[Episode 102] Reward: 169.0 Loss: -0.016222\n",
      "[Episode 103] Reward: 200.0 Loss: -0.018349\n",
      "[Episode 104] Reward: 200.0 Loss: 0.005361\n",
      "[Episode 105] Reward: 91.0 Loss: -0.023034\n",
      "[Episode 106] Reward: 134.0 Loss: -0.00082296\n",
      "[Episode 107] Reward: 126.0 Loss: -0.042067\n",
      "[Episode 108] Reward: 200.0 Loss: 0.02662\n",
      "[Episode 109] Reward: 200.0 Loss: -0.012727\n",
      "[Episode 110] Reward: 83.0 Loss: -0.0076041\n",
      "[Episode 111] Reward: 200.0 Loss: 0.029625\n",
      "[Episode 112] Reward: 200.0 Loss: 0.023267\n",
      "[Episode 113] Reward: 146.0 Loss: 0.029242\n",
      "[Episode 114] Reward: 162.0 Loss: -0.0092098\n",
      "[Episode 115] Reward: 175.0 Loss: 0.022662\n",
      "[Episode 116] Reward: 158.0 Loss: 0.0388\n",
      "[Episode 117] Reward: 200.0 Loss: -0.030356\n",
      "[Episode 118] Reward: 150.0 Loss: 0.01787\n",
      "[Episode 119] Reward: 80.0 Loss: 0.010596\n",
      "[Episode 120] Reward: 51.0 Loss: -0.057443\n",
      "[Episode 121] Reward: 135.0 Loss: 0.028981\n",
      "[Episode 122] Reward: 121.0 Loss: 0.0012034\n",
      "[Episode 123] Reward: 157.0 Loss: 0.018372\n",
      "[Episode 124] Reward: 102.0 Loss: -0.038302\n",
      "[Episode 125] Reward: 68.0 Loss: -0.022693\n",
      "[Episode 126] Reward: 124.0 Loss: 0.0046621\n",
      "[Episode 127] Reward: 166.0 Loss: -0.029638\n",
      "[Episode 128] Reward: 90.0 Loss: -0.034914\n",
      "[Episode 129] Reward: 189.0 Loss: -0.028882\n",
      "[Episode 130] Reward: 184.0 Loss: -0.025285\n",
      "[Episode 131] Reward: 118.0 Loss: 0.021191\n",
      "[Episode 132] Reward: 152.0 Loss: 0.0044082\n",
      "[Episode 133] Reward: 200.0 Loss: -0.014261\n",
      "[Episode 134] Reward: 139.0 Loss: -0.025775\n",
      "[Episode 135] Reward: 156.0 Loss: 0.025575\n",
      "[Episode 136] Reward: 80.0 Loss: 0.0034013\n",
      "[Episode 137] Reward: 106.0 Loss: 0.025092\n",
      "[Episode 138] Reward: 167.0 Loss: 0.011472\n",
      "[Episode 139] Reward: 179.0 Loss: 0.033087\n",
      "[Episode 140] Reward: 120.0 Loss: 0.028901\n",
      "[Episode 141] Reward: 200.0 Loss: 0.026898\n",
      "[Episode 142] Reward: 191.0 Loss: 0.03112\n",
      "[Episode 143] Reward: 200.0 Loss: -0.0083859\n",
      "[Episode 144] Reward: 72.0 Loss: -0.014791\n",
      "[Episode 145] Reward: 35.0 Loss: -0.04847\n",
      "[Episode 146] Reward: 200.0 Loss: -0.015461\n",
      "[Episode 147] Reward: 162.0 Loss: 0.041065\n",
      "[Episode 148] Reward: 200.0 Loss: 0.035874\n",
      "[Episode 149] Reward: 198.0 Loss: 0.021945\n",
      "[Episode 150] Reward: 101.0 Loss: -0.13108\n",
      "[Episode 151] Reward: 108.0 Loss: -0.048327\n",
      "[Episode 152] Reward: 126.0 Loss: 0.011955\n",
      "[Episode 153] Reward: 200.0 Loss: 0.034021\n",
      "[Episode 154] Reward: 200.0 Loss: 0.034124\n",
      "[Episode 155] Reward: 200.0 Loss: -0.0032353\n",
      "[Episode 156] Reward: 200.0 Loss: 0.021116\n",
      "[Episode 157] Reward: 200.0 Loss: -0.00036825\n",
      "[Episode 158] Reward: 194.0 Loss: 0.035011\n",
      "[Episode 159] Reward: 200.0 Loss: 0.010883\n",
      "[Episode 160] Reward: 200.0 Loss: -0.016827\n",
      "[Episode 161] Reward: 200.0 Loss: -0.034227\n",
      "[Episode 162] Reward: 200.0 Loss: 0.004701\n",
      "[Episode 163] Reward: 200.0 Loss: 0.023991\n",
      "[Episode 164] Reward: 200.0 Loss: 0.005649\n",
      "[Episode 165] Reward: 200.0 Loss: 0.032745\n",
      "[Episode 166] Reward: 200.0 Loss: 0.01187\n",
      "[Episode 167] Reward: 200.0 Loss: 0.0040915\n",
      "[Episode 168] Reward: 97.0 Loss: 0.064013\n",
      "[Episode 169] Reward: 200.0 Loss: -0.0059402\n",
      "[Episode 170] Reward: 200.0 Loss: -0.0089569\n",
      "[Episode 171] Reward: 200.0 Loss: 0.0052902\n",
      "[Episode 172] Reward: 200.0 Loss: 0.030574\n",
      "[Episode 173] Reward: 90.0 Loss: 0.012578\n",
      "[Episode 174] Reward: 200.0 Loss: -0.0061898\n",
      "[Episode 175] Reward: 200.0 Loss: -7.3214e-05\n",
      "[Episode 176] Reward: 200.0 Loss: 0.031949\n",
      "[Episode 177] Reward: 135.0 Loss: -0.002421\n",
      "[Episode 178] Reward: 198.0 Loss: 0.011529\n",
      "[Episode 179] Reward: 174.0 Loss: 0.020211\n",
      "[Episode 180] Reward: 151.0 Loss: 0.019195\n",
      "[Episode 181] Reward: 161.0 Loss: 0.035912\n",
      "[Episode 182] Reward: 200.0 Loss: 0.035072\n",
      "[Episode 183] Reward: 194.0 Loss: 0.026542\n",
      "[Episode 184] Reward: 91.0 Loss: 0.084212\n",
      "[Episode 185] Reward: 200.0 Loss: 0.033069\n",
      "[Episode 186] Reward: 131.0 Loss: 0.01895\n",
      "[Episode 187] Reward: 200.0 Loss: -0.012062\n",
      "[Episode 188] Reward: 153.0 Loss: -0.0068235\n",
      "[Episode 189] Reward: 113.0 Loss: 0.018376\n",
      "[Episode 190] Reward: 120.0 Loss: -0.011961\n",
      "[Episode 191] Reward: 126.0 Loss: -0.0012005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 192] Reward: 121.0 Loss: 0.053207\n",
      "[Episode 193] Reward: 200.0 Loss: 0.0011435\n",
      "[Episode 194] Reward: 200.0 Loss: 0.0023832\n",
      "[Episode 195] Reward: 106.0 Loss: -0.014081\n",
      "[Episode 196] Reward: 136.0 Loss: -0.020721\n",
      "[Episode 197] Reward: 200.0 Loss: 2.5145e-05\n",
      "[Episode 198] Reward: 200.0 Loss: -0.00026377\n",
      "[Episode 199] Reward: 93.0 Loss: -0.0067522\n",
      "[Episode 200] Reward: 200.0 Loss: 0.0080798\n",
      "[Episode 201] Reward: 200.0 Loss: -0.017945\n",
      "[Episode 202] Reward: 200.0 Loss: -0.0030089\n",
      "[Episode 203] Reward: 129.0 Loss: 0.01493\n",
      "[Episode 204] Reward: 126.0 Loss: 0.034921\n",
      "[Episode 205] Reward: 200.0 Loss: 0.0023429\n",
      "[Episode 206] Reward: 200.0 Loss: 0.0049147\n",
      "[Episode 207] Reward: 132.0 Loss: 0.025108\n",
      "[Episode 208] Reward: 127.0 Loss: -0.011088\n",
      "[Episode 209] Reward: 172.0 Loss: 0.00053539\n",
      "[Episode 210] Reward: 200.0 Loss: 0.0064403\n",
      "[Episode 211] Reward: 112.0 Loss: 0.024515\n",
      "[Episode 212] Reward: 200.0 Loss: -0.011128\n",
      "[Episode 213] Reward: 200.0 Loss: 0.00014444\n",
      "[Episode 214] Reward: 200.0 Loss: 0.032887\n",
      "[Episode 215] Reward: 77.0 Loss: -0.051608\n",
      "[Episode 216] Reward: 119.0 Loss: 0.030422\n",
      "[Episode 217] Reward: 200.0 Loss: 0.010295\n",
      "[Episode 218] Reward: 161.0 Loss: -0.003054\n",
      "[Episode 219] Reward: 152.0 Loss: -0.024772\n",
      "[Episode 220] Reward: 200.0 Loss: 0.00067149\n",
      "[Episode 221] Reward: 200.0 Loss: -0.020999\n",
      "[Episode 222] Reward: 89.0 Loss: -0.035242\n",
      "[Episode 223] Reward: 158.0 Loss: -0.019536\n",
      "[Episode 224] Reward: 102.0 Loss: 0.012245\n",
      "[Episode 225] Reward: 169.0 Loss: -0.065805\n",
      "[Episode 226] Reward: 81.0 Loss: -0.035753\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fe7f83772809>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Run the neural net to determine output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0maction_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Append the observations and outputs for learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "This code is based on:\n",
    "https://github.com/hunkim/DeepRL-Agents\n",
    "http://karpathy.github.io/2016/05/31/rl/\n",
    "'''\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "hidden_layer_neurons = 24\n",
    "learning_rate = 1e-2\n",
    "gamma = .99\n",
    "\n",
    "# Constants defining our neural network\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, input_size], name=\"input_x\")\n",
    "\n",
    "# First layer of weights\n",
    "W1 = tf.get_variable(\"W1\", shape=[input_size, hidden_layer_neurons],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W1))\n",
    "\n",
    "# Second layer of weights\n",
    "W2 = tf.get_variable(\"W2\", shape=[hidden_layer_neurons, output_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "action_pred = tf.nn.softmax(tf.matmul(layer1, W2))\n",
    "\n",
    "# We need to define the parts of the network needed for learning a policy\n",
    "Y = tf.placeholder(tf.float32, [None, output_size], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32, name=\"reward_signal\")\n",
    "\n",
    "print(Y, action_pred)\n",
    "# Loss function, ∑ Ai*logp(yi∣xi), but we need fake lable Y due to autodiff\n",
    "log_lik = -Y * tf.log(action_pred)  ## we can consider one-hot encoding\n",
    "log_lik_adv = log_lik * advantages\n",
    "loss = tf.reduce_mean(tf.reduce_sum(log_lik_adv, axis=1))\n",
    "\n",
    "# Learning\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "def discount_rewards(r, gamma=0.99):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r, dtype=np.float32)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(len(r))):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "\n",
    "    return discounted_r\n",
    "\n",
    "\n",
    "# Setting up our environment\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "num_episodes = 1000\n",
    "# This list will contain episode rewards from the most recent 100 games\n",
    "# Clear Condition: Average reward per episode >= 195.0 over 100 games\n",
    "EPISODE_100_REWARD_LIST = []\n",
    "for i in range(num_episodes):\n",
    "\n",
    "    # Clear out game variables\n",
    "    xs = np.empty(shape=[0, input_size])\n",
    "    ys = np.empty(shape=[0, output_size])\n",
    "    rewards = np.empty(shape=[0, 1])\n",
    "\n",
    "    reward_sum = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "        # Append the observations to our batch\n",
    "        x = np.reshape(state, [1, input_size])\n",
    "\n",
    "        # Run the neural net to determine output\n",
    "        action_prob = sess.run(action_pred, feed_dict={X: x})        \n",
    "        action = np.random.choice(np.arange(output_size), p=action_prob[0]) ## choose the action label of 0 ~ output_size-1 \n",
    "    \n",
    "        # Append the observations and outputs for learning\n",
    "        xs = np.vstack([xs, x])\n",
    "        y = np.zeros(output_size)\n",
    "        y[action] = 1\n",
    "        \n",
    "        ys = np.vstack([ys, y])\n",
    "\n",
    "        # Determine the outcome of our action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        rewards = np.vstack([rewards, reward])\n",
    "\n",
    "        if done:\n",
    "            # Determine standardized rewards\n",
    "            discounted_rewards = discount_rewards(rewards, gamma)\n",
    "            # Normalization\n",
    "            discounted_rewards = (discounted_rewards - discounted_rewards.mean())/(discounted_rewards.std() + 1e-7)\n",
    "            ll, la, l, _ = sess.run([log_lik, log_lik_adv, loss, train], feed_dict={X: xs,\n",
    "                                                                                    Y: ys,\n",
    "                                                                                    advantages: discounted_rewards})\n",
    "            # print values for debugging\n",
    "            # print(1, ll, la)\n",
    "            EPISODE_100_REWARD_LIST.append(reward_sum)\n",
    "            if len(EPISODE_100_REWARD_LIST) > 100:\n",
    "                EPISODE_100_REWARD_LIST = EPISODE_100_REWARD_LIST[1:]\n",
    "            break\n",
    "\n",
    "\n",
    "    # Print status\n",
    "    print(\"[Episode {i:>}] Reward: {reward_sum:>4} Loss: {l:>5.5}\".format(i = i, reward_sum=reward_sum, l = l))\n",
    "    \n",
    "    if np.mean(EPISODE_100_REWARD_LIST) >= 195.0:\n",
    "        print(\"Game Cleared within {i} steps with the average reward: {np.mean(EPISODE_100_REWARD_LIST)}\".format(i, np.mean(EPISODE_100_REWARD_LIST) ))\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "state = env.reset()\n",
    "reward_sum = 0\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "\n",
    "    x = np.reshape(state, [1, input_size])\n",
    "    action_prob = sess.run(action_pred, feed_dict={X: x})\n",
    "    action = np.argmax(action_prob)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        print(\"Total score: {}\".format(reward_sum))\n",
    "        break\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode 0 finished after 14 timesteps with r=14.0. Running score: 14.0\n",
      "Episode 1 finished after 10 timesteps with r=10.0. Running score: 12.0\n",
      "Episode 2 finished after 16 timesteps with r=16.0. Running score: 13.3333333333\n",
      "Episode 3 finished after 17 timesteps with r=17.0. Running score: 14.25\n",
      "Episode 4 finished after 18 timesteps with r=18.0. Running score: 15.0\n",
      "Episode 5 finished after 17 timesteps with r=17.0. Running score: 15.3333333333\n",
      "Episode 6 finished after 15 timesteps with r=15.0. Running score: 15.2857142857\n",
      "Episode 7 finished after 25 timesteps with r=25.0. Running score: 16.5\n",
      "Episode 8 finished after 16 timesteps with r=16.0. Running score: 16.4444444444\n",
      "Episode 9 finished after 26 timesteps with r=26.0. Running score: 17.4\n",
      "Episode 10 finished after 19 timesteps with r=19.0. Running score: 17.5454545455\n",
      "Episode 11 finished after 14 timesteps with r=14.0. Running score: 17.25\n",
      "Episode 12 finished after 11 timesteps with r=11.0. Running score: 16.7692307692\n",
      "Episode 13 finished after 33 timesteps with r=33.0. Running score: 17.9285714286\n",
      "Episode 14 finished after 15 timesteps with r=15.0. Running score: 17.7333333333\n",
      "Episode 15 finished after 14 timesteps with r=14.0. Running score: 17.5\n",
      "Episode 16 finished after 36 timesteps with r=36.0. Running score: 18.5882352941\n",
      "Episode 17 finished after 12 timesteps with r=12.0. Running score: 18.2222222222\n",
      "Episode 18 finished after 12 timesteps with r=12.0. Running score: 17.8947368421\n",
      "Episode 19 finished after 16 timesteps with r=16.0. Running score: 17.8\n",
      "Episode 20 finished after 28 timesteps with r=28.0. Running score: 18.2857142857\n",
      "Episode 21 finished after 13 timesteps with r=13.0. Running score: 18.0454545455\n",
      "Episode 22 finished after 28 timesteps with r=28.0. Running score: 18.4782608696\n",
      "Episode 23 finished after 12 timesteps with r=12.0. Running score: 18.2083333333\n",
      "Episode 24 finished after 16 timesteps with r=16.0. Running score: 18.12\n",
      "Episode 25 finished after 75 timesteps with r=75.0. Running score: 20.3076923077\n",
      "Episode 26 finished after 52 timesteps with r=52.0. Running score: 21.4814814815\n",
      "Episode 27 finished after 45 timesteps with r=45.0. Running score: 22.3214285714\n",
      "Episode 28 finished after 42 timesteps with r=42.0. Running score: 23.0\n",
      "Episode 29 finished after 17 timesteps with r=17.0. Running score: 22.8\n",
      "Episode 30 finished after 85 timesteps with r=85.0. Running score: 24.8064516129\n",
      "Episode 31 finished after 68 timesteps with r=68.0. Running score: 26.15625\n",
      "Episode 32 finished after 61 timesteps with r=61.0. Running score: 27.2121212121\n",
      "Episode 33 finished after 67 timesteps with r=67.0. Running score: 28.3823529412\n",
      "Episode 34 finished after 43 timesteps with r=43.0. Running score: 28.8\n",
      "Episode 35 finished after 54 timesteps with r=54.0. Running score: 29.5\n",
      "Episode 36 finished after 109 timesteps with r=109.0. Running score: 31.6486486486\n",
      "Episode 37 finished after 28 timesteps with r=28.0. Running score: 31.5526315789\n",
      "Episode 38 finished after 35 timesteps with r=35.0. Running score: 31.641025641\n",
      "Episode 39 finished after 33 timesteps with r=33.0. Running score: 31.675\n",
      "Episode 40 finished after 21 timesteps with r=21.0. Running score: 31.4146341463\n",
      "Episode 41 finished after 40 timesteps with r=40.0. Running score: 31.619047619\n",
      "Episode 42 finished after 38 timesteps with r=38.0. Running score: 31.7674418605\n",
      "Episode 43 finished after 51 timesteps with r=51.0. Running score: 32.2045454545\n",
      "Episode 44 finished after 16 timesteps with r=16.0. Running score: 31.8444444444\n",
      "Episode 45 finished after 33 timesteps with r=33.0. Running score: 31.8695652174\n",
      "Episode 46 finished after 31 timesteps with r=31.0. Running score: 31.8510638298\n",
      "Episode 47 finished after 37 timesteps with r=37.0. Running score: 31.9583333333\n",
      "Episode 48 finished after 61 timesteps with r=61.0. Running score: 32.5510204082\n",
      "Episode 49 finished after 45 timesteps with r=45.0. Running score: 32.8\n",
      "Episode 50 finished after 135 timesteps with r=135.0. Running score: 34.8039215686\n",
      "Episode 51 finished after 37 timesteps with r=37.0. Running score: 34.8461538462\n",
      "Episode 52 finished after 47 timesteps with r=47.0. Running score: 35.0754716981\n",
      "Episode 53 finished after 110 timesteps with r=110.0. Running score: 36.462962963\n",
      "Episode 54 finished after 118 timesteps with r=118.0. Running score: 37.9454545455\n",
      "Episode 55 finished after 73 timesteps with r=73.0. Running score: 38.5714285714\n",
      "Episode 56 finished after 38 timesteps with r=38.0. Running score: 38.5614035088\n",
      "Episode 57 finished after 266 timesteps with r=266.0. Running score: 42.4827586207\n",
      "Episode 58 finished after 36 timesteps with r=36.0. Running score: 42.3728813559\n",
      "Episode 59 finished after 172 timesteps with r=172.0. Running score: 44.5333333333\n",
      "Episode 60 finished after 189 timesteps with r=189.0. Running score: 46.9016393443\n",
      "Episode 61 finished after 166 timesteps with r=166.0. Running score: 48.8225806452\n",
      "Episode 62 finished after 127 timesteps with r=127.0. Running score: 50.0634920635\n",
      "Episode 63 finished after 39 timesteps with r=39.0. Running score: 49.890625\n",
      "Episode 64 finished after 159 timesteps with r=159.0. Running score: 51.5692307692\n",
      "Episode 65 finished after 76 timesteps with r=76.0. Running score: 51.9393939394\n",
      "Episode 66 finished after 59 timesteps with r=59.0. Running score: 52.0447761194\n",
      "Episode 67 finished after 108 timesteps with r=108.0. Running score: 52.8676470588\n",
      "Episode 68 finished after 63 timesteps with r=63.0. Running score: 53.0144927536\n",
      "Episode 69 finished after 65 timesteps with r=65.0. Running score: 53.1857142857\n",
      "Episode 70 finished after 64 timesteps with r=64.0. Running score: 53.338028169\n",
      "Episode 71 finished after 40 timesteps with r=40.0. Running score: 53.1527777778\n",
      "Episode 72 finished after 79 timesteps with r=79.0. Running score: 53.5068493151\n",
      "Episode 73 finished after 142 timesteps with r=142.0. Running score: 54.7027027027\n",
      "Episode 74 finished after 39 timesteps with r=39.0. Running score: 54.4933333333\n",
      "Episode 75 finished after 33 timesteps with r=33.0. Running score: 54.2105263158\n",
      "Episode 76 finished after 35 timesteps with r=35.0. Running score: 53.961038961\n",
      "Episode 77 finished after 45 timesteps with r=45.0. Running score: 53.8461538462\n",
      "Episode 78 finished after 42 timesteps with r=42.0. Running score: 53.6962025316\n",
      "Episode 79 finished after 82 timesteps with r=82.0. Running score: 54.05\n",
      "Episode 80 finished after 219 timesteps with r=219.0. Running score: 56.0864197531\n",
      "Episode 81 finished after 33 timesteps with r=33.0. Running score: 55.8048780488\n",
      "Episode 82 finished after 9 timesteps with r=9.0. Running score: 55.2409638554\n",
      "Episode 83 finished after 9 timesteps with r=9.0. Running score: 54.6904761905\n",
      "Episode 84 finished after 9 timesteps with r=9.0. Running score: 54.1529411765\n",
      "Episode 85 finished after 11 timesteps with r=11.0. Running score: 53.6511627907\n",
      "Episode 86 finished after 11 timesteps with r=11.0. Running score: 53.1609195402\n",
      "Episode 87 finished after 10 timesteps with r=10.0. Running score: 52.6704545455\n",
      "Episode 88 finished after 9 timesteps with r=9.0. Running score: 52.1797752809\n",
      "Episode 89 finished after 11 timesteps with r=11.0. Running score: 51.7222222222\n",
      "Episode 90 finished after 14 timesteps with r=14.0. Running score: 51.3076923077\n",
      "Episode 91 finished after 12 timesteps with r=12.0. Running score: 50.8804347826\n",
      "Episode 92 finished after 188 timesteps with r=188.0. Running score: 52.3548387097\n",
      "Episode 93 finished after 163 timesteps with r=163.0. Running score: 53.5319148936\n",
      "Episode 94 finished after 122 timesteps with r=122.0. Running score: 54.2526315789\n",
      "Episode 95 finished after 174 timesteps with r=174.0. Running score: 55.5\n",
      "Episode 96 finished after 66 timesteps with r=66.0. Running score: 55.6082474227\n",
      "Episode 97 finished after 139 timesteps with r=139.0. Running score: 56.4591836735\n",
      "Episode 98 finished after 52 timesteps with r=52.0. Running score: 56.4141414141\n",
      "Episode 99 finished after 54 timesteps with r=54.0. Running score: 56.39\n",
      "Episode 100 finished after 136 timesteps with r=136.0. Running score: 57.1782178218\n",
      "Episode 101 finished after 139 timesteps with r=139.0. Running score: 57.9803921569\n",
      "Episode 102 finished after 79 timesteps with r=79.0. Running score: 58.1844660194\n",
      "Episode 103 finished after 113 timesteps with r=113.0. Running score: 58.7115384615\n",
      "Episode 104 finished after 90 timesteps with r=90.0. Running score: 59.0095238095\n",
      "Episode 105 finished after 101 timesteps with r=101.0. Running score: 59.4056603774\n",
      "Episode 106 finished after 136 timesteps with r=136.0. Running score: 60.1214953271\n",
      "Episode 107 finished after 86 timesteps with r=86.0. Running score: 60.3611111111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 108 finished after 136 timesteps with r=136.0. Running score: 61.0550458716\n",
      "Episode 109 finished after 285 timesteps with r=285.0. Running score: 63.0909090909\n",
      "Episode 110 finished after 61 timesteps with r=61.0. Running score: 63.0720720721\n",
      "Episode 111 finished after 112 timesteps with r=112.0. Running score: 63.5089285714\n",
      "Episode 112 finished after 98 timesteps with r=98.0. Running score: 63.814159292\n",
      "Episode 113 finished after 79 timesteps with r=79.0. Running score: 63.9473684211\n",
      "Episode 114 finished after 75 timesteps with r=75.0. Running score: 64.0434782609\n",
      "Episode 115 finished after 182 timesteps with r=182.0. Running score: 65.0603448276\n",
      "Episode 116 finished after 59 timesteps with r=59.0. Running score: 65.0085470085\n",
      "Episode 117 finished after 215 timesteps with r=215.0. Running score: 66.2796610169\n",
      "Episode 118 finished after 84 timesteps with r=84.0. Running score: 66.4285714286\n",
      "Episode 119 finished after 157 timesteps with r=157.0. Running score: 67.1833333333\n",
      "Episode 120 finished after 42 timesteps with r=42.0. Running score: 66.9752066116\n",
      "Episode 121 finished after 97 timesteps with r=97.0. Running score: 67.2213114754\n",
      "Episode 122 finished after 101 timesteps with r=101.0. Running score: 67.4959349593\n",
      "Episode 123 finished after 123 timesteps with r=123.0. Running score: 67.9435483871\n",
      "Episode 124 finished after 56 timesteps with r=56.0. Running score: 67.848\n",
      "Episode 125 finished after 72 timesteps with r=72.0. Running score: 67.880952381\n",
      "Episode 126 finished after 268 timesteps with r=268.0. Running score: 69.4566929134\n",
      "Episode 127 finished after 500 timesteps with r=500.0. Running score: 72.8203125\n",
      "Episode 128 finished after 72 timesteps with r=72.0. Running score: 72.8139534884\n",
      "Episode 129 finished after 376 timesteps with r=376.0. Running score: 75.1461538462\n",
      "Episode 130 finished after 106 timesteps with r=106.0. Running score: 75.3816793893\n",
      "Episode 131 finished after 34 timesteps with r=34.0. Running score: 75.0681818182\n",
      "Episode 132 finished after 129 timesteps with r=129.0. Running score: 75.4736842105\n",
      "Episode 133 finished after 179 timesteps with r=179.0. Running score: 76.2462686567\n",
      "Episode 134 finished after 116 timesteps with r=116.0. Running score: 76.5407407407\n",
      "Episode 135 finished after 139 timesteps with r=139.0. Running score: 77.0\n",
      "Episode 136 finished after 137 timesteps with r=137.0. Running score: 77.4379562044\n",
      "Episode 137 finished after 91 timesteps with r=91.0. Running score: 77.5362318841\n",
      "Episode 138 finished after 66 timesteps with r=66.0. Running score: 77.4532374101\n",
      "Episode 139 finished after 102 timesteps with r=102.0. Running score: 77.6285714286\n",
      "Episode 140 finished after 99 timesteps with r=99.0. Running score: 77.780141844\n",
      "Episode 141 finished after 230 timesteps with r=230.0. Running score: 78.8521126761\n",
      "Episode 142 finished after 38 timesteps with r=38.0. Running score: 78.5664335664\n",
      "Episode 143 finished after 55 timesteps with r=55.0. Running score: 78.4027777778\n",
      "Episode 144 finished after 68 timesteps with r=68.0. Running score: 78.3310344828\n",
      "Episode 145 finished after 138 timesteps with r=138.0. Running score: 78.7397260274\n",
      "Episode 146 finished after 82 timesteps with r=82.0. Running score: 78.7619047619\n",
      "Episode 147 finished after 54 timesteps with r=54.0. Running score: 78.5945945946\n",
      "Episode 148 finished after 140 timesteps with r=140.0. Running score: 79.0067114094\n",
      "Episode 149 finished after 113 timesteps with r=113.0. Running score: 79.2333333333\n",
      "Episode 150 finished after 100 timesteps with r=100.0. Running score: 79.3708609272\n",
      "Episode 151 finished after 129 timesteps with r=129.0. Running score: 79.6973684211\n",
      "Episode 152 finished after 286 timesteps with r=286.0. Running score: 81.045751634\n",
      "Episode 153 finished after 52 timesteps with r=52.0. Running score: 80.8571428571\n",
      "Episode 154 finished after 55 timesteps with r=55.0. Running score: 80.6903225806\n",
      "Episode 155 finished after 60 timesteps with r=60.0. Running score: 80.5576923077\n",
      "Episode 156 finished after 56 timesteps with r=56.0. Running score: 80.4012738854\n",
      "Episode 157 finished after 137 timesteps with r=137.0. Running score: 80.7594936709\n",
      "Episode 158 finished after 229 timesteps with r=229.0. Running score: 81.6918238994\n",
      "Episode 159 finished after 55 timesteps with r=55.0. Running score: 81.525\n",
      "Episode 160 finished after 114 timesteps with r=114.0. Running score: 81.7267080745\n",
      "Episode 161 finished after 57 timesteps with r=57.0. Running score: 81.5740740741\n",
      "Episode 162 finished after 79 timesteps with r=79.0. Running score: 81.5582822086\n",
      "Episode 163 finished after 101 timesteps with r=101.0. Running score: 81.6768292683\n",
      "Episode 164 finished after 131 timesteps with r=131.0. Running score: 81.9757575758\n",
      "Episode 165 finished after 79 timesteps with r=79.0. Running score: 81.9578313253\n",
      "Episode 166 finished after 53 timesteps with r=53.0. Running score: 81.7844311377\n",
      "Episode 167 finished after 91 timesteps with r=91.0. Running score: 81.8392857143\n",
      "Episode 168 finished after 57 timesteps with r=57.0. Running score: 81.6923076923\n",
      "Episode 169 finished after 91 timesteps with r=91.0. Running score: 81.7470588235\n",
      "Episode 170 finished after 77 timesteps with r=77.0. Running score: 81.7192982456\n",
      "Episode 171 finished after 119 timesteps with r=119.0. Running score: 81.9360465116\n",
      "Episode 172 finished after 377 timesteps with r=377.0. Running score: 83.6416184971\n",
      "Episode 173 finished after 328 timesteps with r=328.0. Running score: 85.0459770115\n",
      "Episode 174 finished after 56 timesteps with r=56.0. Running score: 84.88\n",
      "Episode 175 finished after 48 timesteps with r=48.0. Running score: 84.6704545455\n",
      "Episode 176 finished after 105 timesteps with r=105.0. Running score: 84.7853107345\n",
      "Episode 177 finished after 66 timesteps with r=66.0. Running score: 84.6797752809\n",
      "Episode 178 finished after 57 timesteps with r=57.0. Running score: 84.5251396648\n",
      "Episode 179 finished after 39 timesteps with r=39.0. Running score: 84.2722222222\n",
      "Episode 180 finished after 368 timesteps with r=368.0. Running score: 85.8397790055\n",
      "Episode 181 finished after 55 timesteps with r=55.0. Running score: 85.6703296703\n",
      "Episode 182 finished after 118 timesteps with r=118.0. Running score: 85.8469945355\n",
      "Episode 183 finished after 41 timesteps with r=41.0. Running score: 85.6032608696\n",
      "Episode 184 finished after 315 timesteps with r=315.0. Running score: 86.8432432432\n",
      "Episode 185 finished after 100 timesteps with r=100.0. Running score: 86.9139784946\n",
      "Episode 186 finished after 75 timesteps with r=75.0. Running score: 86.8502673797\n",
      "Episode 187 finished after 302 timesteps with r=302.0. Running score: 87.9946808511\n",
      "Episode 188 finished after 65 timesteps with r=65.0. Running score: 87.873015873\n",
      "Episode 189 finished after 80 timesteps with r=80.0. Running score: 87.8315789474\n",
      "Episode 190 finished after 82 timesteps with r=82.0. Running score: 87.8010471204\n",
      "Episode 191 finished after 96 timesteps with r=96.0. Running score: 87.84375\n",
      "Episode 192 finished after 54 timesteps with r=54.0. Running score: 87.6683937824\n",
      "Episode 193 finished after 48 timesteps with r=48.0. Running score: 87.4639175258\n",
      "Episode 194 finished after 66 timesteps with r=66.0. Running score: 87.3538461538\n",
      "Episode 195 finished after 184 timesteps with r=184.0. Running score: 87.8469387755\n",
      "Episode 196 finished after 50 timesteps with r=50.0. Running score: 87.654822335\n",
      "Episode 197 finished after 80 timesteps with r=80.0. Running score: 87.6161616162\n",
      "Episode 198 finished after 174 timesteps with r=174.0. Running score: 88.0502512563\n",
      "Episode 199 finished after 64 timesteps with r=64.0. Running score: 87.93\n",
      "Episode 200 finished after 108 timesteps with r=108.0. Running score: 88.0298507463\n",
      "Episode 201 finished after 270 timesteps with r=270.0. Running score: 88.9306930693\n",
      "Episode 202 finished after 77 timesteps with r=77.0. Running score: 88.8719211823\n",
      "Episode 203 finished after 124 timesteps with r=124.0. Running score: 89.0441176471\n",
      "Episode 204 finished after 188 timesteps with r=188.0. Running score: 89.5268292683\n",
      "Episode 205 finished after 82 timesteps with r=82.0. Running score: 89.4902912621\n",
      "Episode 206 finished after 102 timesteps with r=102.0. Running score: 89.5507246377\n",
      "Episode 207 finished after 101 timesteps with r=101.0. Running score: 89.6057692308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 208 finished after 181 timesteps with r=181.0. Running score: 90.043062201\n",
      "Episode 209 finished after 90 timesteps with r=90.0. Running score: 90.0428571429\n",
      "Episode 210 finished after 175 timesteps with r=175.0. Running score: 90.4454976303\n",
      "Episode 211 finished after 204 timesteps with r=204.0. Running score: 90.9811320755\n",
      "Episode 212 finished after 129 timesteps with r=129.0. Running score: 91.1596244131\n",
      "Episode 213 finished after 61 timesteps with r=61.0. Running score: 91.0186915888\n",
      "Episode 214 finished after 200 timesteps with r=200.0. Running score: 91.5255813953\n",
      "Episode 215 finished after 128 timesteps with r=128.0. Running score: 91.6944444444\n",
      "Episode 216 finished after 79 timesteps with r=79.0. Running score: 91.6359447005\n",
      "Episode 217 finished after 58 timesteps with r=58.0. Running score: 91.4816513761\n",
      "Episode 218 finished after 342 timesteps with r=342.0. Running score: 92.6255707763\n",
      "Episode 219 finished after 162 timesteps with r=162.0. Running score: 92.9409090909\n",
      "Episode 220 finished after 92 timesteps with r=92.0. Running score: 92.9366515837\n",
      "Episode 221 finished after 83 timesteps with r=83.0. Running score: 92.8918918919\n",
      "Episode 222 finished after 67 timesteps with r=67.0. Running score: 92.7757847534\n",
      "Episode 223 finished after 61 timesteps with r=61.0. Running score: 92.6339285714\n",
      "Episode 224 finished after 60 timesteps with r=60.0. Running score: 92.4888888889\n",
      "Episode 225 finished after 101 timesteps with r=101.0. Running score: 92.5265486726\n",
      "Episode 226 finished after 91 timesteps with r=91.0. Running score: 92.5198237885\n",
      "Episode 227 finished after 431 timesteps with r=431.0. Running score: 94.0043859649\n",
      "Episode 228 finished after 52 timesteps with r=52.0. Running score: 93.8209606987\n",
      "Episode 229 finished after 32 timesteps with r=32.0. Running score: 93.552173913\n",
      "Episode 230 finished after 43 timesteps with r=43.0. Running score: 93.3333333333\n",
      "Episode 231 finished after 260 timesteps with r=260.0. Running score: 94.0517241379\n",
      "Episode 232 finished after 80 timesteps with r=80.0. Running score: 93.991416309\n",
      "Episode 233 finished after 32 timesteps with r=32.0. Running score: 93.7264957265\n",
      "Episode 234 finished after 46 timesteps with r=46.0. Running score: 93.5234042553\n",
      "Episode 235 finished after 500 timesteps with r=500.0. Running score: 95.2457627119\n",
      "Episode 236 finished after 48 timesteps with r=48.0. Running score: 95.0464135021\n",
      "Episode 237 finished after 184 timesteps with r=184.0. Running score: 95.4201680672\n",
      "Episode 238 finished after 40 timesteps with r=40.0. Running score: 95.1882845188\n",
      "Episode 239 finished after 500 timesteps with r=500.0. Running score: 96.875\n",
      "Episode 240 finished after 492 timesteps with r=492.0. Running score: 98.5145228216\n",
      "Episode 241 finished after 93 timesteps with r=93.0. Running score: 98.4917355372\n",
      "Episode 242 finished after 59 timesteps with r=59.0. Running score: 98.329218107\n",
      "Episode 243 finished after 310 timesteps with r=310.0. Running score: 99.1967213115\n",
      "Episode 244 finished after 61 timesteps with r=61.0. Running score: 99.0408163265\n",
      "Episode 245 finished after 226 timesteps with r=226.0. Running score: 99.5569105691\n",
      "Episode 246 finished after 48 timesteps with r=48.0. Running score: 99.3481781377\n",
      "Episode 247 finished after 159 timesteps with r=159.0. Running score: 99.5887096774\n",
      "Episode 248 finished after 95 timesteps with r=95.0. Running score: 99.5702811245\n",
      "Episode 249 finished after 80 timesteps with r=80.0. Running score: 99.492\n",
      "Episode 250 finished after 138 timesteps with r=138.0. Running score: 99.6454183267\n",
      "Episode 251 finished after 89 timesteps with r=89.0. Running score: 99.6031746032\n",
      "Episode 252 finished after 56 timesteps with r=56.0. Running score: 99.4308300395\n",
      "Episode 253 finished after 33 timesteps with r=33.0. Running score: 99.1692913386\n",
      "Episode 254 finished after 101 timesteps with r=101.0. Running score: 99.1764705882\n",
      "Episode 255 finished after 108 timesteps with r=108.0. Running score: 99.2109375\n",
      "Episode 256 finished after 59 timesteps with r=59.0. Running score: 99.0544747082\n",
      "Episode 257 finished after 72 timesteps with r=72.0. Running score: 98.9496124031\n",
      "Episode 258 finished after 83 timesteps with r=83.0. Running score: 98.888030888\n",
      "Episode 259 finished after 98 timesteps with r=98.0. Running score: 98.8846153846\n",
      "Episode 260 finished after 294 timesteps with r=294.0. Running score: 99.632183908\n",
      "Episode 261 finished after 33 timesteps with r=33.0. Running score: 99.3778625954\n",
      "Episode 262 finished after 47 timesteps with r=47.0. Running score: 99.1787072243\n",
      "Episode 263 finished after 77 timesteps with r=77.0. Running score: 99.0946969697\n",
      "Episode 264 finished after 46 timesteps with r=46.0. Running score: 98.8943396226\n",
      "Episode 265 finished after 51 timesteps with r=51.0. Running score: 98.7142857143\n",
      "Episode 266 finished after 57 timesteps with r=57.0. Running score: 98.5580524345\n",
      "Episode 267 finished after 224 timesteps with r=224.0. Running score: 99.026119403\n",
      "Episode 268 finished after 66 timesteps with r=66.0. Running score: 98.9033457249\n",
      "Episode 269 finished after 32 timesteps with r=32.0. Running score: 98.6555555556\n",
      "Episode 270 finished after 46 timesteps with r=46.0. Running score: 98.4612546125\n",
      "Episode 271 finished after 41 timesteps with r=41.0. Running score: 98.25\n",
      "Episode 272 finished after 64 timesteps with r=64.0. Running score: 98.1245421245\n",
      "Episode 273 finished after 92 timesteps with r=92.0. Running score: 98.102189781\n",
      "Episode 274 finished after 34 timesteps with r=34.0. Running score: 97.8690909091\n",
      "Episode 275 finished after 53 timesteps with r=53.0. Running score: 97.7065217391\n",
      "Episode 276 finished after 66 timesteps with r=66.0. Running score: 97.5920577617\n",
      "Episode 277 finished after 275 timesteps with r=275.0. Running score: 98.2302158273\n",
      "Episode 278 finished after 39 timesteps with r=39.0. Running score: 98.017921147\n",
      "Episode 279 finished after 51 timesteps with r=51.0. Running score: 97.85\n",
      "Episode 280 finished after 154 timesteps with r=154.0. Running score: 98.0498220641\n",
      "Episode 281 finished after 59 timesteps with r=59.0. Running score: 97.9113475177\n",
      "Episode 282 finished after 23 timesteps with r=23.0. Running score: 97.6466431095\n",
      "Episode 283 finished after 32 timesteps with r=32.0. Running score: 97.4154929577\n",
      "Episode 284 finished after 285 timesteps with r=285.0. Running score: 98.0736842105\n",
      "Episode 285 finished after 118 timesteps with r=118.0. Running score: 98.1433566434\n",
      "Episode 286 finished after 22 timesteps with r=22.0. Running score: 97.8780487805\n",
      "Episode 287 finished after 29 timesteps with r=29.0. Running score: 97.6388888889\n",
      "Episode 288 finished after 500 timesteps with r=500.0. Running score: 99.0311418685\n",
      "Episode 289 finished after 22 timesteps with r=22.0. Running score: 98.7655172414\n",
      "Episode 290 finished after 27 timesteps with r=27.0. Running score: 98.5189003436\n",
      "Episode 291 finished after 143 timesteps with r=143.0. Running score: 98.6712328767\n",
      "Episode 292 finished after 100 timesteps with r=100.0. Running score: 98.6757679181\n",
      "Episode 293 finished after 28 timesteps with r=28.0. Running score: 98.4353741497\n",
      "Episode 294 finished after 48 timesteps with r=48.0. Running score: 98.2644067797\n",
      "Episode 295 finished after 413 timesteps with r=413.0. Running score: 99.3277027027\n",
      "Episode 296 finished after 25 timesteps with r=25.0. Running score: 99.0774410774\n",
      "Episode 297 finished after 42 timesteps with r=42.0. Running score: 98.8859060403\n",
      "Episode 298 finished after 40 timesteps with r=40.0. Running score: 98.6889632107\n",
      "Episode 299 finished after 31 timesteps with r=31.0. Running score: 98.4633333333\n",
      "Episode 300 finished after 65 timesteps with r=65.0. Running score: 98.3521594684\n",
      "Episode 301 finished after 27 timesteps with r=27.0. Running score: 98.1158940397\n",
      "Episode 302 finished after 27 timesteps with r=27.0. Running score: 97.8811881188\n",
      "Episode 303 finished after 59 timesteps with r=59.0. Running score: 97.7532894737\n",
      "Episode 304 finished after 40 timesteps with r=40.0. Running score: 97.5639344262\n",
      "Episode 305 finished after 26 timesteps with r=26.0. Running score: 97.3300653595\n",
      "Episode 306 finished after 222 timesteps with r=222.0. Running score: 97.7361563518\n",
      "Episode 307 finished after 65 timesteps with r=65.0. Running score: 97.6298701299\n",
      "Episode 308 finished after 20 timesteps with r=20.0. Running score: 97.3786407767\n",
      "Episode 309 finished after 58 timesteps with r=58.0. Running score: 97.2516129032\n",
      "Episode 310 finished after 36 timesteps with r=36.0. Running score: 97.0546623794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 311 finished after 18 timesteps with r=18.0. Running score: 96.8012820513\n",
      "Episode 312 finished after 148 timesteps with r=148.0. Running score: 96.96485623\n",
      "Episode 313 finished after 42 timesteps with r=42.0. Running score: 96.7898089172\n",
      "Episode 314 finished after 203 timesteps with r=203.0. Running score: 97.126984127\n",
      "Episode 315 finished after 22 timesteps with r=22.0. Running score: 96.8892405063\n",
      "Episode 316 finished after 15 timesteps with r=15.0. Running score: 96.6309148265\n",
      "Episode 317 finished after 233 timesteps with r=233.0. Running score: 97.0597484277\n",
      "Episode 318 finished after 33 timesteps with r=33.0. Running score: 96.8589341693\n",
      "Episode 319 finished after 23 timesteps with r=23.0. Running score: 96.628125\n",
      "Episode 320 finished after 75 timesteps with r=75.0. Running score: 96.5607476636\n",
      "Episode 321 finished after 282 timesteps with r=282.0. Running score: 97.1366459627\n",
      "Episode 322 finished after 64 timesteps with r=64.0. Running score: 97.0340557276\n",
      "Episode 323 finished after 28 timesteps with r=28.0. Running score: 96.8209876543\n",
      "Episode 324 finished after 56 timesteps with r=56.0. Running score: 96.6953846154\n",
      "Episode 325 finished after 65 timesteps with r=65.0. Running score: 96.5981595092\n",
      "Episode 326 finished after 15 timesteps with r=15.0. Running score: 96.3486238532\n",
      "Episode 327 finished after 88 timesteps with r=88.0. Running score: 96.3231707317\n",
      "Episode 328 finished after 389 timesteps with r=389.0. Running score: 97.2127659574\n",
      "Episode 329 finished after 27 timesteps with r=27.0. Running score: 97.0\n",
      "Episode 330 finished after 14 timesteps with r=14.0. Running score: 96.749244713\n",
      "Episode 331 finished after 54 timesteps with r=54.0. Running score: 96.6204819277\n",
      "Episode 332 finished after 12 timesteps with r=12.0. Running score: 96.3663663664\n",
      "Episode 333 finished after 11 timesteps with r=11.0. Running score: 96.1107784431\n",
      "Episode 334 finished after 10 timesteps with r=10.0. Running score: 95.8537313433\n",
      "Episode 335 finished after 11 timesteps with r=11.0. Running score: 95.6011904762\n",
      "Episode 336 finished after 11 timesteps with r=11.0. Running score: 95.350148368\n",
      "Episode 337 finished after 15 timesteps with r=15.0. Running score: 95.1124260355\n",
      "Episode 338 finished after 13 timesteps with r=13.0. Running score: 94.8702064897\n",
      "Episode 339 finished after 15 timesteps with r=15.0. Running score: 94.6352941176\n",
      "Episode 340 finished after 9 timesteps with r=9.0. Running score: 94.3841642229\n",
      "Episode 341 finished after 56 timesteps with r=56.0. Running score: 94.2719298246\n",
      "Episode 342 finished after 10 timesteps with r=10.0. Running score: 94.0262390671\n",
      "Episode 343 finished after 37 timesteps with r=37.0. Running score: 93.8604651163\n",
      "Episode 344 finished after 9 timesteps with r=9.0. Running score: 93.6144927536\n",
      "Episode 345 finished after 9 timesteps with r=9.0. Running score: 93.3699421965\n",
      "Episode 346 finished after 74 timesteps with r=74.0. Running score: 93.3141210375\n",
      "Episode 347 finished after 9 timesteps with r=9.0. Running score: 93.0718390805\n",
      "Episode 348 finished after 9 timesteps with r=9.0. Running score: 92.8309455587\n",
      "Episode 349 finished after 14 timesteps with r=14.0. Running score: 92.6057142857\n",
      "Episode 350 finished after 9 timesteps with r=9.0. Running score: 92.3675213675\n",
      "Episode 351 finished after 8 timesteps with r=8.0. Running score: 92.1278409091\n",
      "Episode 352 finished after 90 timesteps with r=90.0. Running score: 92.1218130312\n",
      "Episode 353 finished after 16 timesteps with r=16.0. Running score: 91.906779661\n",
      "Episode 354 finished after 20 timesteps with r=20.0. Running score: 91.7042253521\n",
      "Episode 355 finished after 36 timesteps with r=36.0. Running score: 91.547752809\n",
      "Episode 356 finished after 11 timesteps with r=11.0. Running score: 91.3221288515\n",
      "Episode 357 finished after 276 timesteps with r=276.0. Running score: 91.8379888268\n",
      "Episode 358 finished after 85 timesteps with r=85.0. Running score: 91.8189415042\n",
      "Episode 359 finished after 61 timesteps with r=61.0. Running score: 91.7333333333\n",
      "Episode 360 finished after 19 timesteps with r=19.0. Running score: 91.5318559557\n",
      "Episode 361 finished after 15 timesteps with r=15.0. Running score: 91.320441989\n",
      "Episode 362 finished after 9 timesteps with r=9.0. Running score: 91.0936639118\n",
      "Episode 363 finished after 10 timesteps with r=10.0. Running score: 90.8708791209\n",
      "Episode 364 finished after 8 timesteps with r=8.0. Running score: 90.6438356164\n",
      "Episode 365 finished after 10 timesteps with r=10.0. Running score: 90.4234972678\n",
      "Episode 366 finished after 10 timesteps with r=10.0. Running score: 90.204359673\n",
      "Episode 367 finished after 21 timesteps with r=21.0. Running score: 90.0163043478\n",
      "Episode 368 finished after 10 timesteps with r=10.0. Running score: 89.7994579946\n",
      "Episode 369 finished after 301 timesteps with r=301.0. Running score: 90.3702702703\n",
      "Episode 370 finished after 59 timesteps with r=59.0. Running score: 90.2857142857\n",
      "Episode 371 finished after 9 timesteps with r=9.0. Running score: 90.0672043011\n",
      "Episode 372 finished after 16 timesteps with r=16.0. Running score: 89.8686327078\n",
      "Episode 373 finished after 15 timesteps with r=15.0. Running score: 89.6684491979\n",
      "Episode 374 finished after 75 timesteps with r=75.0. Running score: 89.6293333333\n",
      "Episode 375 finished after 12 timesteps with r=12.0. Running score: 89.4228723404\n",
      "Episode 376 finished after 73 timesteps with r=73.0. Running score: 89.3793103448\n",
      "Episode 377 finished after 57 timesteps with r=57.0. Running score: 89.2936507937\n",
      "Episode 378 finished after 20 timesteps with r=20.0. Running score: 89.110817942\n",
      "Episode 379 finished after 20 timesteps with r=20.0. Running score: 88.9289473684\n",
      "Episode 380 finished after 17 timesteps with r=17.0. Running score: 88.7401574803\n",
      "Episode 381 finished after 81 timesteps with r=81.0. Running score: 88.719895288\n",
      "Episode 382 finished after 15 timesteps with r=15.0. Running score: 88.5274151436\n",
      "Episode 383 finished after 23 timesteps with r=23.0. Running score: 88.3567708333\n",
      "Episode 384 finished after 18 timesteps with r=18.0. Running score: 88.174025974\n",
      "Episode 385 finished after 201 timesteps with r=201.0. Running score: 88.4663212435\n",
      "Episode 386 finished after 54 timesteps with r=54.0. Running score: 88.3772609819\n",
      "Episode 387 finished after 15 timesteps with r=15.0. Running score: 88.1881443299\n",
      "Episode 388 finished after 59 timesteps with r=59.0. Running score: 88.1131105398\n",
      "Episode 389 finished after 78 timesteps with r=78.0. Running score: 88.0871794872\n",
      "Episode 390 finished after 251 timesteps with r=251.0. Running score: 88.5038363171\n",
      "Episode 391 finished after 16 timesteps with r=16.0. Running score: 88.318877551\n",
      "Episode 392 finished after 36 timesteps with r=36.0. Running score: 88.1857506361\n",
      "Episode 393 finished after 19 timesteps with r=19.0. Running score: 88.0101522843\n",
      "Episode 394 finished after 78 timesteps with r=78.0. Running score: 87.9848101266\n",
      "Episode 395 finished after 48 timesteps with r=48.0. Running score: 87.8838383838\n",
      "Episode 396 finished after 36 timesteps with r=36.0. Running score: 87.7531486146\n",
      "Episode 397 finished after 31 timesteps with r=31.0. Running score: 87.6105527638\n",
      "Episode 398 finished after 60 timesteps with r=60.0. Running score: 87.5413533835\n",
      "Episode 399 finished after 168 timesteps with r=168.0. Running score: 87.7425\n",
      "Episode 400 finished after 40 timesteps with r=40.0. Running score: 87.6234413965\n",
      "Episode 401 finished after 43 timesteps with r=43.0. Running score: 87.5124378109\n",
      "Episode 402 finished after 21 timesteps with r=21.0. Running score: 87.3473945409\n",
      "Episode 403 finished after 27 timesteps with r=27.0. Running score: 87.198019802\n",
      "Episode 404 finished after 38 timesteps with r=38.0. Running score: 87.0765432099\n",
      "Episode 405 finished after 24 timesteps with r=24.0. Running score: 86.921182266\n",
      "Episode 406 finished after 30 timesteps with r=30.0. Running score: 86.7813267813\n",
      "Episode 407 finished after 30 timesteps with r=30.0. Running score: 86.6421568627\n",
      "Episode 408 finished after 23 timesteps with r=23.0. Running score: 86.4865525672\n",
      "Episode 409 finished after 27 timesteps with r=27.0. Running score: 86.3414634146\n",
      "Episode 410 finished after 24 timesteps with r=24.0. Running score: 86.1897810219\n",
      "Episode 411 finished after 27 timesteps with r=27.0. Running score: 86.0461165049\n",
      "Episode 412 finished after 95 timesteps with r=95.0. Running score: 86.0677966102\n",
      "Episode 413 finished after 129 timesteps with r=129.0. Running score: 86.1714975845\n",
      "Episode 414 finished after 44 timesteps with r=44.0. Running score: 86.0698795181\n",
      "Episode 415 finished after 51 timesteps with r=51.0. Running score: 85.9855769231\n",
      "Episode 416 finished after 41 timesteps with r=41.0. Running score: 85.8776978417\n",
      "Episode 417 finished after 37 timesteps with r=37.0. Running score: 85.7607655502\n",
      "Episode 418 finished after 49 timesteps with r=49.0. Running score: 85.6730310263\n",
      "Episode 419 finished after 53 timesteps with r=53.0. Running score: 85.5952380952\n",
      "Episode 420 finished after 95 timesteps with r=95.0. Running score: 85.6175771971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 421 finished after 43 timesteps with r=43.0. Running score: 85.5165876777\n",
      "Episode 422 finished after 36 timesteps with r=36.0. Running score: 85.3995271868\n",
      "Episode 423 finished after 121 timesteps with r=121.0. Running score: 85.483490566\n",
      "Episode 424 finished after 167 timesteps with r=167.0. Running score: 85.6752941176\n",
      "Episode 425 finished after 162 timesteps with r=162.0. Running score: 85.8544600939\n",
      "Episode 426 finished after 41 timesteps with r=41.0. Running score: 85.7494145199\n",
      "Episode 427 finished after 39 timesteps with r=39.0. Running score: 85.6401869159\n",
      "Episode 428 finished after 31 timesteps with r=31.0. Running score: 85.5128205128\n",
      "Episode 429 finished after 28 timesteps with r=28.0. Running score: 85.3790697674\n",
      "Episode 430 finished after 336 timesteps with r=336.0. Running score: 85.9605568445\n",
      "Episode 431 finished after 28 timesteps with r=28.0. Running score: 85.8263888889\n",
      "Episode 432 finished after 68 timesteps with r=68.0. Running score: 85.7852193995\n",
      "Episode 433 finished after 46 timesteps with r=46.0. Running score: 85.6935483871\n",
      "Episode 434 finished after 104 timesteps with r=104.0. Running score: 85.7356321839\n",
      "Episode 435 finished after 42 timesteps with r=42.0. Running score: 85.6353211009\n",
      "Episode 436 finished after 30 timesteps with r=30.0. Running score: 85.5080091533\n",
      "Episode 437 finished after 31 timesteps with r=31.0. Running score: 85.3835616438\n",
      "Episode 438 finished after 39 timesteps with r=39.0. Running score: 85.277904328\n",
      "Episode 439 finished after 48 timesteps with r=48.0. Running score: 85.1931818182\n",
      "Episode 440 finished after 142 timesteps with r=142.0. Running score: 85.3219954649\n",
      "Episode 441 finished after 300 timesteps with r=300.0. Running score: 85.8076923077\n",
      "Episode 442 finished after 142 timesteps with r=142.0. Running score: 85.934537246\n",
      "Episode 443 finished after 139 timesteps with r=139.0. Running score: 86.0540540541\n",
      "Episode 444 finished after 61 timesteps with r=61.0. Running score: 85.997752809\n",
      "Episode 445 finished after 39 timesteps with r=39.0. Running score: 85.8923766816\n",
      "Episode 446 finished after 36 timesteps with r=36.0. Running score: 85.7807606264\n",
      "Episode 447 finished after 39 timesteps with r=39.0. Running score: 85.6763392857\n",
      "Episode 448 finished after 108 timesteps with r=108.0. Running score: 85.7260579065\n",
      "Episode 449 finished after 36 timesteps with r=36.0. Running score: 85.6155555556\n",
      "Episode 450 finished after 45 timesteps with r=45.0. Running score: 85.5254988914\n",
      "Episode 451 finished after 33 timesteps with r=33.0. Running score: 85.4092920354\n",
      "Episode 452 finished after 150 timesteps with r=150.0. Running score: 85.5518763797\n",
      "Episode 453 finished after 47 timesteps with r=47.0. Running score: 85.4669603524\n",
      "Episode 454 finished after 45 timesteps with r=45.0. Running score: 85.378021978\n",
      "Episode 455 finished after 42 timesteps with r=42.0. Running score: 85.2828947368\n",
      "Episode 456 finished after 58 timesteps with r=58.0. Running score: 85.2231947484\n",
      "Episode 457 finished after 50 timesteps with r=50.0. Running score: 85.1462882096\n",
      "Episode 458 finished after 100 timesteps with r=100.0. Running score: 85.1786492375\n",
      "Episode 459 finished after 358 timesteps with r=358.0. Running score: 85.7717391304\n",
      "Episode 460 finished after 49 timesteps with r=49.0. Running score: 85.6919739696\n",
      "Episode 461 finished after 71 timesteps with r=71.0. Running score: 85.6601731602\n",
      "Episode 462 finished after 92 timesteps with r=92.0. Running score: 85.6738660907\n",
      "Episode 463 finished after 92 timesteps with r=92.0. Running score: 85.6875\n",
      "Episode 464 finished after 165 timesteps with r=165.0. Running score: 85.8580645161\n",
      "Episode 465 finished after 59 timesteps with r=59.0. Running score: 85.8004291845\n",
      "Episode 466 finished after 142 timesteps with r=142.0. Running score: 85.9207708779\n",
      "Episode 467 finished after 80 timesteps with r=80.0. Running score: 85.9081196581\n",
      "Episode 468 finished after 38 timesteps with r=38.0. Running score: 85.8059701493\n",
      "Episode 469 finished after 188 timesteps with r=188.0. Running score: 86.0234042553\n",
      "Episode 470 finished after 61 timesteps with r=61.0. Running score: 85.9702760085\n",
      "Episode 471 finished after 31 timesteps with r=31.0. Running score: 85.8538135593\n",
      "Episode 472 finished after 75 timesteps with r=75.0. Running score: 85.8308668076\n",
      "Episode 473 finished after 57 timesteps with r=57.0. Running score: 85.7700421941\n",
      "Episode 474 finished after 51 timesteps with r=51.0. Running score: 85.6968421053\n",
      "Episode 475 finished after 237 timesteps with r=237.0. Running score: 86.0147058824\n",
      "Episode 476 finished after 150 timesteps with r=150.0. Running score: 86.1488469602\n",
      "Episode 477 finished after 59 timesteps with r=59.0. Running score: 86.0920502092\n",
      "Episode 478 finished after 122 timesteps with r=122.0. Running score: 86.1670146138\n",
      "Episode 479 finished after 36 timesteps with r=36.0. Running score: 86.0625\n",
      "Episode 480 finished after 60 timesteps with r=60.0. Running score: 86.0083160083\n",
      "Episode 481 finished after 92 timesteps with r=92.0. Running score: 86.020746888\n",
      "Episode 482 finished after 52 timesteps with r=52.0. Running score: 85.950310559\n",
      "Episode 483 finished after 48 timesteps with r=48.0. Running score: 85.8719008264\n",
      "Episode 484 finished after 116 timesteps with r=116.0. Running score: 85.9340206186\n",
      "Episode 485 finished after 65 timesteps with r=65.0. Running score: 85.8909465021\n",
      "Episode 486 finished after 48 timesteps with r=48.0. Running score: 85.8131416838\n",
      "Episode 487 finished after 41 timesteps with r=41.0. Running score: 85.7213114754\n",
      "Episode 488 finished after 63 timesteps with r=63.0. Running score: 85.6748466258\n",
      "Episode 489 finished after 60 timesteps with r=60.0. Running score: 85.6224489796\n",
      "Episode 490 finished after 62 timesteps with r=62.0. Running score: 85.5743380855\n",
      "Episode 491 finished after 55 timesteps with r=55.0. Running score: 85.512195122\n",
      "Episode 492 finished after 60 timesteps with r=60.0. Running score: 85.4604462475\n",
      "Episode 493 finished after 127 timesteps with r=127.0. Running score: 85.544534413\n",
      "Episode 494 finished after 342 timesteps with r=342.0. Running score: 86.0626262626\n",
      "Episode 495 finished after 112 timesteps with r=112.0. Running score: 86.1149193548\n",
      "Episode 496 finished after 74 timesteps with r=74.0. Running score: 86.0905432596\n",
      "Episode 497 finished after 56 timesteps with r=56.0. Running score: 86.0301204819\n",
      "Episode 498 finished after 53 timesteps with r=53.0. Running score: 85.9639278557\n",
      "Episode 499 finished after 51 timesteps with r=51.0. Running score: 85.894\n",
      "Episode 500 finished after 128 timesteps with r=128.0. Running score: 85.9780439122\n",
      "Episode 501 finished after 53 timesteps with r=53.0. Running score: 85.9123505976\n",
      "Episode 502 finished after 64 timesteps with r=64.0. Running score: 85.8687872763\n",
      "Episode 503 finished after 93 timesteps with r=93.0. Running score: 85.8829365079\n",
      "Episode 504 finished after 38 timesteps with r=38.0. Running score: 85.7881188119\n",
      "Episode 505 finished after 55 timesteps with r=55.0. Running score: 85.7272727273\n",
      "Episode 506 finished after 60 timesteps with r=60.0. Running score: 85.6765285996\n",
      "Episode 507 finished after 80 timesteps with r=80.0. Running score: 85.6653543307\n",
      "Episode 508 finished after 100 timesteps with r=100.0. Running score: 85.6935166994\n",
      "Episode 509 finished after 52 timesteps with r=52.0. Running score: 85.6274509804\n",
      "Episode 510 finished after 78 timesteps with r=78.0. Running score: 85.6125244618\n",
      "Episode 511 finished after 49 timesteps with r=49.0. Running score: 85.541015625\n",
      "Episode 512 finished after 117 timesteps with r=117.0. Running score: 85.6023391813\n",
      "Episode 513 finished after 41 timesteps with r=41.0. Running score: 85.5155642023\n",
      "Episode 514 finished after 47 timesteps with r=47.0. Running score: 85.440776699\n",
      "Episode 515 finished after 128 timesteps with r=128.0. Running score: 85.523255814\n",
      "Episode 516 finished after 43 timesteps with r=43.0. Running score: 85.4410058027\n",
      "Episode 517 finished after 237 timesteps with r=237.0. Running score: 85.7335907336\n",
      "Episode 518 finished after 60 timesteps with r=60.0. Running score: 85.6840077071\n",
      "Episode 519 finished after 111 timesteps with r=111.0. Running score: 85.7326923077\n",
      "Episode 520 finished after 48 timesteps with r=48.0. Running score: 85.660268714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 521 finished after 172 timesteps with r=172.0. Running score: 85.8256704981\n",
      "Episode 522 finished after 270 timesteps with r=270.0. Running score: 86.1778202677\n",
      "Episode 523 finished after 40 timesteps with r=40.0. Running score: 86.0896946565\n",
      "Episode 524 finished after 53 timesteps with r=53.0. Running score: 86.0266666667\n",
      "Episode 525 finished after 76 timesteps with r=76.0. Running score: 86.0076045627\n",
      "Episode 526 finished after 90 timesteps with r=90.0. Running score: 86.0151802657\n",
      "Episode 527 finished after 85 timesteps with r=85.0. Running score: 86.0132575758\n",
      "Episode 528 finished after 48 timesteps with r=48.0. Running score: 85.9413988658\n",
      "Episode 529 finished after 56 timesteps with r=56.0. Running score: 85.8849056604\n",
      "Episode 530 finished after 52 timesteps with r=52.0. Running score: 85.8210922787\n",
      "Episode 531 finished after 72 timesteps with r=72.0. Running score: 85.795112782\n",
      "Episode 532 finished after 46 timesteps with r=46.0. Running score: 85.7204502814\n",
      "Episode 533 finished after 49 timesteps with r=49.0. Running score: 85.6516853933\n",
      "Episode 534 finished after 125 timesteps with r=125.0. Running score: 85.7252336449\n",
      "Episode 535 finished after 124 timesteps with r=124.0. Running score: 85.796641791\n",
      "Episode 536 finished after 90 timesteps with r=90.0. Running score: 85.8044692737\n",
      "Episode 537 finished after 58 timesteps with r=58.0. Running score: 85.7527881041\n",
      "Episode 538 finished after 404 timesteps with r=404.0. Running score: 86.3432282004\n",
      "Episode 539 finished after 102 timesteps with r=102.0. Running score: 86.3722222222\n",
      "Episode 540 finished after 52 timesteps with r=52.0. Running score: 86.3086876155\n",
      "Episode 541 finished after 41 timesteps with r=41.0. Running score: 86.2250922509\n",
      "Episode 542 finished after 225 timesteps with r=225.0. Running score: 86.4806629834\n",
      "Episode 543 finished after 68 timesteps with r=68.0. Running score: 86.4466911765\n",
      "Episode 544 finished after 60 timesteps with r=60.0. Running score: 86.3981651376\n",
      "Episode 545 finished after 71 timesteps with r=71.0. Running score: 86.36996337\n",
      "Episode 546 finished after 59 timesteps with r=59.0. Running score: 86.3199268739\n",
      "Episode 547 finished after 45 timesteps with r=45.0. Running score: 86.2445255474\n",
      "Episode 548 finished after 52 timesteps with r=52.0. Running score: 86.1821493625\n",
      "Episode 549 finished after 87 timesteps with r=87.0. Running score: 86.1836363636\n",
      "Episode 550 finished after 67 timesteps with r=67.0. Running score: 86.1488203267\n",
      "Episode 551 finished after 110 timesteps with r=110.0. Running score: 86.1920289855\n",
      "Episode 552 finished after 230 timesteps with r=230.0. Running score: 86.452079566\n",
      "Episode 553 finished after 52 timesteps with r=52.0. Running score: 86.3898916968\n",
      "Episode 554 finished after 50 timesteps with r=50.0. Running score: 86.3243243243\n",
      "Episode 555 finished after 78 timesteps with r=78.0. Running score: 86.309352518\n",
      "Episode 556 finished after 180 timesteps with r=180.0. Running score: 86.4775583483\n",
      "Episode 557 finished after 282 timesteps with r=282.0. Running score: 86.8279569892\n",
      "Episode 558 finished after 97 timesteps with r=97.0. Running score: 86.8461538462\n",
      "Episode 559 finished after 50 timesteps with r=50.0. Running score: 86.7803571429\n",
      "Episode 560 finished after 58 timesteps with r=58.0. Running score: 86.7290552585\n",
      "Episode 561 finished after 127 timesteps with r=127.0. Running score: 86.8007117438\n",
      "Episode 562 finished after 327 timesteps with r=327.0. Running score: 87.2273534636\n",
      "Episode 563 finished after 313 timesteps with r=313.0. Running score: 87.6276595745\n",
      "Episode 564 finished after 54 timesteps with r=54.0. Running score: 87.5681415929\n",
      "Episode 565 finished after 65 timesteps with r=65.0. Running score: 87.5282685512\n",
      "Episode 566 finished after 50 timesteps with r=50.0. Running score: 87.4620811287\n",
      "Episode 567 finished after 169 timesteps with r=169.0. Running score: 87.6056338028\n",
      "Episode 568 finished after 120 timesteps with r=120.0. Running score: 87.6625659051\n",
      "Episode 569 finished after 152 timesteps with r=152.0. Running score: 87.7754385965\n",
      "Episode 570 finished after 222 timesteps with r=222.0. Running score: 88.0105078809\n",
      "Episode 571 finished after 71 timesteps with r=71.0. Running score: 87.9807692308\n",
      "Episode 572 finished after 46 timesteps with r=46.0. Running score: 87.907504363\n",
      "Episode 573 finished after 54 timesteps with r=54.0. Running score: 87.8484320557\n",
      "Episode 574 finished after 53 timesteps with r=53.0. Running score: 87.787826087\n",
      "Episode 575 finished after 102 timesteps with r=102.0. Running score: 87.8125\n",
      "Episode 576 finished after 319 timesteps with r=319.0. Running score: 88.2131715771\n",
      "Episode 577 finished after 49 timesteps with r=49.0. Running score: 88.1453287197\n",
      "Episode 578 finished after 69 timesteps with r=69.0. Running score: 88.1122625216\n",
      "Episode 579 finished after 61 timesteps with r=61.0. Running score: 88.0655172414\n",
      "Episode 580 finished after 52 timesteps with r=52.0. Running score: 88.0034423408\n",
      "Episode 581 finished after 68 timesteps with r=68.0. Running score: 87.9690721649\n",
      "Episode 582 finished after 128 timesteps with r=128.0. Running score: 88.0377358491\n",
      "Episode 583 finished after 55 timesteps with r=55.0. Running score: 87.9811643836\n",
      "Episode 584 finished after 48 timesteps with r=48.0. Running score: 87.9128205128\n",
      "Episode 585 finished after 55 timesteps with r=55.0. Running score: 87.8566552901\n",
      "Episode 586 finished after 112 timesteps with r=112.0. Running score: 87.8977853492\n",
      "Episode 587 finished after 126 timesteps with r=126.0. Running score: 87.962585034\n",
      "Episode 588 finished after 273 timesteps with r=273.0. Running score: 88.2767402377\n",
      "Episode 589 finished after 123 timesteps with r=123.0. Running score: 88.3355932203\n",
      "Episode 590 finished after 269 timesteps with r=269.0. Running score: 88.641285956\n",
      "Episode 591 finished after 173 timesteps with r=173.0. Running score: 88.7837837838\n",
      "Episode 592 finished after 74 timesteps with r=74.0. Running score: 88.7588532884\n",
      "Episode 593 finished after 118 timesteps with r=118.0. Running score: 88.8080808081\n",
      "Episode 594 finished after 55 timesteps with r=55.0. Running score: 88.7512605042\n",
      "Episode 595 finished after 92 timesteps with r=92.0. Running score: 88.7567114094\n",
      "Episode 596 finished after 497 timesteps with r=497.0. Running score: 89.4405360134\n",
      "Episode 597 finished after 100 timesteps with r=100.0. Running score: 89.4581939799\n",
      "Episode 598 finished after 45 timesteps with r=45.0. Running score: 89.3839732888\n",
      "Episode 599 finished after 55 timesteps with r=55.0. Running score: 89.3266666667\n",
      "Episode 600 finished after 148 timesteps with r=148.0. Running score: 89.4242928453\n",
      "Episode 601 finished after 173 timesteps with r=173.0. Running score: 89.5631229236\n",
      "Episode 602 finished after 55 timesteps with r=55.0. Running score: 89.5058043118\n",
      "Episode 603 finished after 131 timesteps with r=131.0. Running score: 89.5745033113\n",
      "Episode 604 finished after 123 timesteps with r=123.0. Running score: 89.6297520661\n",
      "Episode 605 finished after 43 timesteps with r=43.0. Running score: 89.5528052805\n",
      "Episode 606 finished after 45 timesteps with r=45.0. Running score: 89.4794069193\n",
      "Episode 607 finished after 72 timesteps with r=72.0. Running score: 89.4506578947\n",
      "Episode 608 finished after 42 timesteps with r=42.0. Running score: 89.3727422003\n",
      "Episode 609 finished after 220 timesteps with r=220.0. Running score: 89.5868852459\n",
      "Episode 610 finished after 131 timesteps with r=131.0. Running score: 89.6546644845\n",
      "Episode 611 finished after 53 timesteps with r=53.0. Running score: 89.5947712418\n",
      "Episode 612 finished after 258 timesteps with r=258.0. Running score: 89.8694942904\n",
      "Episode 613 finished after 122 timesteps with r=122.0. Running score: 89.9218241042\n",
      "Episode 614 finished after 142 timesteps with r=142.0. Running score: 90.006504065\n",
      "Episode 615 finished after 246 timesteps with r=246.0. Running score: 90.2597402597\n",
      "Episode 616 finished after 50 timesteps with r=50.0. Running score: 90.1944894652\n",
      "Episode 617 finished after 212 timesteps with r=212.0. Running score: 90.3915857605\n",
      "Episode 618 finished after 53 timesteps with r=53.0. Running score: 90.3311793215\n",
      "Episode 619 finished after 189 timesteps with r=189.0. Running score: 90.4903225806\n",
      "Episode 620 finished after 51 timesteps with r=51.0. Running score: 90.4267310789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 621 finished after 200 timesteps with r=200.0. Running score: 90.6028938907\n",
      "Episode 622 finished after 45 timesteps with r=45.0. Running score: 90.5296950241\n",
      "Episode 623 finished after 137 timesteps with r=137.0. Running score: 90.6041666667\n",
      "Episode 624 finished after 42 timesteps with r=42.0. Running score: 90.5264\n",
      "Episode 625 finished after 52 timesteps with r=52.0. Running score: 90.46485623\n",
      "Episode 626 finished after 123 timesteps with r=123.0. Running score: 90.5167464115\n",
      "Episode 627 finished after 52 timesteps with r=52.0. Running score: 90.4554140127\n",
      "Episode 628 finished after 82 timesteps with r=82.0. Running score: 90.4419713831\n",
      "Episode 629 finished after 91 timesteps with r=91.0. Running score: 90.4428571429\n",
      "Episode 630 finished after 88 timesteps with r=88.0. Running score: 90.4389857369\n",
      "Episode 631 finished after 62 timesteps with r=62.0. Running score: 90.3939873418\n",
      "Episode 632 finished after 126 timesteps with r=126.0. Running score: 90.4502369668\n",
      "Episode 633 finished after 44 timesteps with r=44.0. Running score: 90.3769716088\n",
      "Episode 634 finished after 60 timesteps with r=60.0. Running score: 90.3291338583\n",
      "Episode 635 finished after 253 timesteps with r=253.0. Running score: 90.5849056604\n",
      "Episode 636 finished after 85 timesteps with r=85.0. Running score: 90.5761381476\n",
      "Episode 637 finished after 137 timesteps with r=137.0. Running score: 90.6489028213\n",
      "Episode 638 finished after 62 timesteps with r=62.0. Running score: 90.6040688576\n",
      "Episode 639 finished after 56 timesteps with r=56.0. Running score: 90.55\n",
      "Episode 640 finished after 85 timesteps with r=85.0. Running score: 90.5413416537\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-036e595c1222>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m                         \u001b[0ms1_t_r\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1_r\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mINPUT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                         \u001b[0mQ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_pre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms1_t_r\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                         \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_r\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_r\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mDISCOUNT\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1120\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \"\"\"\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;31m# Did not find anything.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    280\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[0;32m--> 282\u001b[0;31m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[1;32m    283\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         raise TypeError('Fetch argument %r has invalid type %r, '\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3589\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3590\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3592\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3608\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"allow_tensor and allow_operation can't both be False.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3610\u001b[0;31m     \u001b[0mtemp_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_as_graph_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtemp_obj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3612\u001b[0m       \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_as_graph_element\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0motherwise\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m   \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m   \u001b[0mconv_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_as_graph_element\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconv_fn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconv_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "import random as ran\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# 꺼내서 사용할 리플레이 갯수\n",
    "REPLAY = 10\n",
    "# 리플레이를 저장할 리스트\n",
    "REPLAY_MEMORY = []\n",
    "# 미니배치\n",
    "MINIBATCH = 50\n",
    "\n",
    "INPUT = env.observation_space.shape[0]\n",
    "OUTPUT = env.action_space.n\n",
    "\n",
    "# 하이퍼파라미터\n",
    "LEARNING_LATE = 0.001\n",
    "NUM_EPISODE = 2000\n",
    "\n",
    "DISCOUNT = 0.98\n",
    "\n",
    "\n",
    "# 네트워크 구성\n",
    "x=tf.placeholder(dtype=tf.float32, shape=(1,4))\n",
    "\n",
    "W1 = tf.get_variable('W1',shape=[INPUT,16],initializer=tf.contrib.layers.xavier_initializer())\n",
    "W2 = tf.get_variable('W2',shape=[16, OUTPUT],initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "L1=tf.nn.relu(tf.matmul(x,W1))\n",
    "Q_pre = tf.matmul(L1,W2)\n",
    "\n",
    "y=tf.placeholder(dtype=tf.float32, shape=(1, env.action_space.n))\n",
    "\n",
    "# 손실 함수\n",
    "loss = tf.reduce_sum(tf.square(y-Q_pre))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_LATE)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "rList=[]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for episode in range(5000):\n",
    "\n",
    "        s = env.reset()\n",
    "\n",
    "        e = 1. / ((episode/25)+1)\n",
    "        rall = 0\n",
    "        d = False\n",
    "        count=0\n",
    "\n",
    "        while not d:\n",
    "            # env.render()\n",
    "            count+=1\n",
    "\n",
    "            # 현재 상태(s)로 Q값을 예측\n",
    "            s_t = np.reshape(s,[1,INPUT])\n",
    "            Q = sess.run(Q_pre, feed_dict={x:s_t})\n",
    "\n",
    "            # e-greedy 를 사용하여 action값 구함\n",
    "            if e > np.random.rand(1):\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                a = np.argmax(Q)\n",
    "\n",
    "            # action을 취함\n",
    "            s1, r, d, _ = env.step(a)\n",
    "\n",
    "            # state, action, reward, next_state, done 을 메모리에 저장\n",
    "            REPLAY_MEMORY.append([s_t,a,r,s1,d])\n",
    "\n",
    "            # 메모리에 50000개 이상의 값이 들어가면 가장 먼저 들어간 것부터 삭제\n",
    "            if len(REPLAY_MEMORY) > 50000:\n",
    "                del REPLAY_MEMORY[0]\n",
    "\n",
    "            rall += r\n",
    "            s = s1\n",
    "\n",
    "        # 10 번의 스탭마다 미니배치로 학습\n",
    "        if episode % 10 == 1 :\n",
    "\n",
    "            for i in range(MINIBATCH):\n",
    "\n",
    "                # 메모리에서 사용할 리플레이를 랜덤하게 가져옴\n",
    "                for sample in ran.sample(REPLAY_MEMORY, REPLAY):\n",
    "\n",
    "                    s_t_r, a_r, r_r, s1_r ,d_r = sample\n",
    "\n",
    "                    # DQN 알고리즘으로 학습\n",
    "                    if d_r:\n",
    "                        Q[0, a_r] = -100\n",
    "                    else:\n",
    "                        s1_t_r= np.reshape(s1_r,[1,INPUT])\n",
    "\n",
    "                        Q1 = sess.run(Q_pre, feed_dict={x: s1_t_r})\n",
    "\n",
    "                        Q[0, a_r] = r_r + DISCOUNT * np.max(Q1)\n",
    "\n",
    "                    sess.run(train, feed_dict={x: s_t_r, y: Q})\n",
    "\n",
    "\n",
    "\n",
    "        rList.append(rall)\n",
    "        print(\"Episode {} finished after {} timesteps with r={}. Running score: {}\".format(episode, count, rall, np.mean(rList)))\n",
    "\n",
    "\n",
    "    for episode in range(500):\n",
    "        # state 초기화\n",
    "        s = env.reset()\n",
    "\n",
    "        rall = 0\n",
    "        d = False\n",
    "        count = 0\n",
    "        # 에피소드가 끝나기 전까지 반복\n",
    "        while not d :\n",
    "            env.render()\n",
    "            count += 1\n",
    "            # state 값의 전처리\n",
    "            s_t = np.reshape(s, [1, INPUT])\n",
    "\n",
    "            # 현재 상태의 Q값을 에측\n",
    "            Q = sess.run(Q_pre, feed_dict={x: s_t})\n",
    "            a = np.argmax(Q)\n",
    "\n",
    "            # 결정된 action으로 Environment에 입력\n",
    "            s, r, d, _ = env.step(a)\n",
    "\n",
    "            # 총 reward 합\n",
    "            rall += r\n",
    "\n",
    "\n",
    "        rList.append(rall)\n",
    "\n",
    "        print(\"Episode : {} steps : {} r={}. averge reward : {}\".format(episode, count, rall,\n",
    "                                                                        np.mean(rList)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
