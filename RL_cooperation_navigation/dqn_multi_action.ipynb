{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN to cooperate 2 Mobile Manipulator\n",
    "* made by hotae\n",
    "* network includes multi agent's action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "\n",
    "class ENV:\n",
    "    def __init__(self, map_size, obs_pos1, obs_pos2, robot_start_pos, goal_pos):\n",
    "        self.map_size = map_size # integer\n",
    "        self.obs_pos1 = obs_pos1 # [a,b], 2by1 list\n",
    "        self.obs_pos2 = obs_pos2 # [a,b], 2by1 list\n",
    "        self.goal_pos = goal_pos\n",
    "        self.robot_pos1 = robot_start_pos[0:2] #[a,b] 2by1 list\n",
    "        self.robot_pos2 = robot_start_pos[2:4]\n",
    "        # set the walls\n",
    "        self.fig = plt.figure()\n",
    "        ax = plt.axes(xlim=(-0.5,self.map_size), ylim=(-0.5,self.map_size))  \n",
    "        #self.render_env()\n",
    "    def render_env(self):        \n",
    "        # draw the obstacles and goal\n",
    "        obs1 = plt.scatter(self.obs_pos1[0], self.obs_pos1[1], c='r', marker = 's', linewidths = 5) # have to check whether we can receive <list or np.array>        \n",
    "        obs2 = plt.scatter(self.obs_pos2[0], self.obs_pos2[1], c='r', marker = 's', linewidths = 5)\n",
    "        goal = plt.scatter(self.goal_pos[0], self.goal_pos[1], c='g', marker='x', linewidths = 4)\n",
    "        # draw the robot                \n",
    "        ro1 = plt.scatter(self.robot_pos1[0], self.robot_pos1[1], c='b', linewidths = 3)\n",
    "        ro2 = plt.scatter(self.robot_pos2[0], self.robot_pos2[1], c='b', linewidths = 3)        \n",
    "        self.fig.canvas.draw()   \n",
    "        sleep(0.2)\n",
    "        ro1.remove()\n",
    "        ro2.remove()        \n",
    "    def update(self, robot_current_pos):\n",
    "        self.robot_pos1 = robot_current_pos[0:2]\n",
    "        self.robot_pos2 = robot_current_pos[2:4]        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./dqn_multi_action.ckpt\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overriden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"640\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode     0]  total reward: -286.492815165     steps:   500    q:-4.84460020065, failure\n",
      "[Episode     1]  total reward: -255.699367017     steps:   500    q:-22.6124725342, failure\n",
      "[Episode     2]  total reward: -132.28544281     steps:   500    q:-38.6128349304, failure\n",
      "[Episode     3]  total reward: -217.558275049     steps:   500    q:-56.4258880615, failure\n",
      "[Episode     4]  total reward: -98.0841199215     steps:   500    q:-58.5988540649, failure\n",
      "[Episode     5]  total reward: -225.147493563     steps:   500    q:-61.2444190979, failure\n",
      "[Episode     6]  total reward: -280.51588146     steps:   500    q:-66.4735183716, failure\n",
      "[Episode     7]  total reward: -233.947482609    steps:   215    q:-75.3097076416, success\n",
      "[Episode     8]  total reward: -409.494247334     steps:   500    q:-88.2991867065, failure\n",
      "[Episode     9]  total reward: -270.814820347     steps:   500    q:-75.2669372559, failure\n",
      "[Episode    10]  total reward: -165.139959178     steps:   500    q:-93.7340927124, failure\n",
      "[Episode    11]  total reward: -148.979153425     steps:   500    q:-93.9824829102, failure\n",
      "[Episode    12]  total reward: -212.185668681     steps:   500    q:-93.0567779541, failure\n",
      "[Episode    13]  total reward: -71.0030236952     steps:   500    q:-93.8215637207, failure\n",
      "[Episode    14]  total reward: -492.588142777     steps:   500    q:-103.210731506, failure\n",
      "[Episode    15]  total reward: -256.900549169     steps:   500    q:-97.3711090088, failure\n",
      "[Episode    16]  total reward: -220.614494624     steps:   500    q:-67.6491699219, failure\n",
      "[Episode    17]  total reward: -199.765373675     steps:   500    q:-89.5392837524, failure\n",
      "[Episode    18]  total reward: -182.445499144     steps:   500    q:-84.1841125488, failure\n",
      "[Episode    19]  total reward: -49.87828097     steps:   500    q:-95.9623031616, failure\n",
      "[Episode    20]  total reward: -272.275743695     steps:   500    q:-109.191719055, failure\n",
      "[Episode    21]  total reward: -139.658724149     steps:   500    q:-79.6491241455, failure\n",
      "[Episode    22]  total reward: -297.439227675     steps:   500    q:-82.3819656372, failure\n",
      "[Episode    23]  total reward: -291.49693323     steps:   500    q:-117.564651489, failure\n",
      "[Episode    24]  total reward: -324.730545236     steps:   500    q:-106.847434998, failure\n",
      "[Episode    25]  total reward: -338.283977243     steps:   500    q:-124.671440125, failure\n",
      "[Episode    26]  total reward: -392.461075079     steps:   500    q:-92.999168396, failure\n",
      "[Episode    27]  total reward: -152.983716752     steps:   500    q:-126.191390991, failure\n",
      "[Episode    28]  total reward: -134.443259251     steps:   500    q:-90.9945526123, failure\n",
      "[Episode    29]  total reward: -316.909193311     steps:   500    q:-113.246002197, failure\n",
      "[Episode    30]  total reward: -138.616638755     steps:   500    q:-104.132987976, failure\n",
      "[Episode    31]  total reward: -71.72252471     steps:   500    q:-93.6697387695, failure\n",
      "[Episode    32]  total reward: -79.6284616213     steps:   500    q:-106.774971008, failure\n",
      "[Episode    33]  total reward: -167.700268978     steps:   500    q:-106.204956055, failure\n",
      "[Episode    34]  total reward: -292.888954208     steps:   500    q:-102.316589355, failure\n",
      "[Episode    35]  total reward: -247.91933077     steps:   500    q:-132.399673462, failure\n",
      "[Episode    36]  total reward: -152.127903254     steps:   500    q:-132.71446228, failure\n",
      "[Episode    37]  total reward: -300.843073891     steps:   500    q:-101.624809265, failure\n",
      "[Episode    38]  total reward: -90.8951030862     steps:   500    q:-100.778656006, failure\n",
      "[Episode    39]  total reward: -108.085231187     steps:   500    q:-112.265533447, failure\n",
      "[Episode    40]  total reward: -226.013749075     steps:   500    q:-117.660606384, failure\n",
      "[Episode    41]  total reward: -217.18583197     steps:   500    q:-48.3709869385, failure\n",
      "[Episode    42]  total reward: -244.972270493     steps:   500    q:-127.323524475, failure\n",
      "[Episode    43]  total reward: -147.29284734     steps:   500    q:-93.5438842773, failure\n",
      "[Episode    44]  total reward: -237.606662339     steps:   500    q:-92.0609970093, failure\n",
      "[Episode    45]  total reward: -183.039392944     steps:   500    q:-127.33556366, failure\n",
      "[Episode    46]  total reward: -181.982554381     steps:   500    q:-116.826370239, failure\n",
      "[Episode    47]  total reward: -382.500609119     steps:   500    q:-111.565956116, failure\n",
      "[Episode    48]  total reward: -102.676141069     steps:   500    q:-137.969909668, failure\n",
      "[Episode    49]  total reward: -34.9052078545     steps:   500    q:-104.087547302, failure\n",
      "[Episode    50]  total reward: -70.3570345278     steps:   500    q:-92.9945755005, failure\n",
      "[Episode    51]  total reward: -37.84164574     steps:   500    q:-113.534492493, failure\n",
      "[Episode    52]  total reward: -231.43863358     steps:   500    q:-106.730255127, failure\n",
      "[Episode    53]  total reward: -122.688969375     steps:   500    q:-115.786811829, failure\n",
      "[Episode    54]  total reward: -146.712458803     steps:   500    q:-118.57472229, failure\n",
      "[Episode    55]  total reward: -155.932489194     steps:   500    q:-134.243850708, failure\n",
      "[Episode    56]  total reward: -258.906263739     steps:   500    q:-149.282913208, failure\n",
      "[Episode    57]  total reward: -286.062237616     steps:   500    q:-174.370452881, failure\n",
      "[Episode    58]  total reward: -136.851435241     steps:   500    q:-142.202163696, failure\n",
      "[Episode    59]  total reward: -218.264794114     steps:   500    q:-153.559967041, failure\n",
      "[Episode    60]  total reward: -7.95784626563     steps:   500    q:-121.193656921, failure\n",
      "[Episode    61]  total reward: -215.931329534     steps:   500    q:-84.1259002686, failure\n",
      "[Episode    62]  total reward: -299.385074368     steps:   500    q:-118.226516724, failure\n",
      "[Episode    63]  total reward: -131.207433465     steps:   500    q:-121.305114746, failure\n",
      "[Episode    64]  total reward: -417.773230728     steps:   500    q:-121.474884033, failure\n",
      "[Episode    65]  total reward: -380.228822717     steps:   500    q:-141.83795166, failure\n",
      "[Episode    66]  total reward: -337.016500131     steps:   500    q:-102.145065308, failure\n",
      "[Episode    67]  total reward: -108.382242832     steps:   500    q:-97.8230133057, failure\n",
      "[Episode    68]  total reward: -259.202873416     steps:   500    q:-145.64125061, failure\n",
      "[Episode    69]  total reward: -205.159995966     steps:   500    q:-102.895500183, failure\n",
      "[Episode    70]  total reward: -178.326896059     steps:   500    q:-106.756668091, failure\n",
      "[Episode    71]  total reward: -181.269424893     steps:   500    q:-106.819374084, failure\n",
      "[Episode    72]  total reward: -216.605940331     steps:   500    q:-93.9781265259, failure\n",
      "[Episode    73]  total reward: -502.328188628     steps:   500    q:-101.397155762, failure\n",
      "[Episode    74]  total reward: -125.158125002     steps:   500    q:-103.512527466, failure\n",
      "[Episode    75]  total reward: -271.292480262     steps:   500    q:-107.4008255, failure\n",
      "[Episode    76]  total reward: -108.249391394     steps:   500    q:-99.3544235229, failure\n",
      "[Episode    77]  total reward: -123.953901547     steps:   500    q:-89.0287094116, failure\n",
      "[Episode    78]  total reward: -265.117569523     steps:   500    q:-97.9612350464, failure\n",
      "[Episode    79]  total reward: -157.846974222     steps:   500    q:-112.780128479, failure\n",
      "[Episode    80]  total reward: -5.87829018647     steps:   500    q:-135.306655884, failure\n",
      "[Episode    81]  total reward: -302.24886757     steps:   500    q:-137.785217285, failure\n",
      "[Episode    82]  total reward: -93.9858184522     steps:   500    q:-120.361099243, failure\n",
      "[Episode    83]  total reward: -233.164727072     steps:   500    q:-118.966903687, failure\n",
      "[Episode    84]  total reward: -341.729740355     steps:   500    q:-127.785018921, failure\n",
      "[Episode    85]  total reward: -58.108899481     steps:   500    q:-150.141815186, failure\n",
      "[Episode    86]  total reward: -395.06605536     steps:   500    q:-96.4331893921, failure\n",
      "[Episode    87]  total reward: -73.7119453045     steps:   500    q:-132.87449646, failure\n",
      "[Episode    88]  total reward: -185.355064752     steps:   500    q:-52.1104545593, failure\n",
      "[Episode    89]  total reward: -107.949713538     steps:   500    q:-149.96635437, failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode    90]  total reward: -161.771879282     steps:   500    q:-130.876098633, failure\n",
      "[Episode    91]  total reward: -82.2477839128     steps:   500    q:-134.728591919, failure\n",
      "[Episode    92]  total reward: -178.205846038     steps:   500    q:-145.209091187, failure\n",
      "[Episode    93]  total reward: -334.226669835     steps:   500    q:-99.9947662354, failure\n",
      "[Episode    94]  total reward: -201.751581787     steps:   500    q:-96.3139266968, failure\n",
      "[Episode    95]  total reward: -164.896946126     steps:   500    q:-110.575469971, failure\n",
      "[Episode    96]  total reward: -333.876361003     steps:   500    q:-110.668045044, failure\n",
      "[Episode    97]  total reward: -47.898685657     steps:   500    q:-114.62827301, failure\n",
      "[Episode    98]  total reward: -263.43545558     steps:   500    q:-87.0811157227, failure\n",
      "[Episode    99]  total reward: -170.11690747     steps:   500    q:-108.217079163, failure\n",
      "[Episode   100]  total reward: -315.048077609     steps:   500    q:-77.8020629883, failure\n",
      "[Episode   101]  total reward: -363.928573161     steps:   500    q:-93.8879928589, failure\n",
      "[Episode   102]  total reward: -288.755317268     steps:   500    q:-131.630279541, failure\n",
      "[Episode   103]  total reward: -135.315372773     steps:   500    q:-101.900924683, failure\n",
      "[Episode   104]  total reward: -335.212869499     steps:   500    q:-86.0723114014, failure\n",
      "[Episode   105]  total reward: -165.514868078     steps:   500    q:-108.237800598, failure\n",
      "[Episode   106]  total reward: -144.768226541     steps:   500    q:-95.8619689941, failure\n",
      "[Episode   107]  total reward: -485.228926372     steps:   500    q:-127.313606262, failure\n",
      "[Episode   108]  total reward: -172.01627005     steps:   500    q:-136.142318726, failure\n",
      "[Episode   109]  total reward: -141.764885223     steps:   500    q:-70.4716262817, failure\n",
      "[Episode   110]  total reward: -290.606448413     steps:   500    q:-115.467681885, failure\n",
      "[Episode   111]  total reward: -103.394036109     steps:   500    q:-98.7227478027, failure\n",
      "[Episode   112]  total reward: -236.865142409     steps:   500    q:-100.592597961, failure\n",
      "[Episode   113]  total reward: -204.004588226     steps:   500    q:-134.373901367, failure\n",
      "[Episode   114]  total reward: -234.331444569     steps:   500    q:-116.898551941, failure\n",
      "[Episode   115]  total reward: -393.242491005     steps:   500    q:-112.393028259, failure\n",
      "[Episode   116]  total reward: -64.8554574782     steps:   500    q:-102.688873291, failure\n",
      "[Episode   117]  total reward: -47.8422601474     steps:   500    q:-95.1170730591, failure\n",
      "[Episode   118]  total reward: -131.434303641     steps:   500    q:-99.0948562622, failure\n",
      "[Episode   119]  total reward: -181.632475496     steps:   500    q:-98.7674255371, failure\n",
      "[Episode   120]  total reward: -297.968731823     steps:   500    q:-93.7304458618, failure\n",
      "[Episode   121]  total reward: -343.268677621     steps:   500    q:-88.3662185669, failure\n",
      "[Episode   122]  total reward: -147.123623909     steps:   500    q:-72.5772323608, failure\n",
      "[Episode   123]  total reward: -9.52737799063     steps:   500    q:-48.642375946, failure\n",
      "[Episode   124]  total reward: -61.1350028546     steps:   500    q:-61.6905021667, failure\n",
      "[Episode   125]  total reward: -38.2840691533    steps:   414    q:-77.9105529785, success\n",
      "[Episode   126]  total reward: -131.706637836     steps:   500    q:-75.4845657349, failure\n",
      "[Episode   127]  total reward: -51.641678532     steps:   500    q:-92.2809906006, failure\n",
      "[Episode   128]  total reward: -82.1465880012     steps:   500    q:-27.2682418823, failure\n",
      "[Episode   129]  total reward: -210.348212326     steps:   500    q:-60.3711547852, failure\n",
      "[Episode   130]  total reward: -162.405496392     steps:   500    q:-21.3505153656, failure\n",
      "[Episode   131]  total reward: -46.9155637699     steps:   500    q:-19.4255867004, failure\n",
      "[Episode   132]  total reward: -352.151912473     steps:   500    q:-52.033367157, failure\n",
      "[Episode   133]  total reward: -243.09818434     steps:   500    q:-46.9290771484, failure\n",
      "[Episode   134]  total reward: -335.224633034     steps:   500    q:-45.8963050842, failure\n",
      "[Episode   135]  total reward: -397.997677521     steps:   500    q:-53.6464233398, failure\n",
      "[Episode   136]  total reward: -201.855129262     steps:   500    q:-74.8143157959, failure\n",
      "[Episode   137]  total reward: -229.938281837     steps:   500    q:-54.9556121826, failure\n",
      "[Episode   138]  total reward: -189.945676179    steps:   110    q:-34.3286895752, success\n",
      "[Episode   139]  total reward: 655.431789263    steps:    20    q:223.635940552, success\n",
      "[Episode   140]  total reward: -244.092678353     steps:   500    q:110.316184998, failure\n",
      "[Episode   141]  total reward: -382.256649319     steps:   500    q:-10.5686721802, failure\n",
      "[Episode   142]  total reward: -235.703386364    steps:   459    q:243.104888916, success\n",
      "[Episode   143]  total reward: -336.042522431    steps:   340    q:35.9216346741, success\n",
      "[Episode   144]  total reward: -242.882142668     steps:   500    q:95.8297195435, failure\n",
      "[Episode   145]  total reward: -377.949515325     steps:   500    q:-20.6633586884, failure\n",
      "[Episode   146]  total reward: -68.7830568546     steps:   500    q:174.738235474, failure\n",
      "[Episode   147]  total reward: -7.76306489639    steps:   184    q:275.946929932, success\n",
      "[Episode   148]  total reward: -258.476428294    steps:   237    q:269.95501709, success\n",
      "[Episode   149]  total reward: -181.652780358     steps:   500    q:408.567932129, failure\n",
      "[Episode   150]  total reward: -295.105575039     steps:   500    q:33.2324981689, failure\n",
      "[Episode   151]  total reward: -452.0941486     steps:   500    q:-49.0331497192, failure\n",
      "[Episode   152]  total reward: -237.78435846     steps:   500    q:-67.5823364258, failure\n",
      "[Episode   153]  total reward: -165.859039676     steps:   500    q:-37.7761192322, failure\n",
      "[Episode   154]  total reward: -218.421759631     steps:   500    q:0.511539041996, failure\n",
      "[Episode   155]  total reward: -72.6094114466     steps:   500    q:-58.3702278137, failure\n",
      "[Episode   156]  total reward: -120.267842671     steps:   500    q:82.0495758057, failure\n",
      "[Episode   157]  total reward: -136.148139443     steps:   500    q:-61.4223823547, failure\n",
      "[Episode   158]  total reward: -93.4417349341     steps:   500    q:-6.94467973709, failure\n",
      "[Episode   159]  total reward: -280.009502802     steps:   500    q:7.95569038391, failure\n",
      "[Episode   160]  total reward: -69.5322471106     steps:   500    q:-50.1227416992, failure\n",
      "[Episode   161]  total reward: -341.920180607     steps:   500    q:-41.4509468079, failure\n",
      "[Episode   162]  total reward: -148.967114596     steps:   500    q:-6.57620000839, failure\n",
      "[Episode   163]  total reward: -63.4193544493     steps:   500    q:-18.9545001984, failure\n",
      "[Episode   164]  total reward: -103.879918652     steps:   500    q:-5.33908319473, failure\n",
      "[Episode   165]  total reward: -132.874325402     steps:   500    q:-27.8129005432, failure\n",
      "[Episode   166]  total reward: -76.8861772996     steps:   500    q:-34.5402412415, failure\n",
      "[Episode   167]  total reward: -103.994122245     steps:   500    q:-24.8287658691, failure\n",
      "[Episode   168]  total reward: -58.8629141436     steps:   500    q:-24.5300788879, failure\n",
      "[Episode   169]  total reward: -105.659823096     steps:   500    q:-9.90113830566, failure\n",
      "[Episode   170]  total reward: -142.372594904     steps:   500    q:-17.8581600189, failure\n",
      "[Episode   171]  total reward: -142.855286668     steps:   500    q:-18.6157341003, failure\n",
      "[Episode   172]  total reward: -112.652067562     steps:   500    q:-27.5675811768, failure\n",
      "[Episode   173]  total reward: -216.575615983     steps:   500    q:-39.6892356873, failure\n",
      "[Episode   174]  total reward: -114.839327792     steps:   500    q:-36.8725090027, failure\n",
      "[Episode   175]  total reward: -105.197208664     steps:   500    q:-78.0900726318, failure\n",
      "[Episode   176]  total reward: -153.937452584     steps:   500    q:-8.62752914429, failure\n",
      "[Episode   177]  total reward: -226.539597862     steps:   500    q:-71.193687439, failure\n",
      "[Episode   178]  total reward: -36.1975232032     steps:   500    q:-45.6191101074, failure\n",
      "[Episode   179]  total reward: -188.044863487     steps:   500    q:-54.9332504272, failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode   180]  total reward: -58.6537993784     steps:   500    q:-6.9366145134, failure\n",
      "[Episode   181]  total reward: -351.174177624     steps:   500    q:-57.5476722717, failure\n",
      "[Episode   182]  total reward: -468.284382254     steps:   500    q:-72.186050415, failure\n",
      "[Episode   183]  total reward: -374.673737667     steps:   500    q:-26.8029022217, failure\n",
      "[Episode   184]  total reward: -205.864023786     steps:   500    q:-17.3542613983, failure\n",
      "[Episode   185]  total reward: -285.970974147     steps:   500    q:-53.400932312, failure\n",
      "[Episode   186]  total reward: -336.737194471     steps:   500    q:-58.862411499, failure\n",
      "[Episode   187]  total reward: -435.063702015     steps:   500    q:-41.4263343811, failure\n",
      "[Episode   188]  total reward: -242.090872141     steps:   500    q:-39.3208999634, failure\n",
      "[Episode   189]  total reward: -298.293043834     steps:   500    q:-81.2054977417, failure\n",
      "[Episode   190]  total reward: -421.515690723     steps:   500    q:-10.9245824814, failure\n",
      "[Episode   191]  total reward: -329.803211057     steps:   500    q:-137.968383789, failure\n",
      "[Episode   192]  total reward: -74.5566689127     steps:   500    q:-67.153175354, failure\n",
      "[Episode   193]  total reward: -151.825438917     steps:   500    q:3.73973035812, failure\n",
      "[Episode   194]  total reward: -342.158711485     steps:   500    q:-54.4643669128, failure\n",
      "[Episode   195]  total reward: -280.468735384     steps:   500    q:-57.7579689026, failure\n",
      "[Episode   196]  total reward: -447.366152837     steps:   500    q:-127.067756653, failure\n",
      "[Episode   197]  total reward: -208.044291787     steps:   500    q:-55.4360847473, failure\n",
      "[Episode   198]  total reward: -266.725600645     steps:   500    q:-51.9381790161, failure\n",
      "[Episode   199]  total reward: -458.624665186     steps:   500    q:-193.740509033, failure\n",
      "[Episode   200]  total reward: -333.876762915     steps:   500    q:-162.782989502, failure\n",
      "[Episode   201]  total reward: -416.029412436     steps:   500    q:-38.6265068054, failure\n",
      "[Episode   202]  total reward: -452.601753092     steps:   500    q:-134.456390381, failure\n",
      "[Episode   203]  total reward: -264.271894718     steps:   500    q:-101.296234131, failure\n",
      "[Episode   204]  total reward: -333.83330792     steps:   500    q:-189.000640869, failure\n",
      "[Episode   205]  total reward: -479.38243847     steps:   500    q:-142.706848145, failure\n",
      "[Episode   206]  total reward: -264.312736827     steps:   500    q:-141.536117554, failure\n",
      "[Episode   207]  total reward: -486.057082794     steps:   500    q:-151.944885254, failure\n",
      "[Episode   208]  total reward: -369.165557503     steps:   500    q:-142.879547119, failure\n",
      "[Episode   209]  total reward: -402.849132609     steps:   500    q:-124.22592926, failure\n",
      "[Episode   210]  total reward: -392.63064211     steps:   500    q:-130.644058228, failure\n",
      "[Episode   211]  total reward: -364.906792852     steps:   500    q:-121.102867126, failure\n",
      "[Episode   212]  total reward: -435.767380605     steps:   500    q:-169.468231201, failure\n",
      "[Episode   213]  total reward: -381.693263391     steps:   500    q:-158.922424316, failure\n",
      "[Episode   214]  total reward: -358.630872778     steps:   500    q:-141.505401611, failure\n",
      "[Episode   215]  total reward: -429.192409909     steps:   500    q:-170.658569336, failure\n",
      "[Episode   216]  total reward: -510.653864279     steps:   500    q:-192.79447937, failure\n",
      "[Episode   217]  total reward: -497.396389379     steps:   500    q:-207.301040649, failure\n",
      "[Episode   218]  total reward: -230.2346708     steps:   500    q:-184.258102417, failure\n",
      "[Episode   219]  total reward: -446.143704802     steps:   500    q:-155.114440918, failure\n",
      "[Episode   220]  total reward: -407.638703212     steps:   500    q:-200.234375, failure\n",
      "[Episode   221]  total reward: -396.432098297     steps:   500    q:-173.056411743, failure\n",
      "[Episode   222]  total reward: -464.278076092     steps:   500    q:-219.432846069, failure\n",
      "[Episode   223]  total reward: -339.049013799     steps:   500    q:-189.717514038, failure\n",
      "[Episode   224]  total reward: -315.019938169     steps:   500    q:-178.003265381, failure\n",
      "[Episode   225]  total reward: -473.168094033     steps:   500    q:-205.861221313, failure\n",
      "[Episode   226]  total reward: -437.177214993     steps:   500    q:-221.826965332, failure\n",
      "[Episode   227]  total reward: -302.973510466     steps:   500    q:-228.638412476, failure\n",
      "[Episode   228]  total reward: -378.204787372     steps:   500    q:-206.800537109, failure\n",
      "[Episode   229]  total reward: -309.005542765     steps:   500    q:-188.36378479, failure\n",
      "[Episode   230]  total reward: -354.964407421     steps:   500    q:-206.079742432, failure\n",
      "[Episode   231]  total reward: -404.35152133     steps:   500    q:-238.291275024, failure\n",
      "[Episode   232]  total reward: -380.158184465     steps:   500    q:-202.511138916, failure\n",
      "[Episode   233]  total reward: -283.655893694     steps:   500    q:-194.358230591, failure\n",
      "[Episode   234]  total reward: -55.8381628834     steps:   500    q:-185.094161987, failure\n",
      "[Episode   235]  total reward: -381.297832835     steps:   500    q:-258.905731201, failure\n",
      "[Episode   236]  total reward: -58.3683557293    steps:   171    q:-56.4984970093, success\n",
      "[Episode   237]  total reward: -88.4399440673     steps:   500    q:-119.377532959, failure\n",
      "[Episode   238]  total reward: -283.294593384     steps:   500    q:-139.869232178, failure\n",
      "[Episode   239]  total reward: -420.5208023     steps:   500    q:-199.962844849, failure\n",
      "[Episode   240]  total reward: -450.010197847    steps:   270    q:-176.078262329, success\n",
      "[Episode   241]  total reward: -335.728261422     steps:   500    q:-177.928970337, failure\n",
      "[Episode   242]  total reward: -343.047051756    steps:   305    q:-108.487167358, success\n",
      "[Episode   243]  total reward: -458.464128537    steps:   427    q:157.439041138, success\n",
      "[Episode   244]  total reward: -398.973166765     steps:   500    q:452.057739258, failure\n",
      "[Episode   245]  total reward: -18.6616993934    steps:   267    q:400.285339355, success\n",
      "[Episode   246]  total reward: -122.578682523    steps:   376    q:629.492614746, success\n",
      "[Episode   247]  total reward: -134.674112181     steps:   500    q:490.170257568, failure\n",
      "[Episode   248]  total reward: -390.102978111     steps:   500    q:-154.389068604, failure\n",
      "[Episode   249]  total reward: -205.307527968     steps:   500    q:209.197341919, failure\n",
      "[Episode   250]  total reward: -575.715230306     steps:   500    q:213.411819458, failure\n",
      "[Episode   251]  total reward: -559.786618861     steps:   500    q:-149.947143555, failure\n",
      "[Episode   252]  total reward: -506.197846672     steps:   500    q:-61.9925384521, failure\n",
      "[Episode   253]  total reward: -313.04182738     steps:   500    q:19.6812000275, failure\n",
      "[Episode   254]  total reward: -453.720333924     steps:   500    q:87.5479431152, failure\n",
      "[Episode   255]  total reward: 610.41043601    steps:    22    q:355.823852539, success\n",
      "[Episode   256]  total reward: -284.626182398    steps:   206    q:-83.0735015869, success\n",
      "[Episode   257]  total reward: -521.735649824    steps:   387    q:-200.079391479, success\n",
      "[Episode   258]  total reward: -236.373772146    steps:   174    q:396.540985107, success\n",
      "[Episode   259]  total reward: -378.961307459     steps:   500    q:-196.995162964, failure\n",
      "[Episode   260]  total reward: -468.835345465     steps:   500    q:82.7125320435, failure\n",
      "[Episode   261]  total reward: -568.924163438    steps:   369    q:-102.074821472, success\n",
      "[Episode   262]  total reward: -428.952519093     steps:   500    q:-119.308036804, failure\n",
      "[Episode   263]  total reward: -361.631962843     steps:   500    q:-73.1584243774, failure\n",
      "[Episode   264]  total reward: -236.486442547     steps:   500    q:-96.397315979, failure\n",
      "[Episode   265]  total reward: -100.293973974     steps:   500    q:-96.2742156982, failure\n",
      "[Episode   266]  total reward: -230.761255825     steps:   500    q:-37.7065429688, failure\n",
      "[Episode   267]  total reward: -53.6746340272     steps:   500    q:-22.6175785065, failure\n",
      "[Episode   268]  total reward: -145.30894713     steps:   500    q:-44.5515937805, failure\n",
      "[Episode   269]  total reward: -227.679872847     steps:   500    q:29.4584693909, failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode   270]  total reward: -275.765624418     steps:   500    q:-22.9694766998, failure\n",
      "[Episode   271]  total reward: -294.253085965     steps:   500    q:-38.9022254944, failure\n",
      "[Episode   272]  total reward: -108.135988619     steps:   500    q:76.5233306885, failure\n",
      "[Episode   273]  total reward: -247.399185378     steps:   500    q:113.633453369, failure\n",
      "[Episode   274]  total reward: -326.768744026     steps:   500    q:-63.4327468872, failure\n",
      "[Episode   275]  total reward: -220.057692406     steps:   500    q:-41.1445579529, failure\n",
      "[Episode   276]  total reward: -493.119517632     steps:   500    q:-123.59072113, failure\n",
      "[Episode   277]  total reward: -264.082998102     steps:   500    q:-82.6543502808, failure\n",
      "[Episode   278]  total reward: -331.52745473     steps:   500    q:-102.34072113, failure\n",
      "[Episode   279]  total reward: -223.450410196     steps:   500    q:-147.484619141, failure\n",
      "[Episode   280]  total reward: -420.612143636     steps:   500    q:-166.201812744, failure\n",
      "[Episode   281]  total reward: -553.269031947     steps:   500    q:-177.157974243, failure\n",
      "[Episode   282]  total reward: -389.498780585     steps:   500    q:-129.219299316, failure\n",
      "[Episode   283]  total reward: -296.717550027     steps:   500    q:-152.768173218, failure\n",
      "[Episode   284]  total reward: -462.053784576     steps:   500    q:-150.806213379, failure\n",
      "[Episode   285]  total reward: -286.842206831     steps:   500    q:-92.7468261719, failure\n",
      "[Episode   286]  total reward: -194.360956515     steps:   500    q:-59.5167655945, failure\n",
      "[Episode   287]  total reward: -262.793988535     steps:   500    q:-111.561264038, failure\n",
      "[Episode   288]  total reward: -456.87969015     steps:   500    q:-42.6081466675, failure\n",
      "[Episode   289]  total reward: -30.7827115478     steps:   500    q:-121.096183777, failure\n",
      "[Episode   290]  total reward: -67.2116092194     steps:   500    q:-135.285049438, failure\n",
      "[Episode   291]  total reward: -114.915075397     steps:   500    q:-55.0324440002, failure\n",
      "[Episode   292]  total reward: -289.144113429     steps:   500    q:-69.0374069214, failure\n",
      "[Episode   293]  total reward: -211.736131228     steps:   500    q:-23.697933197, failure\n",
      "[Episode   294]  total reward: -112.590328605     steps:   500    q:-63.0264320374, failure\n",
      "[Episode   295]  total reward: -94.850489997     steps:   500    q:-6.67598056793, failure\n",
      "[Episode   296]  total reward: -33.1964144513     steps:   500    q:-79.2794265747, failure\n",
      "[Episode   297]  total reward: -48.5854856156     steps:   500    q:-58.5897064209, failure\n",
      "[Episode   298]  total reward: -189.494054312     steps:   500    q:-74.0362243652, failure\n",
      "[Episode   299]  total reward: -65.454728459     steps:   500    q:-70.24634552, failure\n",
      "[Episode   300]  total reward: -388.890154448     steps:   500    q:-62.9660148621, failure\n",
      "[Episode   301]  total reward: -288.062283152     steps:   500    q:-114.046875, failure\n",
      "[Episode   302]  total reward: -146.45052745     steps:   500    q:-75.5084533691, failure\n",
      "[Episode   303]  total reward: -93.1602992447     steps:   500    q:-87.3075485229, failure\n",
      "[Episode   304]  total reward: -67.4855750358     steps:   500    q:-65.7851867676, failure\n",
      "[Episode   305]  total reward: -112.698853755     steps:   500    q:-73.570854187, failure\n",
      "[Episode   306]  total reward: -296.064390931     steps:   500    q:-107.678108215, failure\n",
      "[Episode   307]  total reward: -142.651112607    steps:   256    q:-85.3834838867, success\n",
      "[Episode   308]  total reward: -0.128963750901     steps:   500    q:-93.9516906738, failure\n",
      "[Episode   309]  total reward: -111.377188932    steps:   345    q:5.84600305557, success\n",
      "[Episode   310]  total reward: -1.03570837482    steps:    90    q:83.8552474976, success\n",
      "[Episode   311]  total reward: 133.208194697    steps:    88    q:18.1689128876, success\n",
      "[Episode   312]  total reward: -399.56450777     steps:   500    q:-95.4989700317, failure\n",
      "[Episode   313]  total reward: -119.244262979     steps:   500    q:-32.7658500671, failure\n",
      "[Episode   314]  total reward: -222.801905001     steps:   500    q:-23.7485179901, failure\n",
      "[Episode   315]  total reward: -122.805437     steps:   500    q:36.5553398132, failure\n",
      "[Episode   316]  total reward: -48.3699349972     steps:   500    q:-53.0246810913, failure\n",
      "[Episode   317]  total reward: -158.000895117     steps:   500    q:-32.7996788025, failure\n",
      "[Episode   318]  total reward: -120.942831154     steps:   500    q:-49.1321983337, failure\n",
      "[Episode   319]  total reward: -139.954426586     steps:   500    q:33.5341110229, failure\n",
      "[Episode   320]  total reward: -127.691140076     steps:   500    q:-31.4931335449, failure\n",
      "[Episode   321]  total reward: -365.71953157     steps:   500    q:-42.4971733093, failure\n",
      "[Episode   322]  total reward: -61.2121247545     steps:   500    q:-37.728515625, failure\n",
      "[Episode   323]  total reward: -300.70370874     steps:   500    q:-15.3071575165, failure\n",
      "[Episode   324]  total reward: -195.725074289     steps:   500    q:-1.47858595848, failure\n",
      "[Episode   325]  total reward: -227.077700101     steps:   500    q:-28.652217865, failure\n",
      "[Episode   326]  total reward: -93.1823302291     steps:   500    q:-37.6209487915, failure\n",
      "[Episode   327]  total reward: -407.530684815     steps:   500    q:0.260201185942, failure\n",
      "[Episode   328]  total reward: -27.153128528     steps:   500    q:-23.748374939, failure\n",
      "[Episode   329]  total reward: -358.87385732     steps:   500    q:-21.032749176, failure\n",
      "[Episode   330]  total reward: -164.656942724     steps:   500    q:85.8597640991, failure\n",
      "[Episode   331]  total reward: -63.9141002775     steps:   500    q:145.666473389, failure\n",
      "[Episode   332]  total reward: -15.7570472223     steps:   500    q:90.1849365234, failure\n",
      "[Episode   333]  total reward: -328.835991703     steps:   500    q:17.667760849, failure\n",
      "[Episode   334]  total reward: -228.481150113     steps:   500    q:46.5035133362, failure\n",
      "[Episode   335]  total reward: -28.4752876364     steps:   500    q:128.521759033, failure\n",
      "[Episode   336]  total reward: -193.178478805     steps:   500    q:29.2446842194, failure\n",
      "[Episode   337]  total reward: -274.861245848     steps:   500    q:123.730857849, failure\n",
      "[Episode   338]  total reward: -226.760780363     steps:   500    q:123.480354309, failure\n",
      "[Episode   339]  total reward: -68.9031602482     steps:   500    q:211.809158325, failure\n",
      "[Episode   340]  total reward: -128.084901574     steps:   500    q:-65.8536148071, failure\n",
      "[Episode   341]  total reward: -235.910629262     steps:   500    q:153.949615479, failure\n",
      "[Episode   342]  total reward: -26.3147405583     steps:   500    q:32.549407959, failure\n"
     ]
    }
   ],
   "source": [
    "# Plotting setting\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from time import sleep\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import dqn_cooperation\n",
    "from collections import deque\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# Create New environment with transition law\n",
    "ACTION_NUM = 5\n",
    "INPUT_SIZE = 10\n",
    "OUTPUT_SIZE = ACTION_NUM**2\n",
    "VEL = 0.5\n",
    "TIME_GAP = 1\n",
    "MAP_SIZE = 7\n",
    "\n",
    "def annealing_epsilon(episode, min_e, max_e, target_episode):\n",
    "\n",
    "    slope = (min_e - max_e) / (target_episode)\n",
    "    intercept = max_e\n",
    "\n",
    "    return max(min_e, slope * episode + intercept)\n",
    "\n",
    "class new_env:     \n",
    "    def create_env(self, arg_state=[2.,3.,2.,4.], g_pos=[5.,5.], obs_pos1=[2.,2.], obs_pos2=[3.,4.], obs_size=5):\n",
    "        self.state = np.array(arg_state+g_pos+obs_pos1+obs_pos2) # reset\n",
    "        self.n_state = np.array(arg_state+g_pos+obs_pos1+obs_pos2)\n",
    "        self.obstacle_size = obs_size\n",
    "        return self.state, self.obstacle_size\n",
    "    #def add_obs(self, obs_pos), we postpone this \n",
    "    \n",
    "    def next_step(self, arg_state, arg_action):\n",
    "\n",
    "        self._fail = False\n",
    "        self.reward = 0\n",
    "        # convert to each action\n",
    "        arg_action1 = arg_action // ACTION_NUM\n",
    "        arg_action2 = arg_action - ACTION_NUM*arg_action1\n",
    "        '''position update through action\n",
    "        UP = 0, DOWN = 1, LEFT = 2, RIGHT = 3'''        \n",
    "        # agent 1\n",
    "        if arg_action1 == 0:\n",
    "            self.n_state[0:4] = arg_state[0:4] + np.array([0,1,0,0])*VEL*TIME_GAP\n",
    "        elif arg_action1 == 1:\n",
    "            self.n_state[0:4] = arg_state[0:4] + np.array([0,-1,0,0])*VEL*TIME_GAP\n",
    "        elif arg_action1 == 2:\n",
    "            self.n_state[0:4] = arg_state[0:4] + np.array([-1,0,0,0])*VEL*TIME_GAP\n",
    "        elif arg_action1 == 3:\n",
    "            self.n_state[0:4] = arg_state[0:4] + np.array([1,0,0,0])*VEL*TIME_GAP\n",
    "        else:\n",
    "            self.n_state[0:4] = arg_state[0:4] # stop        \n",
    "        # agent 2  \n",
    "        if arg_action2 == 0:\n",
    "            self.n_state[0:4] = arg_state[0:4] + np.array([0,0,0,1])*VEL*TIME_GAP\n",
    "        elif arg_action2 == 1:\n",
    "            self.n_state[0:4] = arg_state[0:4] + np.array([0,0,0,-1])*VEL*TIME_GAP\n",
    "        elif arg_action2 == 2:\n",
    "            self.n_state[0:4] = arg_state[0:4] + np.array([0,0,-1,0])*VEL*TIME_GAP\n",
    "        elif arg_action1 == 3:\n",
    "            self.n_state[0:4] = arg_state[0:4] + np.array([0,0,1,0])*VEL*TIME_GAP      \n",
    "        else:\n",
    "            self.n_state[0:4] = arg_state[0:4] # stop   \n",
    "            \n",
    "        '''get the reward'''\n",
    "        if np.linalg.norm((self.n_state[0:2]+self.n_state[2:4])/2-self.n_state[4:6])!=0:\n",
    "            self.reward = (1/np.linalg.norm((self.n_state[0:2]+self.n_state[2:4])/2-self.n_state[4:6])-\\\n",
    "            1/np.linalg.norm((arg_state[0:2]+arg_state[2:4])/2-self.n_state[4:6]))*100\n",
    "        if np.linalg.norm(self.n_state[0:2]-self.n_state[6:8])<1 or np.linalg.norm(self.n_state[2:4]-self.n_state[6:8])<1:\n",
    "            self.reward = self.reward-1 # collision\n",
    "        if np.linalg.norm(self.n_state[0:2]-self.n_state[8:10])<1 or np.linalg.norm(self.n_state[2:4]-self.n_state[8:10])<1:\n",
    "            self.reward = self.reward-1 # collision\n",
    "        if np.linalg.norm((self.n_state[0:2]+self.n_state[2:4])/2-self.n_state[4:6])<1 and np.linalg.norm(self.n_state[0:2]-self.n_state[2:4])<3: # approximately set condition\n",
    "            self.reward = self.reward + 1000 # achieve goal\n",
    "            self._fail = True\n",
    "        if np.linalg.norm(self.n_state[0:2]-self.n_state[2:4])>2.5:\n",
    "            self.reward = self.reward-np.linalg.norm(self.n_state[0:2]-self.n_state[2:4])*2 # drop the object\n",
    "            #self._fail = True     \n",
    "        return self.n_state, self.reward, self._fail\n",
    "    \n",
    "#env = new_env() \n",
    "#state, g_pos, o_pos, o_size = env.create_env() # set the enviornment\n",
    "DISCOUNT_RATE = 0.98\n",
    "REPLAY_MEMORY = 10000\n",
    "BATCH_SIZE = 50\n",
    "MAX_EPI = 2000\n",
    "MAX_STEP = 500\n",
    "\n",
    "# minimum epsilon for epsilon greedy\n",
    "MIN_E = 0.1\n",
    "# epsilon will be `MIN_E` at `EPSILON_DECAYING_STEP`\n",
    "EPSILON_DECAYING_EPI = MAX_EPI * 0.2\n",
    "TARGET_UPDATE_FQ = 100\n",
    "\n",
    "def train_minibatch(mainDQN, targetDQN, minibatch):\n",
    "    state_array = np.array([x[0] for x in minibatch])\n",
    "    action_array = np.array([x[1] for x in minibatch]) # [ x among 0~24] * BATCH_SIZE\n",
    "    reward_array = np.array([x[2] for x in minibatch])\n",
    "    n_state_array = np.array([x[3] for x in minibatch]) # [[1,2,3,4][1,2,3,4]...as much as BATCH_SIZE NUMBER]\n",
    "    _fail_array = np.array([x[4] for x in minibatch])\n",
    "    \n",
    "    \n",
    "    X_batch = state_array   \n",
    "    Y_batch = mainDQN.predict(state_array) # 25 elements * BATCH_SIZE \n",
    "    \n",
    "    # consideration for action constraint \n",
    "    target_q = targetDQN.predict(n_state_array) # [[1 ...25][1...25]...batch_size]\n",
    "    j = 0\n",
    "    for x in n_state_array:        \n",
    "        t_dqn = targetDQN.predict(x) #[[1 2 3 ...]]\n",
    "        t_dqn = t_dqn.flatten() # [1 2 3 ...]\n",
    "        if x[0]<TIME_GAP*VEL:\n",
    "            for i in range(ACTION_NUM):\n",
    "                t_dqn[2*ACTION_NUM+i] = -float(\"inf\") # put a large num on action 2(left)\n",
    "        if x[1]<TIME_GAP*VEL:\n",
    "            for i in range(ACTION_NUM):\n",
    "                t_dqn[1*ACTION_NUM+i] = -float(\"inf\") # put a large num on action 1(down)\n",
    "        if x[0] > MAP_SIZE - TIME_GAP*VEL:\n",
    "            for i in range(ACTION_NUM):\n",
    "                t_dqn[3*ACTION_NUM+i] = -float(\"inf\") # remove action 3(right)\n",
    "        if x[1] > MAP_SIZE - TIME_GAP*VEL:\n",
    "            for i in range(ACTION_NUM):\n",
    "                t_dqn[0*ACTION_NUM+i] = -float(\"inf\") # remove action 0(up)  \n",
    "        if x[2]<TIME_GAP*VEL:\n",
    "            for i in range(ACTION_NUM):\n",
    "                t_dqn[i*ACTION_NUM+2] = -float(\"inf\")# put a large num on action 2(left)\n",
    "        if x[3]<TIME_GAP*VEL:\n",
    "            for i in range(ACTION_NUM):\n",
    "                t_dqn[i*ACTION_NUM+1] = -float(\"inf\") # put a large num on action 1(down)\n",
    "        if x[2] > MAP_SIZE - TIME_GAP*VEL:\n",
    "            for i in range(ACTION_NUM):\n",
    "                t_dqn[i*ACTION_NUM+3] = -float(\"inf\")# remove action 3(right)\n",
    "        if x[3] > MAP_SIZE - TIME_GAP*VEL:\n",
    "            for i in range(ACTION_NUM):\n",
    "                t_dqn[i*ACTION_NUM+0] = -float(\"inf\") # remove action 0(up)    \n",
    "        target_q[j] = t_dqn\n",
    "        j += 1\n",
    "        \n",
    "    Q_target = reward_array + DISCOUNT_RATE*np.max(target_q, axis=1)*~_fail_array # if fail, Q = reward\n",
    "    \n",
    "    Y_batch[np.arange(len(X_batch)), action_array] = Q_target\n",
    "    \n",
    "    # Train\n",
    "    cost_batch, _ = mainDQN.update(X_batch, Y_batch)\n",
    "    return cost_batch\n",
    "\n",
    "def get_copy_var_ops(dest_scope_name = \"target\", src_scope_name = \"main\"):\n",
    "    op_holder = []\n",
    "    \n",
    "    src_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=src_scope_name)\n",
    "    dest_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=dest_scope_name)\n",
    "\n",
    "    for src_var, dest_var in zip(src_vars, dest_vars):\n",
    "        op_holder.append(dest_var.assign(src_var.value()))\n",
    "\n",
    "    return op_holder\n",
    "\n",
    "\n",
    "def main():\n",
    "    replay_buffer = deque(maxlen=REPLAY_MEMORY) # detract element from both sides    \n",
    "    total_reward_buffer = []\n",
    "    step_buffer = []\n",
    "    avg_q_value = []\n",
    "    new_graph = tf.Graph()\n",
    "    with tf.Session(graph=new_graph) as sess:\n",
    "        mainDQN = dqn_cooperation.DQN(sess, INPUT_SIZE, OUTPUT_SIZE, name = \"main\")\n",
    "        mainDQN.build_network(32,64,0.005)\n",
    "        targetDQN = dqn_cooperation.DQN(sess, INPUT_SIZE, OUTPUT_SIZE, name = \"target\")\n",
    "        targetDQN.build_network(32,64,0.005)\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        #restore model\n",
    "        \n",
    "        new_saver = tf.train.import_meta_graph(\"./dqn_multi_action.ckpt.meta\")        \n",
    "        new_saver.restore(sess,\"./dqn_multi_action.ckpt\")\n",
    "        \n",
    "        # initial copy main q -> target q\n",
    "        copy_ops = get_copy_var_ops(dest_scope_name = \"target\", src_scope_name = \"main\")\n",
    "        sess.run(copy_ops)\n",
    "        \n",
    "        reward_accum_last100 = 0\n",
    "        reward_sum = 0\n",
    "        \n",
    "        game = ENV(MAP_SIZE, [2,2], [3,4], [2.,3.,2.,4.], [5,5])\n",
    "        game.render_env()\n",
    "        \n",
    "        for episode in range(MAX_EPI):\n",
    "            \n",
    "            '''\n",
    "            if episode < 100:\n",
    "                e = 0.3\n",
    "            e = 0.05\n",
    "            '''\n",
    "            _fail = False\n",
    "            step_count = 0 # how many moves included in an episode\n",
    "            env1 = new_env()\n",
    "            state, _= env1.create_env() # get only state            \n",
    "            reward_sum = 0      \n",
    "            goal_ = True\n",
    "            max_qlist = []\n",
    "            while not _fail:\n",
    "                e = annealing_epsilon(episode, MIN_E, 1.0, EPSILON_DECAYING_EPI)\n",
    "                # after sufficient learning, we present the game scene\n",
    "                if episode > MAX_EPI:\n",
    "                    game.update(state)\n",
    "                    game.render_env()\n",
    "                    \n",
    "                step_count += 1\n",
    "                if np.random.rand()< e:\n",
    "                    act_candi1 = range(ACTION_NUM)\n",
    "                    act_candi2 = range(ACTION_NUM)\n",
    "                    if state[0]<TIME_GAP*VEL:\n",
    "                        act_candi1.remove(2) # remove action 2(left)\n",
    "                    if state[1]<TIME_GAP*VEL:\n",
    "                        act_candi1.remove(1) # remove action 1(down)\n",
    "                    if state[0] > MAP_SIZE - TIME_GAP*VEL:\n",
    "                        act_candi1.remove(3) # remove action 3(right)\n",
    "                    if state[1] > MAP_SIZE - TIME_GAP*VEL:\n",
    "                        act_candi1.remove(0) # remove action 0(up)    \n",
    "                    if state[2]<TIME_GAP*VEL:\n",
    "                        act_candi2.remove(2) # remove action 2(left)\n",
    "                    if state[3]<TIME_GAP*VEL:\n",
    "                        act_candi2.remove(1) # remove action 1(down)\n",
    "                    if state[2] > MAP_SIZE - TIME_GAP*VEL:\n",
    "                        act_candi2.remove(3) # remove action 3(right)\n",
    "                    if state[3] > MAP_SIZE - TIME_GAP*VEL:\n",
    "                        act_candi2.remove(0) # remove action 0(up)    \n",
    "                    act_candi1_ = np.array(act_candi1)\n",
    "                    act_candi2_ = np.array(act_candi2)\n",
    "\n",
    "                    action_l1 = random.sample(act_candi1, 1) # choose up, down, left, right, stop for agent 1\n",
    "                    action_l2 = random.sample(act_candi2, 1) # for agent 2\n",
    "                    action = action_l1[0]*(ACTION_NUM) + action_l2[0] # convert to index\n",
    "                else:\n",
    "                    act_candi = mainDQN.predict(state) # [[1 2 3 ... as much as OUTPUT_SIZE]]\n",
    "                    act_candi = act_candi.flatten()\n",
    "\n",
    "                    if state[0]<TIME_GAP*VEL:\n",
    "                        for i in range(ACTION_NUM):\n",
    "                            act_candi[2*ACTION_NUM+i] = -float(\"inf\") # put a large num on action 2(left)\n",
    "                    if state[1]<TIME_GAP*VEL:\n",
    "                        for i in range(ACTION_NUM):\n",
    "                            act_candi[1*ACTION_NUM+i] = -float(\"inf\") # put a large num on action 1(down)\n",
    "                    if state[0] > MAP_SIZE - TIME_GAP*VEL:\n",
    "                        for i in range(ACTION_NUM):\n",
    "                            act_candi[3*ACTION_NUM+i] = -float(\"inf\") # remove action 3(right)\n",
    "                    if state[1] > MAP_SIZE - TIME_GAP*VEL:\n",
    "                        for i in range(ACTION_NUM):\n",
    "                            act_candi[0*ACTION_NUM+i] = -float(\"inf\") # remove action 0(up)  \n",
    "                    if state[2]<TIME_GAP*VEL:\n",
    "                        for i in range(ACTION_NUM):\n",
    "                            act_candi[i*ACTION_NUM+2] = -float(\"inf\")# put a large num on action 2(left)\n",
    "                    if state[3]<TIME_GAP*VEL:\n",
    "                        for i in range(ACTION_NUM):\n",
    "                            act_candi[i*ACTION_NUM+1] = -float(\"inf\") # put a large num on action 1(down)\n",
    "                    if state[2] > MAP_SIZE - TIME_GAP*VEL:\n",
    "                        for i in range(ACTION_NUM):\n",
    "                            act_candi[i*ACTION_NUM+3] = -float(\"inf\")# remove action 3(right)\n",
    "                    if state[3] > MAP_SIZE - TIME_GAP*VEL:\n",
    "                        for i in range(ACTION_NUM):\n",
    "                            act_candi[i*ACTION_NUM+0] = -float(\"inf\") # remove action 0(up)    \n",
    "\n",
    "                    action = np.argmax(act_candi)   \n",
    "                    '''\n",
    "                    dd_predict = mainDQN.predict(state).flatten()\n",
    "                    aa = np.max(dd_predict)\n",
    "                    max_indx, = np.where(dd_predict==aa)                    \n",
    "                    action = random.sample(max_indx,1)[0]\n",
    "                    '''\n",
    "                \n",
    "                n_state, reward, _fail = env1.next_step(state, action) # have to input the action \n",
    "                # if count >30, stop that episode and start new episode\n",
    "                if step_count >MAX_STEP-1:\n",
    "                    #reward = -30\n",
    "                    _fail = True\n",
    "                    goal_ = False\n",
    "                    \n",
    "                reward_sum += DISCOUNT_RATE**step_count * reward    # sum total reward and penalty about long time(-0.5)     \n",
    "                \n",
    "                replay_buffer.append((state, action, reward, n_state, _fail)) #resolve the correlation                \n",
    "                if _fail == True and goal_ == True:\n",
    "                    success_tuple = (state, action, reward, n_state, _fail)\n",
    "                state = n_state\n",
    "                \n",
    "                q_values = mainDQN.predict(state) # [[1 2 3 ... as much as OUTPUT_SIZE]]\n",
    "                q_values = q_values.flatten()\n",
    "                max_q = np.max(np.array(q_values))\n",
    "                max_qlist.append(max_q)\n",
    "                # train minibatch of main Q-NET and update the  Q-network from main Q-NET\n",
    "                if len(replay_buffer)>BATCH_SIZE*3:\n",
    "                    # add success\n",
    "                    #if goal_ == True and _fail == True:\n",
    "                    #    minibatch = random.sample(replay_buffer, BATCH_SIZE-1)\n",
    "                    #    minibatch.append(success_tuple)                        \n",
    "                    #else:\n",
    "                    minibatch = random.sample(replay_buffer, BATCH_SIZE) \n",
    "                    train_minibatch(mainDQN, targetDQN, minibatch) # training number = step number\n",
    "                if step_count % TARGET_UPDATE_FQ == 0:\n",
    "                    sess.run(copy_ops)\n",
    "            \n",
    "            avg_q_value.append(np.mean(max_qlist))\n",
    "            total_reward_buffer.append(reward_sum)  \n",
    "            step_buffer.append(step_count)\n",
    "            if goal_ == True:       \n",
    "                print(\"[Episode {:>5}]  total reward: {:>5}    steps: {:>5}    q:{:>5}, success\".format(episode, reward_sum, step_count, np.mean(max_qlist)))\n",
    "            else:\n",
    "                print(\"[Episode {:>5}]  total reward: {:>5}     steps: {:>5}    q:{:>5}, failure\".format(episode, reward_sum, step_count, np.mean(max_qlist)))\n",
    "        #print(\"Success ratio: {}\".format(reward_accum_last100/100))\n",
    "        fig1 =plt.figure()\n",
    "        plt.plot(range(MAX_EPI), total_reward_buffer)\n",
    "        plt.show()\n",
    "        \n",
    "        # save model  \n",
    "        #new_saver = tf.train.Saver()\n",
    "        save_path = new_saver.save(sess, \"./dqn_multi_action_add_success_hard.ckpt\")   \n",
    "        # save data (reward, step)\n",
    "        f = open(\"reward_add_success_hard.txt\", 'w')\n",
    "        for i in range(len(total_reward_buffer)):\n",
    "            f.write(\"{:>5}  {:>5}  {:>5}\\n\".format(total_reward_buffer[i], step_buffer[i], avg_q_value[i]))\n",
    "        f.close\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()    \n",
    "    end = time.time()-start\n",
    "    print(end)       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# two plot (reward, step)\n",
    "\n",
    "file = open('reward_add_success_easy.txt', 'r')    # hello.txt 파일을 읽기 모드(r)로 열기. 파일 객체 반환\n",
    "s = file.read().split( )                  # 파일에서 문자열 읽기\n",
    "reward = []\n",
    "step = []\n",
    "q = []\n",
    "for i in range(0,len(s),3):\n",
    "    reward.append(float(s[i]))\n",
    "    step.append(int(s[i+1]))\n",
    "    q.append(float(s[i+2]))\n",
    "                         # Hello, world!\n",
    "file.close()                     # 파일 객체 닫기\n",
    "\n",
    "fig1 =plt.figure()\n",
    "plt.plot(range(len(reward)),reward, lw =0.3)\n",
    "fig2 =plt.figure()\n",
    "plt.plot(range(len(step)),step, lw=0.3)\n",
    "fig3 =plt.figure()\n",
    "plt.plot(range(len(q)),q, lw=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
