{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Practice 1\n",
    " _by Hotae Lee_\n",
    "* Q network - FrozenLake\n",
    "* Q network - CartPole\n",
    "* DQN 2013(Deeper Network & replay buffer) - CartPole\n",
    "* DQN 2015(Use double network to solve unstationary target) -CartPole\n",
    "* Policy Gradient (Actor-Critic) - CartPole\n",
    "* Make test environment\n",
    "* Q Network for 3 WMRs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q network - FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of successful episodes: 0.489%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0d2822f290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "# Input and output size based on the Env\n",
    "input_size = env.observation_space.n\n",
    "output_size = env.action_space.n\n",
    "learning_rate = 0.1\n",
    "\n",
    "# These lines establish the feed-forward part of the network used to\n",
    "# choose actions\n",
    "X = tf.placeholder(shape=[1, input_size], dtype=tf.float32)  # state input\n",
    "W = tf.Variable(tf.random_uniform(\n",
    "    [input_size, output_size], 0, 0.01))  # weight\n",
    "\n",
    "Qpred = tf.matmul(X, W)  # Out Q prediction\n",
    "Y = tf.placeholder(shape=[1, output_size], dtype=tf.float32)  # Y label\n",
    "\n",
    "loss = tf.reduce_sum(tf.square(Y - Qpred))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# Set Q-learning related parameters\n",
    "dis = .99\n",
    "num_episodes = 2000\n",
    "\n",
    "# Create lists to contain total rewards and steps per episode\n",
    "rList = []\n",
    "    \n",
    "\n",
    "def one_hot(x):\n",
    "    return np.identity(16)[x:x + 1]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        # Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        e = 1. / ((i / 50) + 10)\n",
    "        rAll = 0\n",
    "        done = False\n",
    "        local_loss = []\n",
    "\n",
    "        # The Q-Network training\n",
    "        while not done:\n",
    "            # Choose an action by greedily (with e chance of random action)\n",
    "            # from the Q-network\n",
    "            Qs = sess.run(Qpred, feed_dict={X: one_hot(s)})\n",
    "            if np.random.rand(1) < e:\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                a = np.argmax(Qs)\n",
    "\n",
    "            # Get new state and reward from environment\n",
    "            s1, reward, done, _ = env.step(a)\n",
    "            if done:\n",
    "                # Update Q, and no Qs+1, since it's a terminal state\n",
    "                Qs[0, a] = reward\n",
    "            else:\n",
    "                # Obtain the Q_s1 values by feeding the new state through our\n",
    "                # network\n",
    "                Qs1 = sess.run(Qpred, feed_dict={X: one_hot(s1)})\n",
    "                # Update Q\n",
    "                Qs[0, a] = reward + dis * np.max(Qs1)\n",
    "\n",
    "            # Train our network using target (Y) and predicted Q (Qpred) values\n",
    "            sess.run(train, feed_dict={X: one_hot(s), Y: Qs})\n",
    "\n",
    "            rAll += reward\n",
    "            s = s1\n",
    "        rList.append(rAll)\n",
    "\n",
    "print(\"Percent of successful episodes: \" +\n",
    "      str(sum(rList) / num_episodes) + \"%\")\n",
    "plt.bar(range(len(rList)), rList, color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPole Random action test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "(array([-0.00062584,  0.20521483, -0.02730596, -0.32927179]), 1.0, False)\n",
      "(array([ 0.00347845,  0.40071464, -0.03389139, -0.63043915]), 1.0, False)\n",
      "(array([ 0.01149275,  0.5962927 , -0.04650018, -0.93359993]), 1.0, False)\n",
      "(array([ 0.0234186 ,  0.7920101 , -0.06517217, -1.24052482]), 1.0, False)\n",
      "(array([ 0.0392588 ,  0.9879055 , -0.08998267, -1.55289116]), 1.0, False)\n",
      "(array([ 0.05901691,  1.18398371, -0.12104049, -1.87223779]), 1.0, False)\n",
      "(array([ 0.08269659,  1.38020241, -0.15848525, -2.19991147]), 1.0, False)\n",
      "(array([ 0.11030063,  1.57645655, -0.20248348, -2.53700319]), 1.0, False)\n",
      "(array([ 0.14182977,  1.38346807, -0.25322354, -2.31255875]), 1.0, True)\n",
      "('Reward for this episode was:', 9.0)\n",
      "\u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "(array([ 0.16949913,  1.57985932, -0.29947472, -2.67140586]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n",
      "(array([ 0.20109631,  1.38794821, -0.35290284, -2.48308707]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n",
      "(array([ 0.22885528,  1.19706692, -0.40256458, -2.31602341]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n",
      "(array([ 0.25279662,  1.0071967 , -0.44888505, -2.16916871]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n",
      "(array([ 0.27294055,  0.81829275, -0.49226842, -2.04146905]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n",
      "(array([ 0.2893064 ,  1.01426331, -0.5330978 , -2.43947355]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n",
      "(array([ 0.30959167,  0.82602511, -0.58188727, -2.34570893]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n",
      "(array([ 0.32611217,  1.02048369, -0.62880145, -2.75097569]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n",
      "(array([ 0.34652185,  0.83259704, -0.68382096, -2.69597418]), 0.0, True)\n",
      "('Reward for this episode was:', 0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.04890821, -0.00819998,  0.04947044,  0.00126633])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "random_episodes = 0\n",
    "reward_sum = 0\n",
    "while random_episodes < 10:\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    print(observation, reward, done)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        random_episodes += 1\n",
    "        print(\"Reward for this episode was:\", reward_sum)\n",
    "        reward_sum = 0\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Network - CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode: 0  steps: 16\n",
      "Episode: 1  steps: 15\n",
      "Episode: 2  steps: 20\n",
      "Episode: 3  steps: 11\n",
      "Episode: 4  steps: 16\n",
      "Episode: 5  steps: 20\n",
      "Episode: 6  steps: 17\n",
      "Episode: 7  steps: 19\n",
      "Episode: 8  steps: 16\n",
      "Episode: 9  steps: 21\n",
      "Episode: 10  steps: 10\n",
      "Episode: 11  steps: 12\n",
      "Episode: 12  steps: 18\n",
      "Episode: 13  steps: 13\n",
      "Episode: 14  steps: 11\n",
      "Episode: 15  steps: 12\n",
      "Episode: 16  steps: 8\n",
      "Episode: 17  steps: 10\n",
      "Episode: 18  steps: 15\n",
      "Episode: 19  steps: 9\n",
      "Episode: 20  steps: 10\n",
      "Episode: 21  steps: 10\n",
      "Episode: 22  steps: 10\n",
      "Episode: 23  steps: 12\n",
      "Episode: 24  steps: 70\n",
      "Episode: 25  steps: 61\n",
      "Episode: 26  steps: 32\n",
      "Episode: 27  steps: 61\n",
      "Episode: 28  steps: 22\n",
      "Episode: 29  steps: 22\n",
      "Episode: 30  steps: 10\n",
      "Episode: 31  steps: 41\n",
      "Episode: 32  steps: 36\n",
      "Episode: 33  steps: 23\n",
      "Episode: 34  steps: 19\n",
      "Episode: 35  steps: 10\n",
      "Episode: 36  steps: 11\n",
      "Episode: 37  steps: 9\n",
      "Episode: 38  steps: 9\n",
      "Episode: 39  steps: 28\n",
      "Episode: 40  steps: 28\n",
      "Episode: 41  steps: 32\n",
      "Episode: 42  steps: 30\n",
      "Episode: 43  steps: 33\n",
      "Episode: 44  steps: 31\n",
      "Episode: 45  steps: 43\n",
      "Episode: 46  steps: 37\n",
      "Episode: 47  steps: 33\n",
      "Episode: 48  steps: 74\n",
      "Episode: 49  steps: 40\n",
      "Episode: 50  steps: 71\n",
      "Episode: 51  steps: 38\n",
      "Episode: 52  steps: 39\n",
      "Episode: 53  steps: 29\n",
      "Episode: 54  steps: 67\n",
      "Episode: 55  steps: 30\n",
      "Episode: 56  steps: 137\n",
      "Episode: 57  steps: 104\n",
      "Episode: 58  steps: 49\n",
      "Episode: 59  steps: 193\n",
      "Episode: 60  steps: 200\n",
      "Episode: 61  steps: 53\n",
      "Episode: 62  steps: 169\n",
      "Episode: 63  steps: 73\n",
      "Episode: 64  steps: 24\n",
      "Episode: 65  steps: 16\n",
      "Episode: 66  steps: 29\n",
      "Episode: 67  steps: 25\n",
      "Episode: 68  steps: 47\n",
      "Episode: 69  steps: 58\n",
      "Episode: 70  steps: 33\n",
      "Episode: 71  steps: 23\n",
      "Episode: 72  steps: 54\n",
      "Episode: 73  steps: 50\n",
      "Episode: 74  steps: 30\n",
      "Episode: 75  steps: 51\n",
      "Episode: 76  steps: 29\n",
      "Episode: 77  steps: 49\n",
      "Episode: 78  steps: 42\n",
      "Episode: 79  steps: 38\n",
      "Episode: 80  steps: 27\n",
      "Episode: 81  steps: 32\n",
      "Episode: 82  steps: 68\n",
      "Episode: 83  steps: 68\n",
      "Episode: 84  steps: 27\n",
      "Episode: 85  steps: 34\n",
      "Episode: 86  steps: 26\n",
      "Episode: 87  steps: 8\n",
      "Episode: 88  steps: 10\n",
      "Episode: 89  steps: 22\n",
      "Episode: 90  steps: 38\n",
      "Episode: 91  steps: 39\n",
      "Episode: 92  steps: 32\n",
      "Episode: 93  steps: 30\n",
      "Episode: 94  steps: 22\n",
      "Episode: 95  steps: 10\n",
      "Episode: 96  steps: 9\n",
      "Episode: 97  steps: 8\n",
      "Episode: 98  steps: 10\n",
      "Episode: 99  steps: 10\n",
      "Episode: 100  steps: 25\n",
      "Episode: 101  steps: 28\n",
      "Episode: 102  steps: 40\n",
      "Episode: 103  steps: 41\n",
      "Episode: 104  steps: 29\n",
      "Episode: 105  steps: 26\n",
      "Episode: 106  steps: 23\n",
      "Episode: 107  steps: 34\n",
      "Episode: 108  steps: 27\n",
      "Episode: 109  steps: 78\n",
      "Episode: 110  steps: 33\n",
      "Episode: 111  steps: 32\n",
      "Episode: 112  steps: 27\n",
      "Episode: 113  steps: 33\n",
      "Episode: 114  steps: 8\n",
      "Episode: 115  steps: 40\n",
      "Episode: 116  steps: 24\n",
      "Episode: 117  steps: 21\n",
      "Episode: 118  steps: 44\n",
      "Episode: 119  steps: 27\n",
      "Episode: 120  steps: 39\n",
      "Episode: 121  steps: 61\n",
      "Episode: 122  steps: 62\n",
      "Episode: 123  steps: 43\n",
      "Episode: 124  steps: 29\n",
      "Episode: 125  steps: 57\n",
      "Episode: 126  steps: 38\n",
      "Episode: 127  steps: 23\n",
      "Episode: 128  steps: 24\n",
      "Episode: 129  steps: 26\n",
      "Episode: 130  steps: 35\n",
      "Episode: 131  steps: 43\n",
      "Episode: 132  steps: 33\n",
      "Episode: 133  steps: 52\n",
      "Episode: 134  steps: 36\n",
      "Episode: 135  steps: 36\n",
      "Episode: 136  steps: 50\n",
      "Episode: 137  steps: 29\n",
      "Episode: 138  steps: 45\n",
      "Episode: 139  steps: 51\n",
      "Episode: 140  steps: 74\n",
      "Episode: 141  steps: 37\n",
      "Episode: 142  steps: 23\n",
      "Episode: 143  steps: 29\n",
      "Episode: 144  steps: 37\n",
      "Episode: 145  steps: 25\n",
      "Episode: 146  steps: 34\n",
      "Episode: 147  steps: 23\n",
      "Episode: 148  steps: 30\n",
      "Episode: 149  steps: 26\n",
      "Episode: 150  steps: 62\n",
      "Episode: 151  steps: 88\n",
      "Episode: 152  steps: 27\n",
      "Episode: 153  steps: 73\n",
      "Episode: 154  steps: 27\n",
      "Episode: 155  steps: 23\n",
      "Episode: 156  steps: 30\n",
      "Episode: 157  steps: 11\n",
      "Episode: 158  steps: 22\n",
      "Episode: 159  steps: 45\n",
      "Episode: 160  steps: 43\n",
      "Episode: 161  steps: 29\n",
      "Episode: 162  steps: 28\n",
      "Episode: 163  steps: 30\n",
      "Episode: 164  steps: 85\n",
      "Episode: 165  steps: 47\n",
      "Episode: 166  steps: 30\n",
      "Episode: 167  steps: 36\n",
      "Episode: 168  steps: 18\n",
      "Episode: 169  steps: 24\n",
      "Episode: 170  steps: 22\n",
      "Episode: 171  steps: 9\n",
      "Episode: 172  steps: 76\n",
      "Episode: 173  steps: 16\n",
      "Episode: 174  steps: 35\n",
      "Episode: 175  steps: 21\n",
      "Episode: 176  steps: 27\n",
      "Episode: 177  steps: 24\n",
      "Episode: 178  steps: 35\n",
      "Episode: 179  steps: 30\n",
      "Episode: 180  steps: 34\n",
      "Episode: 181  steps: 21\n",
      "Episode: 182  steps: 8\n",
      "Episode: 183  steps: 23\n",
      "Episode: 184  steps: 28\n",
      "Episode: 185  steps: 22\n",
      "Episode: 186  steps: 66\n",
      "Episode: 187  steps: 26\n",
      "Episode: 188  steps: 34\n",
      "Episode: 189  steps: 25\n",
      "Episode: 190  steps: 25\n",
      "Episode: 191  steps: 26\n",
      "Episode: 192  steps: 9\n",
      "Episode: 193  steps: 43\n",
      "Episode: 194  steps: 32\n",
      "Episode: 195  steps: 31\n",
      "Episode: 196  steps: 31\n",
      "Episode: 197  steps: 36\n",
      "Episode: 198  steps: 31\n",
      "Episode: 199  steps: 22\n",
      "Episode: 200  steps: 42\n",
      "Episode: 201  steps: 27\n",
      "Episode: 202  steps: 24\n",
      "Episode: 203  steps: 38\n",
      "Episode: 204  steps: 31\n",
      "Episode: 205  steps: 28\n",
      "Episode: 206  steps: 39\n",
      "Episode: 207  steps: 33\n",
      "Episode: 208  steps: 31\n",
      "Episode: 209  steps: 63\n",
      "Episode: 210  steps: 60\n",
      "Episode: 211  steps: 51\n",
      "Episode: 212  steps: 39\n",
      "Episode: 213  steps: 38\n",
      "Episode: 214  steps: 26\n",
      "Episode: 215  steps: 30\n",
      "Episode: 216  steps: 28\n",
      "Episode: 217  steps: 23\n",
      "Episode: 218  steps: 53\n",
      "Episode: 219  steps: 26\n",
      "Episode: 220  steps: 29\n",
      "Episode: 221  steps: 20\n",
      "Episode: 222  steps: 50\n",
      "Episode: 223  steps: 20\n",
      "Episode: 224  steps: 35\n",
      "Episode: 225  steps: 29\n",
      "Episode: 226  steps: 37\n",
      "Episode: 227  steps: 35\n",
      "Episode: 228  steps: 44\n",
      "Episode: 229  steps: 28\n",
      "Episode: 230  steps: 29\n",
      "Episode: 231  steps: 24\n",
      "Episode: 232  steps: 27\n",
      "Episode: 233  steps: 27\n",
      "Episode: 234  steps: 55\n",
      "Episode: 235  steps: 37\n",
      "Episode: 236  steps: 53\n",
      "Episode: 237  steps: 55\n",
      "Episode: 238  steps: 61\n",
      "Episode: 239  steps: 54\n",
      "Episode: 240  steps: 42\n",
      "Episode: 241  steps: 43\n",
      "Episode: 242  steps: 112\n",
      "Episode: 243  steps: 28\n",
      "Episode: 244  steps: 21\n",
      "Episode: 245  steps: 35\n",
      "Episode: 246  steps: 15\n",
      "Episode: 247  steps: 46\n",
      "Episode: 248  steps: 30\n",
      "Episode: 249  steps: 23\n",
      "Episode: 250  steps: 33\n",
      "Episode: 251  steps: 33\n",
      "Episode: 252  steps: 44\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f388c8ca0780>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Train our network using target and predicted Q values on each episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "This code is based on\n",
    "https://github.com/hunkim/DeepRL-Agents\n",
    "'''\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Constants defining our neural network\n",
    "learning_rate = 1e-1\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, input_size], name = \"input_x\")\n",
    "\n",
    "# First layer of weights\n",
    "W1 = tf.get_variable(\"W1\", shape=[input_size, output_size],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "Qpred = tf.matmul(X, W1)\n",
    "\n",
    "# We need to define the parts of the network needed for learning a policy\n",
    "Y = tf.placeholder(shape=[None, output_size], dtype=tf.float32)\n",
    "\n",
    "# Loss function\n",
    "loss = tf.reduce_sum(tf.square(Y - Qpred))\n",
    "# Learning\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# Values for q learning\n",
    "max_episodes = 5000\n",
    "dis = 0.9\n",
    "step_history = []\n",
    "\n",
    "\n",
    "# Setting up our environment\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    e = 1. / ((episode / 10) + 1)\n",
    "    step_count = 0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # The Q-Network training\n",
    "    while not done:\n",
    "        step_count += 1\n",
    "        x = np.reshape(state, [1, input_size])\n",
    "        # Choose an action by greedily (with e chance of random action) from\n",
    "        # the Q-network\n",
    "        Q = sess.run(Qpred, feed_dict={X: x})\n",
    "        if np.random.rand(1) < e:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q)\n",
    "\n",
    "        # Get new state and reward from environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            Q[0, action] = -100\n",
    "        else:\n",
    "            x_next = np.reshape(next_state, [1, input_size])\n",
    "            # Obtain the Q' values by feeding the new state through our network\n",
    "            Q_next = sess.run(Qpred, feed_dict={X: x_next})\n",
    "            Q[0, action] = reward + dis * np.max(Q_next)\n",
    "\n",
    "        # Train our network using target and predicted Q values on each episode\n",
    "        sess.run(train, feed_dict={X: x, Y: Q})\n",
    "        state = next_state\n",
    "\n",
    "    step_history.append(step_count)\n",
    "    print(\"Episode: {}  steps: {}\".format(episode, step_count))\n",
    "    # If last 10's avg steps are 500, it's good enough\n",
    "    if len(step_history) > 10 and np.mean(step_history[-10:]) > 500:\n",
    "        break\n",
    "\n",
    "# See our trained network in action\n",
    "observation = env.reset()\n",
    "reward_sum = 0\n",
    "while True:\n",
    "    env.render()\n",
    "\n",
    "    x = np.reshape(observation, [1, input_size])\n",
    "    Q = sess.run(Qpred, feed_dict={X: x})\n",
    "    action = np.argmax(Q)\n",
    "\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        print(\"Total score: {}\".format(reward_sum))\n",
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN NIPS2013 TEST\n",
    "* by _sung kim_\n",
    "\n",
    "about cart_pole using DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'loss' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4fa37c7cce78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-4fa37c7cce78>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[Episode {:>5}]  steps: {:>5} e: {:>5.2f}  loss: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'loss' referenced before assignment"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DQN (NIPS 2013)\n",
    "Playing Atari with Deep Reinforcement Learning\n",
    "https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import dqn\n",
    "import gym\n",
    "from collections import deque\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.wrappers.Monitor(env, 'gym-results/', force=True)\n",
    "INPUT_SIZE = env.observation_space.shape[0]\n",
    "OUTPUT_SIZE = env.action_space.n\n",
    "\n",
    "DISCOUNT_RATE = 0.99\n",
    "REPLAY_MEMORY = 50000\n",
    "MAX_EPISODE = 5000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# minimum epsilon for epsilon greedy\n",
    "MIN_E = 0.0\n",
    "# epsilon will be `MIN_E` at `EPSILON_DECAYING_EPISODE`\n",
    "EPSILON_DECAYING_EPISODE = MAX_EPISODE * 0.01\n",
    "\n",
    "\n",
    "def bot_play(mainDQN):\n",
    "    \"\"\"Runs a single episode with rendering and prints a reward\n",
    "    Args:\n",
    "        mainDQN (dqn.DQN): DQN Agentbot_play\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = np.argmax(mainDQN.predict(state))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            print(\"Total score: {}\".format(total_reward))\n",
    "            break\n",
    "\n",
    "\n",
    "def train_minibatch(DQN, train_batch):\n",
    "    \"\"\"Prepare X_batch, y_batch and train them\n",
    "    Recall our loss function is\n",
    "        target = reward + discount * max Q(s',a)\n",
    "                 or reward if done early\n",
    "        Loss function: [target - Q(s, a)]^2\n",
    "    Hence,\n",
    "        X_batch is a state list\n",
    "        y_batch is reward + discount * max Q\n",
    "                   or reward if terminated early\n",
    "    Args:\n",
    "        DQN (dqn.DQN): DQN Agent to train & run\n",
    "        train_batch (list): Minibatch of Replay memory\n",
    "            Eeach element is a tuple of (s, a, r, s', done)\n",
    "    Returns:\n",
    "        loss: Returns a loss\n",
    "    \"\"\"\n",
    "    state_array = np.vstack([x[0] for x in train_batch])\n",
    "    action_array = np.array([x[1] for x in train_batch])\n",
    "    reward_array = np.array([x[2] for x in train_batch])\n",
    "    next_state_array = np.vstack([x[3] for x in train_batch])\n",
    "    done_array = np.array([x[4] for x in train_batch])\n",
    "\n",
    "    X_batch = state_array\n",
    "    y_batch = DQN.predict(state_array) #training by Neural Net\n",
    "\n",
    "    Q_target = reward_array + DISCOUNT_RATE * np.max(DQN.predict(next_state_array), axis=1) * ~done_array\n",
    "    y_batch[np.arange(len(X_batch)), action_array] = Q_target\n",
    "\n",
    "    # Train our network using target and predicted Q values on each episode\n",
    "    loss, _ = DQN.update(X_batch, y_batch)\n",
    "   \n",
    "    return loss\n",
    "\n",
    "\n",
    "def annealing_epsilon(episode, min_e, max_e, target_episode):\n",
    "    \"\"\"Return an linearly annealed epsilon\n",
    "    Epsilon will decrease over time until it reaches `target_episode`\n",
    "         (epsilon)\n",
    "             |\n",
    "    max_e ---|\\\n",
    "             | \\\n",
    "             |  \\\n",
    "             |   \\\n",
    "    min_e ---|____\\_______________(episode)\n",
    "                  |\n",
    "                 target_episode\n",
    "     slope = (min_e - max_e) / (target_episode)\n",
    "     intercept = max_e\n",
    "     e = slope * episode + intercept\n",
    "    Args:\n",
    "        episode (int): Current episode\n",
    "        min_e (float): Minimum epsilon\n",
    "        max_e (float): Maximum epsilon\n",
    "        target_episode (int): epsilon becomes the `min_e` at `target_episode`\n",
    "    Returns:\n",
    "        float: epsilon between `min_e` and `max_e`\n",
    "    \"\"\"\n",
    "\n",
    "    slope = (min_e - max_e) / (target_episode)\n",
    "    intercept = max_e\n",
    "\n",
    "    return max(min_e, slope * episode + intercept)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # store the previous observations in replay memory\n",
    "    replay_buffer = deque(maxlen=REPLAY_MEMORY) \n",
    "    last_100_game_reward = deque(maxlen=100)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        mainDQN = dqn.DQN(sess, INPUT_SIZE, OUTPUT_SIZE)\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        loss = 0\n",
    "\n",
    "        for episode in range(MAX_EPISODE):\n",
    "            e = annealing_epsilon(episode, MIN_E, 1.0, EPSILON_DECAYING_EPISODE)\n",
    "            done = False\n",
    "            state = env.reset()            \n",
    "\n",
    "            step_count = 0\n",
    "            while not done:\n",
    "\n",
    "                if np.random.rand() < e:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = np.argmax(mainDQN.predict(state))\n",
    "\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                if done:\n",
    "                    reward = -1\n",
    "\n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "                state = next_state\n",
    "                step_count += 1\n",
    "\n",
    "                if len(replay_buffer) > BATCH_SIZE:\n",
    "                    minibatch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "                    loss = train_minibatch(mainDQN, minibatch)\n",
    "\n",
    "            print(\"[Episode {:>5}]  steps: {:>5} e: {:>5.2f}  loss: {}\".format(episode, step_count, e, loss))\n",
    "            \n",
    "\n",
    "            # CartPole-v0 Game Clear Logic\n",
    "            last_100_game_reward.append(step_count)\n",
    "            if len(last_100_game_reward) == last_100_game_reward.maxlen:\n",
    "                avg_reward = np.mean(last_100_game_reward)\n",
    "                if avg_reward > 199.0:\n",
    "                    print(\"Game Cleared within {} episodes with avg reward {}\".format(episode, avg_reward))\n",
    "                    break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN 2015 Nature\n",
    "* Made by Sung Kim\n",
    "* Revised by Hotae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode: 0  steps: 16\n",
      "Episode: 1  steps: 44\n",
      "Episode: 2  steps: 20\n",
      "Episode: 3  steps: 19\n",
      "Episode: 4  steps: 14\n",
      "Episode: 5  steps: 33\n",
      "Episode: 6  steps: 19\n",
      "Episode: 7  steps: 26\n",
      "Episode: 8  steps: 12\n",
      "Episode: 9  steps: 13\n",
      "Episode: 10  steps: 17\n",
      "Episode: 11  steps: 10\n",
      "Episode: 12  steps: 18\n",
      "Episode: 13  steps: 9\n",
      "Episode: 14  steps: 16\n",
      "Episode: 15  steps: 10\n",
      "Episode: 16  steps: 18\n",
      "Episode: 17  steps: 10\n",
      "Episode: 18  steps: 15\n",
      "Episode: 19  steps: 11\n",
      "Episode: 20  steps: 11\n",
      "Episode: 21  steps: 9\n",
      "Episode: 22  steps: 12\n",
      "Episode: 23  steps: 14\n",
      "Episode: 24  steps: 13\n",
      "Episode: 25  steps: 9\n",
      "Episode: 26  steps: 13\n",
      "Episode: 27  steps: 8\n",
      "Episode: 28  steps: 8\n",
      "Episode: 29  steps: 10\n",
      "Episode: 30  steps: 17\n",
      "Episode: 31  steps: 12\n",
      "Episode: 32  steps: 9\n",
      "Episode: 33  steps: 11\n",
      "Episode: 34  steps: 14\n",
      "Episode: 35  steps: 11\n",
      "Episode: 36  steps: 14\n",
      "Episode: 37  steps: 9\n",
      "Episode: 38  steps: 13\n",
      "Episode: 39  steps: 8\n",
      "Episode: 40  steps: 8\n",
      "Episode: 41  steps: 10\n",
      "Episode: 42  steps: 10\n",
      "Episode: 43  steps: 8\n",
      "Episode: 44  steps: 9\n",
      "Episode: 45  steps: 9\n",
      "Episode: 46  steps: 15\n",
      "Episode: 47  steps: 12\n",
      "Episode: 48  steps: 8\n",
      "Episode: 49  steps: 14\n",
      "Episode: 50  steps: 10\n",
      "Episode: 51  steps: 9\n",
      "Episode: 52  steps: 11\n",
      "Episode: 53  steps: 12\n",
      "Episode: 54  steps: 12\n",
      "Episode: 55  steps: 12\n",
      "Episode: 56  steps: 11\n",
      "Episode: 57  steps: 12\n",
      "Episode: 58  steps: 16\n",
      "Episode: 59  steps: 16\n",
      "Episode: 60  steps: 23\n",
      "Episode: 61  steps: 23\n",
      "Episode: 62  steps: 28\n",
      "Episode: 63  steps: 11\n",
      "Episode: 64  steps: 17\n",
      "Episode: 65  steps: 12\n",
      "Episode: 66  steps: 20\n",
      "Episode: 67  steps: 9\n",
      "Episode: 68  steps: 11\n",
      "Episode: 69  steps: 24\n",
      "Episode: 70  steps: 48\n",
      "Episode: 71  steps: 38\n",
      "Episode: 72  steps: 32\n",
      "Episode: 73  steps: 48\n",
      "Episode: 74  steps: 32\n",
      "Episode: 75  steps: 32\n",
      "Episode: 76  steps: 42\n",
      "Episode: 77  steps: 81\n",
      "Episode: 78  steps: 62\n",
      "Episode: 79  steps: 44\n",
      "Episode: 80  steps: 67\n",
      "Episode: 81  steps: 79\n",
      "Episode: 82  steps: 31\n",
      "Episode: 83  steps: 38\n",
      "Episode: 84  steps: 36\n",
      "Episode: 85  steps: 35\n",
      "Episode: 86  steps: 26\n",
      "Episode: 87  steps: 41\n",
      "Episode: 88  steps: 44\n",
      "Episode: 89  steps: 36\n",
      "Episode: 90  steps: 38\n",
      "Episode: 91  steps: 40\n",
      "Episode: 92  steps: 62\n",
      "Episode: 93  steps: 30\n",
      "Episode: 94  steps: 28\n",
      "Episode: 95  steps: 43\n",
      "Episode: 96  steps: 24\n",
      "Episode: 97  steps: 30\n",
      "Episode: 98  steps: 39\n",
      "Episode: 99  steps: 36\n",
      "Episode: 100  steps: 31\n",
      "Episode: 101  steps: 25\n",
      "Episode: 102  steps: 34\n",
      "Episode: 103  steps: 26\n",
      "Episode: 104  steps: 25\n",
      "Episode: 105  steps: 85\n",
      "Episode: 106  steps: 54\n",
      "Episode: 107  steps: 153\n",
      "Episode: 108  steps: 76\n",
      "Episode: 109  steps: 47\n",
      "Episode: 110  steps: 107\n",
      "Episode: 111  steps: 85\n",
      "Episode: 112  steps: 51\n",
      "Episode: 113  steps: 64\n",
      "Episode: 114  steps: 31\n",
      "Episode: 115  steps: 76\n",
      "Episode: 116  steps: 115\n",
      "Episode: 117  steps: 79\n",
      "Episode: 118  steps: 55\n",
      "Episode: 119  steps: 200\n",
      "Episode: 120  steps: 196\n",
      "Episode: 121  steps: 147\n",
      "Episode: 122  steps: 130\n",
      "Episode: 123  steps: 174\n",
      "Episode: 124  steps: 53\n",
      "Episode: 125  steps: 107\n",
      "Episode: 126  steps: 49\n",
      "Episode: 127  steps: 200\n",
      "Episode: 128  steps: 200\n",
      "Episode: 129  steps: 200\n",
      "Episode: 130  steps: 62\n",
      "Episode: 131  steps: 200\n",
      "Episode: 132  steps: 200\n",
      "Episode: 133  steps: 116\n",
      "Episode: 134  steps: 200\n",
      "Episode: 135  steps: 200\n",
      "Episode: 136  steps: 200\n",
      "Episode: 137  steps: 200\n",
      "Episode: 138  steps: 200\n",
      "Episode: 139  steps: 200\n",
      "Episode: 140  steps: 200\n",
      "Episode: 141  steps: 200\n",
      "Episode: 142  steps: 200\n",
      "Episode: 143  steps: 200\n",
      "Episode: 144  steps: 200\n",
      "Episode: 145  steps: 200\n",
      "Episode: 146  steps: 200\n",
      "Episode: 147  steps: 200\n",
      "Episode: 148  steps: 200\n",
      "Episode: 149  steps: 200\n",
      "Episode: 150  steps: 200\n",
      "Episode: 151  steps: 200\n",
      "Episode: 152  steps: 200\n",
      "Episode: 153  steps: 200\n",
      "Episode: 154  steps: 200\n",
      "Episode: 155  steps: 200\n",
      "Episode: 156  steps: 200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bdf6977f521d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-bdf6977f521d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0;31m# Choose an action by greedily from the Q-network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# Get new state and reward from environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/rl_test/dqn.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \"\"\"\n\u001b[1;32m     67\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Qpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_X\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m           if (not is_tensor_handle_feed and\n\u001b[0;32m-> 1107\u001b[0;31m               not subfeed_t.get_shape().is_compatible_with(np_val.shape)):\n\u001b[0m\u001b[1;32m   1108\u001b[0m             raise ValueError('Cannot feed value of shape %r for Tensor %r, '\n\u001b[1;32m   1109\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.pyc\u001b[0m in \u001b[0;36mis_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \"\"\"\n\u001b[0;32m--> 822\u001b[0;31m     \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.pyc\u001b[0m in \u001b[0;36mas_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m   \u001b[0;34m\"\"\"Converts the given object to a TensorShape.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Double DQN (Nature 2015)\n",
    "http://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf\n",
    "Notes:\n",
    "    The difference is that now there are two DQNs (DQN & Target DQN)\n",
    "    y_i = r_i + 𝛾 * max(Q(next_state, action; 𝜃_target))\n",
    "    Loss: (y_i - Q(state, action; 𝜃))^2\n",
    "    Every C step, 𝜃_target <- 𝜃\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque\n",
    "import dqn\n",
    "\n",
    "import gym\n",
    "from typing import List\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.wrappers.Monitor(env, directory=\"gym-results/\", force=True)\n",
    "\n",
    "# Constants defining our neural network\n",
    "INPUT_SIZE = env.observation_space.shape[0]\n",
    "OUTPUT_SIZE = env.action_space.n\n",
    "\n",
    "DISCOUNT_RATE = 0.99\n",
    "REPLAY_MEMORY = 50000\n",
    "BATCH_SIZE = 64\n",
    "TARGET_UPDATE_FREQUENCY = 5\n",
    "MAX_EPISODES = 5000\n",
    "\n",
    "\n",
    "def replay_train(mainDQN, targetDQN, train_batch):\n",
    "    \"\"\"Trains `mainDQN` with target Q values given by `targetDQN`\n",
    "    Args:\n",
    "        mainDQN (dqn.DQN): Main DQN that will be trained\n",
    "        targetDQN (dqn.DQN): Target DQN that will predict Q_target\n",
    "        train_batch (list): Minibatch of replay memory\n",
    "            Each element is (s, a, r, s', done)\n",
    "            [(state, action, reward, next_state, done), ...]\n",
    "    Returns:\n",
    "        float: After updating `mainDQN`, it returns a `loss`\n",
    "    \"\"\"\n",
    "    states = np.vstack([x[0] for x in train_batch])\n",
    "    actions = np.array([x[1] for x in train_batch])\n",
    "    rewards = np.array([x[2] for x in train_batch])\n",
    "    next_states = np.vstack([x[3] for x in train_batch])\n",
    "    done = np.array([x[4] for x in train_batch])\n",
    "\n",
    "    X = states\n",
    "\n",
    "    Q_target = rewards + DISCOUNT_RATE * np.max(targetDQN.predict(next_states), axis=1) * ~done\n",
    "\n",
    "    y = mainDQN.predict(states)\n",
    "    y[np.arange(len(X)), actions] = Q_target\n",
    "\n",
    "    # Train our network using target and predicted Q values on each episode\n",
    "    return mainDQN.update(X, y)\n",
    "\n",
    "\n",
    "def get_copy_var_ops(dest_scope_name, src_scope_name):\n",
    "    \"\"\"Creates TF operations that copy weights from `src_scope` to `dest_scope`\n",
    "    Args:\n",
    "        dest_scope_name (str): Destination weights (copy to)\n",
    "        src_scope_name (str): Source weight (copy from)\n",
    "    Returns:\n",
    "        List[tf.Operation]: Update operations are created and returned\n",
    "    \"\"\"\n",
    "    # Copy variables src_scope to dest_scope\n",
    "    op_holder = []\n",
    "\n",
    "    src_vars = tf.get_collection(\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES, scope=src_scope_name)\n",
    "    dest_vars = tf.get_collection(\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES, scope=dest_scope_name)\n",
    "\n",
    "    for src_var, dest_var in zip(src_vars, dest_vars):\n",
    "        op_holder.append(dest_var.assign(src_var.value()))\n",
    "\n",
    "    return op_holder\n",
    "\n",
    "\n",
    "def bot_play(mainDQN, env):\n",
    "    \"\"\"Test runs with rendering and prints the total score\n",
    "    Args:\n",
    "        mainDQN (dqn.DQN): DQN agent to run a test\n",
    "        env (gym.Env): Gym Environment\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    reward_sum = 0\n",
    "\n",
    "    while True:\n",
    "\n",
    "        env.render()\n",
    "        action = np.argmax(mainDQN.predict(state))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        if done:\n",
    "            print(\"Total score: {}\".format(reward_sum))\n",
    "            break\n",
    "\n",
    "\n",
    "def main():\n",
    "    # store the previous observations in replay memory\n",
    "    replay_buffer = deque(maxlen=REPLAY_MEMORY)\n",
    "\n",
    "    last_100_game_reward = deque(maxlen=100)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        mainDQN = dqn.DQN(sess, INPUT_SIZE, OUTPUT_SIZE, name=\"main\")\n",
    "        targetDQN = dqn.DQN(sess, INPUT_SIZE, OUTPUT_SIZE, name=\"target\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # initial copy q_net -> target_net\n",
    "        copy_ops = get_copy_var_ops(dest_scope_name=\"target\",\n",
    "                                    src_scope_name=\"main\")\n",
    "        sess.run(copy_ops)\n",
    "\n",
    "        for episode in range(MAX_EPISODES):\n",
    "            e = 1. / ((episode / 10) + 1)\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            state = env.reset()\n",
    "\n",
    "            while not done:\n",
    "                if np.random.rand() < e:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    # Choose an action by greedily from the Q-network\n",
    "                    action = np.argmax(mainDQN.predict(state))\n",
    "\n",
    "                # Get new state and reward from environment\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                if done:  # Penalty\n",
    "                    reward = -1\n",
    "\n",
    "                # Save the experience to our buffer\n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "                if len(replay_buffer) > BATCH_SIZE:\n",
    "                    minibatch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "                    loss, _ = replay_train(mainDQN, targetDQN, minibatch)\n",
    "\n",
    "                if step_count % TARGET_UPDATE_FREQUENCY == 0:\n",
    "                    sess.run(copy_ops)\n",
    "\n",
    "                state = next_state\n",
    "                step_count += 1\n",
    "\n",
    "            print(\"Episode: {}  steps: {}\".format(episode, step_count))\n",
    "\n",
    "            # CartPole-v0 Game Clear Checking Logic\n",
    "            last_100_game_reward.append(step_count)\n",
    "\n",
    "            if len(last_100_game_reward) == last_100_game_reward.maxlen:\n",
    "                avg_reward = np.mean(last_100_game_reward)\n",
    "\n",
    "                if avg_reward > 199:\n",
    "                    print(\"Game Cleared in {episode} episodes with avg reward {avg_reward}\")\n",
    "                    break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy gradient - cartpole\n",
    "* by Sung KIm\n",
    "* revised Hotae, logistic classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[Episode     0] Reward: 13.0 Loss:    0.60864\n",
      "[Episode     1] Reward: 21.0 Loss:     2.0819\n",
      "[Episode     2] Reward:  9.0 Loss:   -0.88321\n",
      "[Episode     3] Reward: 13.0 Loss:   -0.29381\n",
      "[Episode     4] Reward: 55.0 Loss:    0.50959\n",
      "[Episode     5] Reward: 63.0 Loss:   -0.51001\n",
      "[Episode     6] Reward: 11.0 Loss:   -0.13696\n",
      "[Episode     7] Reward: 21.0 Loss:     -1.075\n",
      "[Episode     8] Reward: 18.0 Loss:   -0.30048\n",
      "[Episode     9] Reward: 20.0 Loss:   -0.49842\n",
      "[Episode    10] Reward: 33.0 Loss:     1.3165\n",
      "[Episode    11] Reward: 14.0 Loss:    0.99391\n",
      "[Episode    12] Reward: 12.0 Loss:    0.69467\n",
      "[Episode    13] Reward: 14.0 Loss:    -0.3432\n",
      "[Episode    14] Reward: 11.0 Loss:    0.98703\n",
      "[Episode    15] Reward: 44.0 Loss:    0.35846\n",
      "[Episode    16] Reward: 51.0 Loss:     1.1509\n",
      "[Episode    17] Reward: 11.0 Loss:    0.16704\n",
      "[Episode    18] Reward: 12.0 Loss:    0.75571\n",
      "[Episode    19] Reward: 31.0 Loss:   -0.59869\n",
      "[Episode    20] Reward: 13.0 Loss:   -0.28639\n",
      "[Episode    21] Reward: 11.0 Loss:    0.11176\n",
      "[Episode    22] Reward: 14.0 Loss:   -0.22324\n",
      "[Episode    23] Reward: 25.0 Loss:    0.93837\n",
      "[Episode    24] Reward: 11.0 Loss:     1.9565\n",
      "[Episode    25] Reward: 41.0 Loss:   -0.52439\n",
      "[Episode    26] Reward: 27.0 Loss:     0.5838\n",
      "[Episode    27] Reward: 13.0 Loss:   -0.38735\n",
      "[Episode    28] Reward: 21.0 Loss:    0.34701\n",
      "[Episode    29] Reward: 20.0 Loss:   -0.13592\n",
      "[Episode    30] Reward: 26.0 Loss:   0.029688\n",
      "[Episode    31] Reward: 21.0 Loss:     1.1387\n",
      "[Episode    32] Reward: 37.0 Loss:    0.42123\n",
      "[Episode    33] Reward: 22.0 Loss:    0.77763\n",
      "[Episode    34] Reward: 18.0 Loss:  -0.016177\n",
      "[Episode    35] Reward: 19.0 Loss:    0.73739\n",
      "[Episode    36] Reward: 26.0 Loss:    0.91943\n",
      "[Episode    37] Reward: 16.0 Loss:   -0.21661\n",
      "[Episode    38] Reward: 14.0 Loss:   -0.19652\n",
      "[Episode    39] Reward: 30.0 Loss:   0.094099\n",
      "[Episode    40] Reward: 30.0 Loss:   -0.39061\n",
      "[Episode    41] Reward: 18.0 Loss:  -0.049321\n",
      "[Episode    42] Reward: 32.0 Loss:   -0.26643\n",
      "[Episode    43] Reward: 32.0 Loss:   -0.50275\n",
      "[Episode    44] Reward: 28.0 Loss:    0.22925\n",
      "[Episode    45] Reward: 23.0 Loss:   -0.12492\n",
      "[Episode    46] Reward: 13.0 Loss:     1.0095\n",
      "[Episode    47] Reward: 16.0 Loss:  0.0034029\n",
      "[Episode    48] Reward: 39.0 Loss:   -0.32242\n",
      "[Episode    49] Reward: 12.0 Loss:  -0.063647\n",
      "[Episode    50] Reward: 28.0 Loss:     0.7546\n",
      "[Episode    51] Reward: 17.0 Loss:   -0.01877\n",
      "[Episode    52] Reward: 17.0 Loss:    0.33006\n",
      "[Episode    53] Reward: 20.0 Loss:   0.067014\n",
      "[Episode    54] Reward: 41.0 Loss:    -1.4623\n",
      "[Episode    55] Reward: 22.0 Loss:   0.046865\n",
      "[Episode    56] Reward: 13.0 Loss:    0.09867\n",
      "[Episode    57] Reward: 142.0 Loss:   -0.23395\n",
      "[Episode    58] Reward: 42.0 Loss:   0.044139\n",
      "[Episode    59] Reward: 44.0 Loss:   -0.98909\n",
      "[Episode    60] Reward: 27.0 Loss:    -2.3236\n",
      "[Episode    61] Reward: 30.0 Loss:    0.11867\n",
      "[Episode    62] Reward: 49.0 Loss:   -0.41675\n",
      "[Episode    63] Reward: 13.0 Loss:  -0.060206\n",
      "[Episode    64] Reward: 30.0 Loss:   -0.18612\n",
      "[Episode    65] Reward: 14.0 Loss:  0.0091887\n",
      "[Episode    66] Reward: 57.0 Loss:   -0.70738\n",
      "[Episode    67] Reward: 47.0 Loss:    -1.0445\n",
      "[Episode    68] Reward: 24.0 Loss:    -2.5143\n",
      "[Episode    69] Reward: 29.0 Loss:   -0.20105\n",
      "[Episode    70] Reward: 14.0 Loss:   0.088086\n",
      "[Episode    71] Reward: 39.0 Loss:    0.47532\n",
      "[Episode    72] Reward: 86.0 Loss:    -2.6407\n",
      "[Episode    73] Reward: 51.0 Loss:     0.2153\n",
      "[Episode    74] Reward: 66.0 Loss:   -0.55062\n",
      "[Episode    75] Reward: 72.0 Loss:     1.4068\n",
      "[Episode    76] Reward: 54.0 Loss:    0.84872\n",
      "[Episode    77] Reward: 34.0 Loss:     -1.502\n",
      "[Episode    78] Reward: 44.0 Loss:   -0.14526\n",
      "[Episode    79] Reward: 25.0 Loss:   0.037046\n",
      "[Episode    80] Reward: 44.0 Loss:    0.39408\n",
      "[Episode    81] Reward: 89.0 Loss:    0.87354\n",
      "[Episode    82] Reward: 67.0 Loss:     0.6836\n",
      "[Episode    83] Reward: 24.0 Loss:   -0.30477\n",
      "[Episode    84] Reward: 28.0 Loss:   -0.33958\n",
      "[Episode    85] Reward: 70.0 Loss:    -3.3876\n",
      "[Episode    86] Reward: 19.0 Loss:     1.6192\n",
      "[Episode    87] Reward: 64.0 Loss:   -0.57433\n",
      "[Episode    88] Reward: 29.0 Loss:   -0.36058\n",
      "[Episode    89] Reward: 14.0 Loss:    -0.9324\n",
      "[Episode    90] Reward: 34.0 Loss:    -1.0288\n",
      "[Episode    91] Reward: 85.0 Loss:    -4.4498\n",
      "[Episode    92] Reward: 157.0 Loss:     3.5297\n",
      "[Episode    93] Reward: 47.0 Loss:    0.86689\n",
      "[Episode    94] Reward: 68.0 Loss:    0.53727\n",
      "[Episode    95] Reward: 37.0 Loss:   -0.79644\n",
      "[Episode    96] Reward: 32.0 Loss:    0.19718\n",
      "[Episode    97] Reward: 37.0 Loss:    -0.9725\n",
      "[Episode    98] Reward: 49.0 Loss:       1.02\n",
      "[Episode    99] Reward: 21.0 Loss:    -1.6452\n",
      "[Episode   100] Reward: 29.0 Loss:    -1.4042\n",
      "[Episode   101] Reward: 42.0 Loss:     1.2698\n",
      "[Episode   102] Reward: 82.0 Loss:     4.1644\n",
      "[Episode   103] Reward: 59.0 Loss:   -0.84099\n",
      "[Episode   104] Reward: 58.0 Loss:   -0.45244\n",
      "[Episode   105] Reward: 79.0 Loss:    -1.5215\n",
      "[Episode   106] Reward: 39.0 Loss:    -3.9586\n",
      "[Episode   107] Reward: 15.0 Loss:    0.45966\n",
      "[Episode   108] Reward: 17.0 Loss:    0.96966\n",
      "[Episode   109] Reward: 166.0 Loss:     -1.026\n",
      "[Episode   110] Reward: 73.0 Loss:     1.1272\n",
      "[Episode   111] Reward: 74.0 Loss: -0.0097425\n",
      "[Episode   112] Reward: 39.0 Loss:    0.54327\n",
      "[Episode   113] Reward: 110.0 Loss:     -2.513\n",
      "[Episode   114] Reward: 70.0 Loss:    -2.7411\n",
      "[Episode   115] Reward: 184.0 Loss:   -0.22054\n",
      "[Episode   116] Reward: 47.0 Loss:    0.27831\n",
      "[Episode   117] Reward: 59.0 Loss:    -1.5699\n",
      "[Episode   118] Reward: 79.0 Loss:     2.0112\n",
      "[Episode   119] Reward: 90.0 Loss:   -0.34555\n",
      "[Episode   120] Reward: 72.0 Loss:     1.1913\n",
      "[Episode   121] Reward: 83.0 Loss:   -0.81514\n",
      "[Episode   122] Reward: 90.0 Loss:    0.28941\n",
      "[Episode   123] Reward: 55.0 Loss:   -0.26958\n",
      "[Episode   124] Reward: 92.0 Loss:   -0.73711\n",
      "[Episode   125] Reward: 102.0 Loss:    -2.2724\n",
      "[Episode   126] Reward: 129.0 Loss:    0.98968\n",
      "[Episode   127] Reward: 108.0 Loss:    -2.2703\n",
      "[Episode   128] Reward: 107.0 Loss:     -3.235\n",
      "[Episode   129] Reward: 70.0 Loss:    -2.8212\n",
      "[Episode   130] Reward: 147.0 Loss:     2.0055\n",
      "[Episode   131] Reward: 94.0 Loss:    -2.7137\n",
      "[Episode   132] Reward: 104.0 Loss:    -3.6149\n",
      "[Episode   133] Reward: 35.0 Loss:     1.5514\n",
      "[Episode   134] Reward: 41.0 Loss:   -0.64563\n",
      "[Episode   135] Reward: 61.0 Loss:     1.8414\n",
      "[Episode   136] Reward: 36.0 Loss:  -0.055276\n",
      "[Episode   137] Reward: 126.0 Loss:   -0.38001\n",
      "[Episode   138] Reward: 113.0 Loss:    -5.6867\n",
      "[Episode   139] Reward: 95.0 Loss:    -1.2922\n",
      "[Episode   140] Reward: 42.0 Loss:    -1.0541\n",
      "[Episode   141] Reward: 200.0 Loss:     2.4268\n",
      "[Episode   142] Reward: 185.0 Loss:    -2.4322\n",
      "[Episode   143] Reward: 152.0 Loss:    0.85668\n",
      "[Episode   144] Reward: 92.0 Loss:    0.54619\n",
      "[Episode   145] Reward: 131.0 Loss:    -1.6014\n",
      "[Episode   146] Reward: 136.0 Loss:   -0.80686\n",
      "[Episode   147] Reward: 120.0 Loss:    -1.5749\n",
      "[Episode   148] Reward: 61.0 Loss:    -0.2386\n",
      "[Episode   149] Reward: 115.0 Loss:     1.2187\n",
      "[Episode   150] Reward: 149.0 Loss:    -1.5939\n",
      "[Episode   151] Reward: 84.0 Loss:    -1.4636\n",
      "[Episode   152] Reward: 64.0 Loss:     1.6269\n",
      "[Episode   153] Reward: 91.0 Loss:   -0.12398\n",
      "[Episode   154] Reward: 150.0 Loss:   -0.14096\n",
      "[Episode   155] Reward: 100.0 Loss:   -0.21873\n",
      "[Episode   156] Reward: 98.0 Loss:     2.6766\n",
      "[Episode   157] Reward: 167.0 Loss:     1.3534\n",
      "[Episode   158] Reward: 180.0 Loss:   -0.89085\n",
      "[Episode   159] Reward: 200.0 Loss:     2.6081\n",
      "[Episode   160] Reward: 125.0 Loss:     1.3086\n",
      "[Episode   161] Reward: 154.0 Loss:    -2.5247\n",
      "[Episode   162] Reward: 197.0 Loss:     3.0227\n",
      "[Episode   163] Reward: 107.0 Loss:    -7.2642\n",
      "[Episode   164] Reward: 119.0 Loss:    -7.0043\n",
      "[Episode   165] Reward: 159.0 Loss:     6.0137\n",
      "[Episode   166] Reward: 200.0 Loss:     1.8648\n",
      "[Episode   167] Reward: 173.0 Loss:    -3.9627\n",
      "[Episode   168] Reward: 200.0 Loss:   -0.67355\n",
      "[Episode   169] Reward: 49.0 Loss:    -0.5164\n",
      "[Episode   170] Reward: 147.0 Loss:    -5.8238\n",
      "[Episode   171] Reward: 58.0 Loss:     1.4405\n",
      "[Episode   172] Reward: 197.0 Loss:     5.1937\n",
      "[Episode   173] Reward: 163.0 Loss:   -0.69203\n",
      "[Episode   174] Reward: 107.0 Loss:     1.0991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode   175] Reward: 200.0 Loss:    0.39511\n",
      "[Episode   176] Reward: 88.0 Loss:      5.993\n",
      "[Episode   177] Reward: 138.0 Loss:      -2.44\n",
      "[Episode   178] Reward: 74.0 Loss:   -0.66181\n",
      "[Episode   179] Reward: 114.0 Loss:     3.7968\n",
      "[Episode   180] Reward: 78.0 Loss:     2.7774\n",
      "[Episode   181] Reward: 176.0 Loss:     2.1454\n",
      "[Episode   182] Reward: 139.0 Loss:    0.42752\n",
      "[Episode   183] Reward: 200.0 Loss:    -5.3246\n",
      "[Episode   184] Reward: 69.0 Loss:      6.814\n",
      "[Episode   185] Reward: 200.0 Loss:    -1.1385\n",
      "[Episode   186] Reward: 81.0 Loss:    -1.8012\n",
      "[Episode   187] Reward: 144.0 Loss:     -3.799\n",
      "[Episode   188] Reward: 92.0 Loss:   -0.34131\n",
      "[Episode   189] Reward: 200.0 Loss:    -2.9547\n",
      "[Episode   190] Reward: 200.0 Loss:    -2.6662\n",
      "[Episode   191] Reward: 119.0 Loss:     3.5621\n",
      "[Episode   192] Reward: 54.0 Loss:     3.1151\n",
      "[Episode   193] Reward: 109.0 Loss:     3.2466\n",
      "[Episode   194] Reward: 128.0 Loss:     1.2536\n",
      "[Episode   195] Reward: 180.0 Loss:     6.4702\n",
      "[Episode   196] Reward: 103.0 Loss:     2.5601\n",
      "[Episode   197] Reward: 200.0 Loss:     7.6012\n",
      "[Episode   198] Reward: 83.0 Loss:    0.91261\n",
      "[Episode   199] Reward: 200.0 Loss:   -0.15339\n",
      "[Episode   200] Reward: 200.0 Loss:     5.8998\n",
      "[Episode   201] Reward: 168.0 Loss:    -6.9029\n",
      "[Episode   202] Reward: 103.0 Loss:    -2.0872\n",
      "[Episode   203] Reward: 200.0 Loss:   -0.66076\n",
      "[Episode   204] Reward: 90.0 Loss:     4.4422\n",
      "[Episode   205] Reward: 111.0 Loss:     2.6441\n",
      "[Episode   206] Reward: 158.0 Loss:     2.3522\n",
      "[Episode   207] Reward: 200.0 Loss:     11.403\n",
      "[Episode   208] Reward: 112.0 Loss:     5.9494\n",
      "[Episode   209] Reward: 200.0 Loss:    0.49789\n",
      "[Episode   210] Reward: 200.0 Loss:     6.4823\n",
      "[Episode   211] Reward: 184.0 Loss:     8.3135\n",
      "[Episode   212] Reward: 200.0 Loss:     4.0535\n",
      "[Episode   213] Reward: 200.0 Loss:     5.3911\n",
      "[Episode   214] Reward: 185.0 Loss:     1.5798\n",
      "[Episode   215] Reward: 200.0 Loss:     3.6039\n",
      "[Episode   216] Reward: 200.0 Loss:     2.7837\n",
      "[Episode   217] Reward: 142.0 Loss:     2.2348\n",
      "[Episode   218] Reward: 200.0 Loss:     1.0664\n",
      "[Episode   219] Reward: 191.0 Loss:     2.3941\n",
      "[Episode   220] Reward: 200.0 Loss:    -4.2515\n",
      "[Episode   221] Reward: 200.0 Loss:    -1.9854\n",
      "[Episode   222] Reward: 200.0 Loss:    -6.9287\n",
      "[Episode   223] Reward: 200.0 Loss:       1.92\n",
      "[Episode   224] Reward: 200.0 Loss:     2.3378\n",
      "[Episode   225] Reward: 200.0 Loss:     2.6579\n",
      "[Episode   226] Reward: 200.0 Loss:     8.4384\n",
      "[Episode   227] Reward: 145.0 Loss:     6.0015\n",
      "[Episode   228] Reward: 200.0 Loss:     2.3793\n",
      "[Episode   229] Reward: 200.0 Loss:     5.0544\n",
      "[Episode   230] Reward: 200.0 Loss:    -4.7184\n",
      "[Episode   231] Reward: 200.0 Loss:     3.1229\n",
      "[Episode   232] Reward: 200.0 Loss:   -0.22728\n",
      "[Episode   233] Reward: 200.0 Loss:     0.1902\n",
      "[Episode   234] Reward: 200.0 Loss:   -0.98368\n",
      "[Episode   235] Reward: 200.0 Loss:    0.43076\n",
      "[Episode   236] Reward: 200.0 Loss:    -3.7379\n",
      "[Episode   237] Reward: 200.0 Loss:    -1.8907\n",
      "[Episode   238] Reward: 46.0 Loss:    -10.025\n",
      "[Episode   239] Reward: 200.0 Loss:   -0.54661\n",
      "[Episode   240] Reward: 193.0 Loss:    0.71645\n",
      "[Episode   241] Reward: 200.0 Loss:     1.0107\n",
      "[Episode   242] Reward: 200.0 Loss:    -2.4109\n",
      "[Episode   243] Reward: 200.0 Loss:     4.3607\n",
      "[Episode   244] Reward: 103.0 Loss:    -1.0032\n",
      "[Episode   245] Reward: 200.0 Loss:     4.2593\n",
      "[Episode   246] Reward: 200.0 Loss:     2.7161\n",
      "[Episode   247] Reward: 200.0 Loss:     -4.829\n",
      "[Episode   248] Reward: 200.0 Loss:  -0.068662\n",
      "[Episode   249] Reward: 200.0 Loss:     2.1741\n",
      "[Episode   250] Reward: 200.0 Loss:     5.8622\n",
      "[Episode   251] Reward: 200.0 Loss:     1.3466\n",
      "[Episode   252] Reward: 200.0 Loss:    0.72019\n",
      "[Episode   253] Reward: 166.0 Loss:    0.44705\n",
      "[Episode   254] Reward: 193.0 Loss:     4.4569\n",
      "[Episode   255] Reward: 200.0 Loss:   -0.29782\n",
      "[Episode   256] Reward: 200.0 Loss:    -6.2066\n",
      "[Episode   257] Reward: 200.0 Loss:     5.0521\n",
      "[Episode   258] Reward: 200.0 Loss:   -0.97117\n",
      "[Episode   259] Reward: 200.0 Loss:    -2.2188\n",
      "[Episode   260] Reward: 143.0 Loss:     7.4637\n",
      "[Episode   261] Reward: 200.0 Loss:   -0.92849\n",
      "[Episode   262] Reward: 200.0 Loss:     5.1277\n",
      "[Episode   263] Reward: 200.0 Loss:    -3.2393\n",
      "[Episode   264] Reward: 200.0 Loss:     -5.251\n",
      "[Episode   265] Reward: 200.0 Loss:     -2.305\n",
      "[Episode   266] Reward: 200.0 Loss:   -0.57321\n",
      "[Episode   267] Reward: 200.0 Loss:     1.9255\n",
      "[Episode   268] Reward: 200.0 Loss:    -1.9155\n",
      "[Episode   269] Reward: 196.0 Loss:    -3.0204\n",
      "[Episode   270] Reward: 200.0 Loss:     5.3062\n",
      "[Episode   271] Reward: 200.0 Loss:     4.3113\n",
      "[Episode   272] Reward: 179.0 Loss:    -4.8041\n",
      "[Episode   273] Reward: 200.0 Loss:   -0.17921\n",
      "[Episode   274] Reward: 183.0 Loss:   -0.07391\n",
      "[Episode   275] Reward: 200.0 Loss:     1.6845\n",
      "[Episode   276] Reward: 200.0 Loss:     6.5226\n",
      "[Episode   277] Reward: 200.0 Loss:     6.7444\n",
      "[Episode   278] Reward: 200.0 Loss:     2.4166\n",
      "[Episode   279] Reward: 200.0 Loss:    -7.4485\n",
      "[Episode   280] Reward: 200.0 Loss:     3.7385\n",
      "[Episode   281] Reward: 200.0 Loss:     5.1119\n",
      "[Episode   282] Reward: 196.0 Loss:     7.1571\n",
      "[Episode   283] Reward: 200.0 Loss:     1.0134\n",
      "[Episode   284] Reward: 192.0 Loss:    -2.1154\n",
      "[Episode   285] Reward: 200.0 Loss:    -7.0866\n",
      "[Episode   286] Reward: 200.0 Loss:      7.201\n",
      "[Episode   287] Reward: 200.0 Loss:     3.1392\n",
      "[Episode   288] Reward: 200.0 Loss:     2.2838\n",
      "[Episode   289] Reward: 200.0 Loss:     3.4486\n",
      "[Episode   290] Reward: 165.0 Loss:    -2.0334\n",
      "[Episode   291] Reward: 200.0 Loss:     9.0665\n",
      "[Episode   292] Reward: 200.0 Loss:     6.7516\n",
      "[Episode   293] Reward: 200.0 Loss:     4.2306\n",
      "[Episode   294] Reward: 200.0 Loss:    -1.2202\n",
      "[Episode   295] Reward: 200.0 Loss:     5.0285\n",
      "[Episode   296] Reward: 200.0 Loss:     2.0167\n",
      "[Episode   297] Reward: 200.0 Loss:    -2.5742\n",
      "[Episode   298] Reward: 200.0 Loss:     7.7966\n",
      "[Episode   299] Reward: 200.0 Loss:   0.087677\n",
      "[Episode   300] Reward: 200.0 Loss:     2.6849\n",
      "[Episode   301] Reward: 200.0 Loss:     5.2488\n",
      "[Episode   302] Reward: 200.0 Loss:   -0.54825\n",
      "[Episode   303] Reward: 200.0 Loss:   -0.91877\n",
      "[Episode   304] Reward: 86.0 Loss:   -0.58179\n",
      "[Episode   305] Reward: 200.0 Loss:     0.8424\n",
      "[Episode   306] Reward: 144.0 Loss:    -5.2871\n",
      "[Episode   307] Reward: 200.0 Loss:    -1.5907\n",
      "[Episode   308] Reward: 200.0 Loss:    -4.6229\n",
      "[Episode   309] Reward: 200.0 Loss:     9.1101\n",
      "[Episode   310] Reward: 200.0 Loss:     4.5846\n",
      "[Episode   311] Reward: 200.0 Loss:    0.17664\n",
      "[Episode   312] Reward: 200.0 Loss:     1.1347\n",
      "[Episode   313] Reward: 200.0 Loss:     6.4283\n",
      "[Episode   314] Reward: 200.0 Loss:    -2.5254\n",
      "[Episode   315] Reward: 200.0 Loss:    -3.7775\n",
      "[Episode   316] Reward: 200.0 Loss:    -1.2602\n",
      "[Episode   317] Reward: 200.0 Loss:     -4.801\n",
      "[Episode   318] Reward: 200.0 Loss:    -3.4891\n",
      "[Episode   319] Reward: 200.0 Loss:     1.6588\n",
      "[Episode   320] Reward: 200.0 Loss:      2.751\n",
      "[Episode   321] Reward: 200.0 Loss:    -2.5134\n",
      "[Episode   322] Reward: 200.0 Loss:    0.78784\n",
      "[Episode   323] Reward: 200.0 Loss:    -1.5275\n",
      "[Episode   324] Reward: 200.0 Loss:      1.491\n",
      "[Episode   325] Reward: 140.0 Loss:    -8.0618\n",
      "[Episode   326] Reward: 200.0 Loss:   -0.26736\n",
      "[Episode   327] Reward: 200.0 Loss:    -1.7236\n",
      "[Episode   328] Reward: 200.0 Loss:      1.445\n",
      "[Episode   329] Reward: 162.0 Loss:     2.5437\n",
      "[Episode   330] Reward: 200.0 Loss:    -3.3629\n",
      "[Episode   331] Reward: 150.0 Loss:    -4.8565\n",
      "[Episode   332] Reward: 200.0 Loss:     4.6593\n",
      "[Episode   333] Reward: 200.0 Loss:     1.3396\n",
      "[Episode   334] Reward: 200.0 Loss:    -1.5258\n",
      "[Episode   335] Reward: 200.0 Loss:    -3.8407\n",
      "[Episode   336] Reward: 200.0 Loss:   -0.26338\n",
      "[Episode   337] Reward: 200.0 Loss:     1.1155\n",
      "[Episode   338] Reward: 200.0 Loss:     -2.779\n",
      "[Episode   339] Reward: 200.0 Loss:     3.4815\n",
      "[Episode   340] Reward: 200.0 Loss:      -3.91\n",
      "[Episode   341] Reward: 200.0 Loss:     0.8358\n",
      "[Episode   342] Reward: 200.0 Loss:    -3.1986\n",
      "[Episode   343] Reward: 200.0 Loss:    -2.9637\n",
      "[Episode   344] Reward: 200.0 Loss:   -0.31828\n",
      "[Episode   345] Reward: 200.0 Loss:    -0.2093\n",
      "[Episode   346] Reward: 200.0 Loss:     7.8822\n",
      "[Episode   347] Reward: 200.0 Loss:     3.9251\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4a4244a8e203>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# Run the neural net to determine output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0maction_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Determine the output based on our net, allowing for some randomness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1120\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \"\"\"\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;31m# Did not find anything.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    280\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[0;32m--> 282\u001b[0;31m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[1;32m    283\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         raise TypeError('Fetch argument %r has invalid type %r, '\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3589\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3590\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3592\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3666\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3667\u001b[0m       \u001b[0;31m# Actually obj is just the object it's referring to.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3668\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3669\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3670\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mgraph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;34m\"\"\"The `Graph` that contains this tensor.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "This code is based on:\n",
    "https://github.com/hunkim/DeepRL-Agents\n",
    "http://karpathy.github.io/2016/05/31/rl/\n",
    "'''\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "hidden_layer_neurons = 24\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# Constants defining our neural network\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = 1  # logistic regression, one p output\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, input_size], name=\"input_x\")\n",
    "\n",
    "# First layer of weights\n",
    "W1 = tf.get_variable(\"W1\", shape=[input_size, hidden_layer_neurons],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W1))\n",
    "\n",
    "# Second layer of weights\n",
    "W2 = tf.get_variable(\"W2\", shape=[hidden_layer_neurons, output_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "action_pred = tf.nn.sigmoid(tf.matmul(layer1, W2))\n",
    "\n",
    "# Y (fake) and advantages (rewards)\n",
    "Y = tf.placeholder(tf.float32, [None, output_size], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32, name=\"reward_signal\")\n",
    "\n",
    "# Loss function: log_likelihood * advantages\n",
    "#log_lik = -tf.log(Y * action_pred + (1 - Y) * (1 - action_pred))     # using author(awjuliani)'s original cost function (maybe log_likelihood)\n",
    "log_lik = -Y*tf.log(action_pred) - (1 - Y)*tf.log(1 - action_pred)    # using logistic regression cost function, 0 or 1 two classification\n",
    "loss = tf.reduce_sum(log_lik * advantages)\n",
    "\n",
    "# Learning\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "def discount_rewards(r, gamma=0.99):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r, dtype=np.float32)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(len(r))):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "\n",
    "    return discounted_r\n",
    "\n",
    "# Testing Code\n",
    "# It's always recommended to test your code\n",
    "input = [1, 1, 1]\n",
    "output = discount_rewards(input)\n",
    "expect = [1 + 0.99 + 0.99**2, 1 + 0.99, 1]\n",
    "np.testing.assert_almost_equal(output, expect)\n",
    "\n",
    "\n",
    "\n",
    "# Setting up our environment\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "max_num_episodes = 500\n",
    "\n",
    "# This list will contain episode rewards from the most recent 100 games\n",
    "# Clear Condition: Average reward per episode >= 195.0 over 100 games\n",
    "EPISODE_100_REWARD_LIST = []\n",
    "for step in range(max_num_episodes):\n",
    "    # Initialize x stack, y stack, and rewards\n",
    "    xs = np.empty(shape=[0, input_size])\n",
    "    ys = np.empty(shape=[0, 1])\n",
    "    rewards = np.empty(shape=[0, 1])\n",
    "\n",
    "    reward_sum = 0\n",
    "    observation = env.reset()\n",
    "\n",
    "    while True:\n",
    "        x = np.reshape(observation, [1, input_size])\n",
    "\n",
    "        # Run the neural net to determine output\n",
    "        action_prob = sess.run(action_pred, feed_dict={X: x})\n",
    "\n",
    "        # Determine the output based on our net, allowing for some randomness\n",
    "        action = 0 if action_prob < np.random.uniform() else 1\n",
    "\n",
    "        # Append the observations and outputs for learning\n",
    "        xs = np.vstack([xs, x])\n",
    "        ys = np.vstack([ys, action])  # Fake action\n",
    "\n",
    "        # Determine the outcome of our action\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        rewards = np.vstack([rewards, reward])\n",
    "        reward_sum += reward\n",
    "\n",
    "        if done:\n",
    "            # Determine standardized rewards\n",
    "            discounted_rewards = discount_rewards(rewards)\n",
    "            # Normalization\n",
    "            discounted_rewards = (discounted_rewards - discounted_rewards.mean())/(discounted_rewards.std() + 1e-7)\n",
    "            l, _ = sess.run([loss, train],\n",
    "                            feed_dict={X: xs, Y: ys, advantages: discounted_rewards})\n",
    "\n",
    "            EPISODE_100_REWARD_LIST.append(reward_sum)\n",
    "            if len(EPISODE_100_REWARD_LIST) > 100:\n",
    "                EPISODE_100_REWARD_LIST = EPISODE_100_REWARD_LIST[1:]\n",
    "            break\n",
    "\n",
    "    # Print status\n",
    "    print(\"[Episode {step:>5}] Reward: {reward_sum:>4} Loss: {l:>10.5}\".format(step=step, reward_sum=reward_sum, l=l))    \n",
    "    \n",
    "    if np.mean(EPISODE_100_REWARD_LIST) >= 195:\n",
    "        print(\"Game Cleared within {step} steps with the average reward: {np.mean(EPISODE_100_REWARD_LIST)}\".format(step, np.mean(EPISODE_100_REWARD_LIST)))\n",
    "        break\n",
    "\n",
    "# See our trained bot in action\n",
    "observation = env.reset()\n",
    "reward_sum = 0\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    x = np.reshape(observation, [1, input_size])\n",
    "    action_prob = sess.run(action_pred, feed_dict={X: x})\n",
    "    action = 0 if action_prob < 0.5 else 1  # No randomness\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        print(\"Total score: {}\".format(reward_sum))\n",
    "        break\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  PG with softmax\n",
    "*revised by hotae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hotae319/anaconda2/envs/tensorflow/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "(<tf.Tensor 'input_y:0' shape=(?, 2) dtype=float32>, <tf.Tensor 'Softmax:0' shape=(?, 2) dtype=float32>)\n",
      "[Episode 0] Reward: 34.0 Loss: -0.012835\n",
      "[Episode 1] Reward: 22.0 Loss: 0.0052052\n",
      "[Episode 2] Reward: 27.0 Loss: -0.0028947\n",
      "[Episode 3] Reward: 16.0 Loss: 0.046365\n",
      "[Episode 4] Reward: 13.0 Loss: 0.16681\n",
      "[Episode 5] Reward: 33.0 Loss: 0.059537\n",
      "[Episode 6] Reward: 41.0 Loss: 0.10446\n",
      "[Episode 7] Reward: 14.0 Loss: 0.027798\n",
      "[Episode 8] Reward: 20.0 Loss: -0.02997\n",
      "[Episode 9] Reward: 26.0 Loss: 0.013564\n",
      "[Episode 10] Reward: 20.0 Loss: 0.012534\n",
      "[Episode 11] Reward: 24.0 Loss: 0.037356\n",
      "[Episode 12] Reward: 31.0 Loss: -0.0030864\n",
      "[Episode 13] Reward: 19.0 Loss: 0.043295\n",
      "[Episode 14] Reward: 38.0 Loss: 0.011046\n",
      "[Episode 15] Reward: 23.0 Loss: -0.031297\n",
      "[Episode 16] Reward: 61.0 Loss: 0.0043522\n",
      "[Episode 17] Reward: 14.0 Loss: -0.057166\n",
      "[Episode 18] Reward: 13.0 Loss: -0.011575\n",
      "[Episode 19] Reward: 29.0 Loss: -0.0053068\n",
      "[Episode 20] Reward: 19.0 Loss: 0.010474\n",
      "[Episode 21] Reward: 37.0 Loss: -0.0084054\n",
      "[Episode 22] Reward: 12.0 Loss: -0.09638\n",
      "[Episode 23] Reward: 48.0 Loss: -0.087595\n",
      "[Episode 24] Reward: 41.0 Loss: 0.0057407\n",
      "[Episode 25] Reward: 35.0 Loss: -0.10424\n",
      "[Episode 26] Reward: 57.0 Loss: -0.013281\n",
      "[Episode 27] Reward: 16.0 Loss: -0.010284\n",
      "[Episode 28] Reward: 13.0 Loss: 0.081994\n",
      "[Episode 29] Reward: 53.0 Loss: -0.023939\n",
      "[Episode 30] Reward: 47.0 Loss: -0.009289\n",
      "[Episode 31] Reward: 22.0 Loss: 0.015816\n",
      "[Episode 32] Reward: 22.0 Loss: -0.052673\n",
      "[Episode 33] Reward: 45.0 Loss: -0.031702\n",
      "[Episode 34] Reward: 145.0 Loss: 0.0024651\n",
      "[Episode 35] Reward: 41.0 Loss: 0.007227\n",
      "[Episode 36] Reward: 52.0 Loss: 0.0029274\n",
      "[Episode 37] Reward: 62.0 Loss: 0.0025668\n",
      "[Episode 38] Reward: 91.0 Loss: 0.00042833\n",
      "[Episode 39] Reward: 51.0 Loss: -0.016651\n",
      "[Episode 40] Reward: 42.0 Loss: -0.0020908\n",
      "[Episode 41] Reward: 17.0 Loss: -0.016893\n",
      "[Episode 42] Reward: 159.0 Loss: -0.0095389\n",
      "[Episode 43] Reward: 81.0 Loss: -0.020099\n",
      "[Episode 44] Reward: 34.0 Loss: -0.11956\n",
      "[Episode 45] Reward: 111.0 Loss: -0.0086253\n",
      "[Episode 46] Reward: 39.0 Loss: -0.08923\n",
      "[Episode 47] Reward: 32.0 Loss: -0.0088636\n",
      "[Episode 48] Reward: 29.0 Loss: -0.0418\n",
      "[Episode 49] Reward: 46.0 Loss: -0.060408\n",
      "[Episode 50] Reward: 85.0 Loss: -0.02227\n",
      "[Episode 51] Reward: 61.0 Loss: -0.04507\n",
      "[Episode 52] Reward: 37.0 Loss: -0.017329\n",
      "[Episode 53] Reward: 48.0 Loss: 0.004807\n",
      "[Episode 54] Reward: 41.0 Loss: -0.0067083\n",
      "[Episode 55] Reward: 72.0 Loss: -0.04051\n",
      "[Episode 56] Reward: 75.0 Loss: -0.022336\n",
      "[Episode 57] Reward: 72.0 Loss: 0.0063893\n",
      "[Episode 58] Reward: 52.0 Loss: 0.0054087\n",
      "[Episode 59] Reward: 108.0 Loss: -0.0074419\n",
      "[Episode 60] Reward: 23.0 Loss: -0.27425\n",
      "[Episode 61] Reward: 104.0 Loss: 0.020135\n",
      "[Episode 62] Reward: 126.0 Loss: -0.0017943\n",
      "[Episode 63] Reward: 44.0 Loss: -0.0080138\n",
      "[Episode 64] Reward: 73.0 Loss: -0.0064137\n",
      "[Episode 65] Reward: 37.0 Loss: -0.14846\n",
      "[Episode 66] Reward: 81.0 Loss: 0.00276\n",
      "[Episode 67] Reward: 139.0 Loss: 0.011051\n",
      "[Episode 68] Reward: 60.0 Loss: 0.029498\n",
      "[Episode 69] Reward: 142.0 Loss: 0.0021974\n",
      "[Episode 70] Reward: 148.0 Loss: 0.012379\n",
      "[Episode 71] Reward: 103.0 Loss: 0.002371\n",
      "[Episode 72] Reward: 112.0 Loss: 0.035309\n",
      "[Episode 73] Reward: 95.0 Loss: 0.023376\n",
      "[Episode 74] Reward: 120.0 Loss: 0.0070093\n",
      "[Episode 75] Reward: 139.0 Loss: 0.01888\n",
      "[Episode 76] Reward: 102.0 Loss: -0.042207\n",
      "[Episode 77] Reward: 190.0 Loss: -0.0029921\n",
      "[Episode 78] Reward: 175.0 Loss: -0.017063\n",
      "[Episode 79] Reward: 200.0 Loss: 0.010747\n",
      "[Episode 80] Reward: 77.0 Loss: -0.050343\n",
      "[Episode 81] Reward: 135.0 Loss: 0.042747\n",
      "[Episode 82] Reward: 173.0 Loss: -0.012731\n",
      "[Episode 83] Reward: 200.0 Loss: -0.015699\n",
      "[Episode 84] Reward: 169.0 Loss: 0.020748\n",
      "[Episode 85] Reward: 106.0 Loss: -0.019942\n",
      "[Episode 86] Reward: 131.0 Loss: -0.049879\n",
      "[Episode 87] Reward: 200.0 Loss: 0.011188\n",
      "[Episode 88] Reward: 170.0 Loss: 0.016184\n",
      "[Episode 89] Reward: 200.0 Loss: -0.0010427\n",
      "[Episode 90] Reward: 123.0 Loss: 0.016008\n",
      "[Episode 91] Reward: 200.0 Loss: 0.033238\n",
      "[Episode 92] Reward: 166.0 Loss: -0.0039194\n",
      "[Episode 93] Reward: 93.0 Loss: 0.00091045\n",
      "[Episode 94] Reward: 200.0 Loss: -0.0079454\n",
      "[Episode 95] Reward: 200.0 Loss: 0.033052\n",
      "[Episode 96] Reward: 200.0 Loss: 0.023489\n",
      "[Episode 97] Reward: 200.0 Loss: 0.03788\n",
      "[Episode 98] Reward: 120.0 Loss: -0.039069\n",
      "[Episode 99] Reward: 155.0 Loss: 0.0023989\n",
      "[Episode 100] Reward: 108.0 Loss: 0.056285\n",
      "[Episode 101] Reward: 136.0 Loss: -0.005192\n",
      "[Episode 102] Reward: 169.0 Loss: -0.016222\n",
      "[Episode 103] Reward: 200.0 Loss: -0.018349\n",
      "[Episode 104] Reward: 200.0 Loss: 0.005361\n",
      "[Episode 105] Reward: 91.0 Loss: -0.023034\n",
      "[Episode 106] Reward: 134.0 Loss: -0.00082296\n",
      "[Episode 107] Reward: 126.0 Loss: -0.042067\n",
      "[Episode 108] Reward: 200.0 Loss: 0.02662\n",
      "[Episode 109] Reward: 200.0 Loss: -0.012727\n",
      "[Episode 110] Reward: 83.0 Loss: -0.0076041\n",
      "[Episode 111] Reward: 200.0 Loss: 0.029625\n",
      "[Episode 112] Reward: 200.0 Loss: 0.023267\n",
      "[Episode 113] Reward: 146.0 Loss: 0.029242\n",
      "[Episode 114] Reward: 162.0 Loss: -0.0092098\n",
      "[Episode 115] Reward: 175.0 Loss: 0.022662\n",
      "[Episode 116] Reward: 158.0 Loss: 0.0388\n",
      "[Episode 117] Reward: 200.0 Loss: -0.030356\n",
      "[Episode 118] Reward: 150.0 Loss: 0.01787\n",
      "[Episode 119] Reward: 80.0 Loss: 0.010596\n",
      "[Episode 120] Reward: 51.0 Loss: -0.057443\n",
      "[Episode 121] Reward: 135.0 Loss: 0.028981\n",
      "[Episode 122] Reward: 121.0 Loss: 0.0012034\n",
      "[Episode 123] Reward: 157.0 Loss: 0.018372\n",
      "[Episode 124] Reward: 102.0 Loss: -0.038302\n",
      "[Episode 125] Reward: 68.0 Loss: -0.022693\n",
      "[Episode 126] Reward: 124.0 Loss: 0.0046621\n",
      "[Episode 127] Reward: 166.0 Loss: -0.029638\n",
      "[Episode 128] Reward: 90.0 Loss: -0.034914\n",
      "[Episode 129] Reward: 189.0 Loss: -0.028882\n",
      "[Episode 130] Reward: 184.0 Loss: -0.025285\n",
      "[Episode 131] Reward: 118.0 Loss: 0.021191\n",
      "[Episode 132] Reward: 152.0 Loss: 0.0044082\n",
      "[Episode 133] Reward: 200.0 Loss: -0.014261\n",
      "[Episode 134] Reward: 139.0 Loss: -0.025775\n",
      "[Episode 135] Reward: 156.0 Loss: 0.025575\n",
      "[Episode 136] Reward: 80.0 Loss: 0.0034013\n",
      "[Episode 137] Reward: 106.0 Loss: 0.025092\n",
      "[Episode 138] Reward: 167.0 Loss: 0.011472\n",
      "[Episode 139] Reward: 179.0 Loss: 0.033087\n",
      "[Episode 140] Reward: 120.0 Loss: 0.028901\n",
      "[Episode 141] Reward: 200.0 Loss: 0.026898\n",
      "[Episode 142] Reward: 191.0 Loss: 0.03112\n",
      "[Episode 143] Reward: 200.0 Loss: -0.0083859\n",
      "[Episode 144] Reward: 72.0 Loss: -0.014791\n",
      "[Episode 145] Reward: 35.0 Loss: -0.04847\n",
      "[Episode 146] Reward: 200.0 Loss: -0.015461\n",
      "[Episode 147] Reward: 162.0 Loss: 0.041065\n",
      "[Episode 148] Reward: 200.0 Loss: 0.035874\n",
      "[Episode 149] Reward: 198.0 Loss: 0.021945\n",
      "[Episode 150] Reward: 101.0 Loss: -0.13108\n",
      "[Episode 151] Reward: 108.0 Loss: -0.048327\n",
      "[Episode 152] Reward: 126.0 Loss: 0.011955\n",
      "[Episode 153] Reward: 200.0 Loss: 0.034021\n",
      "[Episode 154] Reward: 200.0 Loss: 0.034124\n",
      "[Episode 155] Reward: 200.0 Loss: -0.0032353\n",
      "[Episode 156] Reward: 200.0 Loss: 0.021116\n",
      "[Episode 157] Reward: 200.0 Loss: -0.00036825\n",
      "[Episode 158] Reward: 194.0 Loss: 0.035011\n",
      "[Episode 159] Reward: 200.0 Loss: 0.010883\n",
      "[Episode 160] Reward: 200.0 Loss: -0.016827\n",
      "[Episode 161] Reward: 200.0 Loss: -0.034227\n",
      "[Episode 162] Reward: 200.0 Loss: 0.004701\n",
      "[Episode 163] Reward: 200.0 Loss: 0.023991\n",
      "[Episode 164] Reward: 200.0 Loss: 0.005649\n",
      "[Episode 165] Reward: 200.0 Loss: 0.032745\n",
      "[Episode 166] Reward: 200.0 Loss: 0.01187\n",
      "[Episode 167] Reward: 200.0 Loss: 0.0040915\n",
      "[Episode 168] Reward: 97.0 Loss: 0.064013\n",
      "[Episode 169] Reward: 200.0 Loss: -0.0059402\n",
      "[Episode 170] Reward: 200.0 Loss: -0.0089569\n",
      "[Episode 171] Reward: 200.0 Loss: 0.0052902\n",
      "[Episode 172] Reward: 200.0 Loss: 0.030574\n",
      "[Episode 173] Reward: 90.0 Loss: 0.012578\n",
      "[Episode 174] Reward: 200.0 Loss: -0.0061898\n",
      "[Episode 175] Reward: 200.0 Loss: -7.3214e-05\n",
      "[Episode 176] Reward: 200.0 Loss: 0.031949\n",
      "[Episode 177] Reward: 135.0 Loss: -0.002421\n",
      "[Episode 178] Reward: 198.0 Loss: 0.011529\n",
      "[Episode 179] Reward: 174.0 Loss: 0.020211\n",
      "[Episode 180] Reward: 151.0 Loss: 0.019195\n",
      "[Episode 181] Reward: 161.0 Loss: 0.035912\n",
      "[Episode 182] Reward: 200.0 Loss: 0.035072\n",
      "[Episode 183] Reward: 194.0 Loss: 0.026542\n",
      "[Episode 184] Reward: 91.0 Loss: 0.084212\n",
      "[Episode 185] Reward: 200.0 Loss: 0.033069\n",
      "[Episode 186] Reward: 131.0 Loss: 0.01895\n",
      "[Episode 187] Reward: 200.0 Loss: -0.012062\n",
      "[Episode 188] Reward: 153.0 Loss: -0.0068235\n",
      "[Episode 189] Reward: 113.0 Loss: 0.018376\n",
      "[Episode 190] Reward: 120.0 Loss: -0.011961\n",
      "[Episode 191] Reward: 126.0 Loss: -0.0012005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 192] Reward: 121.0 Loss: 0.053207\n",
      "[Episode 193] Reward: 200.0 Loss: 0.0011435\n",
      "[Episode 194] Reward: 200.0 Loss: 0.0023832\n",
      "[Episode 195] Reward: 106.0 Loss: -0.014081\n",
      "[Episode 196] Reward: 136.0 Loss: -0.020721\n",
      "[Episode 197] Reward: 200.0 Loss: 2.5145e-05\n",
      "[Episode 198] Reward: 200.0 Loss: -0.00026377\n",
      "[Episode 199] Reward: 93.0 Loss: -0.0067522\n",
      "[Episode 200] Reward: 200.0 Loss: 0.0080798\n",
      "[Episode 201] Reward: 200.0 Loss: -0.017945\n",
      "[Episode 202] Reward: 200.0 Loss: -0.0030089\n",
      "[Episode 203] Reward: 129.0 Loss: 0.01493\n",
      "[Episode 204] Reward: 126.0 Loss: 0.034921\n",
      "[Episode 205] Reward: 200.0 Loss: 0.0023429\n",
      "[Episode 206] Reward: 200.0 Loss: 0.0049147\n",
      "[Episode 207] Reward: 132.0 Loss: 0.025108\n",
      "[Episode 208] Reward: 127.0 Loss: -0.011088\n",
      "[Episode 209] Reward: 172.0 Loss: 0.00053539\n",
      "[Episode 210] Reward: 200.0 Loss: 0.0064403\n",
      "[Episode 211] Reward: 112.0 Loss: 0.024515\n",
      "[Episode 212] Reward: 200.0 Loss: -0.011128\n",
      "[Episode 213] Reward: 200.0 Loss: 0.00014444\n",
      "[Episode 214] Reward: 200.0 Loss: 0.032887\n",
      "[Episode 215] Reward: 77.0 Loss: -0.051608\n",
      "[Episode 216] Reward: 119.0 Loss: 0.030422\n",
      "[Episode 217] Reward: 200.0 Loss: 0.010295\n",
      "[Episode 218] Reward: 161.0 Loss: -0.003054\n",
      "[Episode 219] Reward: 152.0 Loss: -0.024772\n",
      "[Episode 220] Reward: 200.0 Loss: 0.00067149\n",
      "[Episode 221] Reward: 200.0 Loss: -0.020999\n",
      "[Episode 222] Reward: 89.0 Loss: -0.035242\n",
      "[Episode 223] Reward: 158.0 Loss: -0.019536\n",
      "[Episode 224] Reward: 102.0 Loss: 0.012245\n",
      "[Episode 225] Reward: 169.0 Loss: -0.065805\n",
      "[Episode 226] Reward: 81.0 Loss: -0.035753\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fe7f83772809>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Run the neural net to determine output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0maction_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Append the observations and outputs for learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "This code is based on:\n",
    "https://github.com/hunkim/DeepRL-Agents\n",
    "http://karpathy.github.io/2016/05/31/rl/\n",
    "'''\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "hidden_layer_neurons = 24\n",
    "learning_rate = 1e-2\n",
    "gamma = .99\n",
    "\n",
    "# Constants defining our neural network\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, input_size], name=\"input_x\")\n",
    "\n",
    "# First layer of weights\n",
    "W1 = tf.get_variable(\"W1\", shape=[input_size, hidden_layer_neurons],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W1))\n",
    "\n",
    "# Second layer of weights\n",
    "W2 = tf.get_variable(\"W2\", shape=[hidden_layer_neurons, output_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "action_pred = tf.nn.softmax(tf.matmul(layer1, W2))\n",
    "\n",
    "# We need to define the parts of the network needed for learning a policy\n",
    "Y = tf.placeholder(tf.float32, [None, output_size], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32, name=\"reward_signal\")\n",
    "\n",
    "print(Y, action_pred)\n",
    "# Loss function, ∑ Ai*logp(yi∣xi), but we need fake lable Y due to autodiff\n",
    "log_lik = -Y * tf.log(action_pred)  ## we can consider one-hot encoding\n",
    "log_lik_adv = log_lik * advantages\n",
    "loss = tf.reduce_mean(tf.reduce_sum(log_lik_adv, axis=1))\n",
    "\n",
    "# Learning\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "def discount_rewards(r, gamma=0.99):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r, dtype=np.float32)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(len(r))):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "\n",
    "    return discounted_r\n",
    "\n",
    "\n",
    "# Setting up our environment\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "num_episodes = 1000\n",
    "# This list will contain episode rewards from the most recent 100 games\n",
    "# Clear Condition: Average reward per episode >= 195.0 over 100 games\n",
    "EPISODE_100_REWARD_LIST = []\n",
    "for i in range(num_episodes):\n",
    "\n",
    "    # Clear out game variables\n",
    "    xs = np.empty(shape=[0, input_size])\n",
    "    ys = np.empty(shape=[0, output_size])\n",
    "    rewards = np.empty(shape=[0, 1])\n",
    "\n",
    "    reward_sum = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "        # Append the observations to our batch\n",
    "        x = np.reshape(state, [1, input_size])\n",
    "\n",
    "        # Run the neural net to determine output\n",
    "        action_prob = sess.run(action_pred, feed_dict={X: x})        \n",
    "        action = np.random.choice(np.arange(output_size), p=action_prob[0]) ## choose the action label of 0 ~ output_size-1 \n",
    "    \n",
    "        # Append the observations and outputs for learning\n",
    "        xs = np.vstack([xs, x])\n",
    "        y = np.zeros(output_size)\n",
    "        y[action] = 1\n",
    "        \n",
    "        ys = np.vstack([ys, y])\n",
    "\n",
    "        # Determine the outcome of our action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        rewards = np.vstack([rewards, reward])\n",
    "\n",
    "        if done:\n",
    "            # Determine standardized rewards\n",
    "            discounted_rewards = discount_rewards(rewards, gamma)\n",
    "            # Normalization\n",
    "            discounted_rewards = (discounted_rewards - discounted_rewards.mean())/(discounted_rewards.std() + 1e-7)\n",
    "            ll, la, l, _ = sess.run([log_lik, log_lik_adv, loss, train], feed_dict={X: xs,\n",
    "                                                                                    Y: ys,\n",
    "                                                                                    advantages: discounted_rewards})\n",
    "            # print values for debugging\n",
    "            # print(1, ll, la)\n",
    "            EPISODE_100_REWARD_LIST.append(reward_sum)\n",
    "            if len(EPISODE_100_REWARD_LIST) > 100:\n",
    "                EPISODE_100_REWARD_LIST = EPISODE_100_REWARD_LIST[1:]\n",
    "            break\n",
    "\n",
    "\n",
    "    # Print status\n",
    "    print(\"[Episode {i:>}] Reward: {reward_sum:>4} Loss: {l:>5.5}\".format(i = i, reward_sum=reward_sum, l = l))\n",
    "    \n",
    "    if np.mean(EPISODE_100_REWARD_LIST) >= 195.0:\n",
    "        print(\"Game Cleared within {i} steps with the average reward: {np.mean(EPISODE_100_REWARD_LIST)}\".format(i, np.mean(EPISODE_100_REWARD_LIST) ))\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "state = env.reset()\n",
    "reward_sum = 0\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "\n",
    "    x = np.reshape(state, [1, input_size])\n",
    "    action_prob = sess.run(action_pred, feed_dict={X: x})\n",
    "    action = np.argmax(action_prob)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        print(\"Total score: {}\".format(reward_sum))\n",
    "        break\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
